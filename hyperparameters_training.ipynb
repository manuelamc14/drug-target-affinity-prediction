{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelamunoz/miniforge3/envs/DS-env-3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import torch.nn as nn\n",
    "from models.gcn import GCNNet\n",
    "from utils import *\n",
    "import itertools\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, file_name):\n",
    "  print('Training on {} samples...'.format(len(train_loader.dataset)))\n",
    "  model.train()\n",
    "  losses=[]\n",
    "  for batch_idx, data in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % LOG_INTERVAL == 0:\n",
    "      print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                                                                      batch_idx * len(data.x),\n",
    "                                                                      len(train_loader.dataset),\n",
    "                                                                      100. * batch_idx / len(train_loader),\n",
    "                                                                      loss.item()))\n",
    "    losses.append(loss.item())\n",
    "    # Write the loss values to a CSV file\n",
    "  \n",
    "    with open(file_name, 'a') as f:\n",
    "      writer = csv.writer(f)\n",
    "      writer.writerow([epoch] + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predicting(model, device, loader):\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "            total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "    return total_labels.numpy().flatten(),total_preds.numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of hyperparameters to search over\n",
    "learning_rates = [0.0005]\n",
    "batch_sizes = [1024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all possible hyperparameter combinations\n",
    "hyperparameter_grid = list(itertools.product(\n",
    "    learning_rates, batch_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all hyperparameter combinations and train models\n",
    "for hyperparameters in hyperparameter_grid:\n",
    "    # Unpack the hyperparameters\n",
    "    learning_rate, batch_size = hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LOG_INTERVAL = 20\n",
    "NUM_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset of interest\n",
    "dataset = 'davis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed data found: data/processed/davis_train.pt, loading ...\n",
      "Pre-processed data found: data/processed/davis_test.pt, loading ...\n"
     ]
    }
   ],
   "source": [
    "# access to the processed training data file\n",
    "processed_data_file_train = 'data/processed/' + dataset + '_train.pt'\n",
    "# access to the processed test data file\n",
    "processed_data_file_test = 'data/processed/' + dataset + '_test.pt'\n",
    "# train / test data\n",
    "train_data = TestbedDataset(root='data', dataset=dataset+'_train')\n",
    "test_data = TestbedDataset(root='data', dataset=dataset+'_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005 1024\n",
      "Training on 25046 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelamunoz/miniforge3/envs/DS-env-3.10/lib/python3.10/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [0/25046 (0%)]\tLoss: 31.038729\n",
      "Train epoch: 1 [659280/25046 (80%)]\tLoss: 1.213940\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  1 ; best_mse,best_ci: 1.0649043 0 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 2 [0/25046 (0%)]\tLoss: 1.235556\n",
      "Train epoch: 2 [658780/25046 (80%)]\tLoss: 1.007857\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  2 ; best_mse,best_ci: 0.748785 1 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 3 [0/25046 (0%)]\tLoss: 0.974528\n",
      "Train epoch: 3 [654640/25046 (80%)]\tLoss: 0.785360\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  3 ; best_mse,best_ci: 0.7243942 2 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 4 [0/25046 (0%)]\tLoss: 0.832554\n",
      "Train epoch: 4 [662380/25046 (80%)]\tLoss: 0.785494\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  4 ; best_mse,best_ci: 0.7187799 3 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 5 [0/25046 (0%)]\tLoss: 0.766522\n",
      "Train epoch: 5 [665220/25046 (80%)]\tLoss: 0.797011\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  5 ; best_mse,best_ci: 0.67489153 4 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 6 [0/25046 (0%)]\tLoss: 0.756587\n",
      "Train epoch: 6 [656180/25046 (80%)]\tLoss: 0.626547\n",
      "Make prediction for 5010 samples...\n",
      "0.7256512 No improvement since epoch  5 ; best_mse,best_ci: 0.67489153 4 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 7 [0/25046 (0%)]\tLoss: 0.645315\n",
      "Train epoch: 7 [658900/25046 (80%)]\tLoss: 0.676082\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  7 ; best_mse,best_ci: 0.59316397 6 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 8 [0/25046 (0%)]\tLoss: 0.606152\n",
      "Train epoch: 8 [658560/25046 (80%)]\tLoss: 0.654311\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  8 ; best_mse,best_ci: 0.5782133 7 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 9 [0/25046 (0%)]\tLoss: 0.645781\n",
      "Train epoch: 9 [657400/25046 (80%)]\tLoss: 0.633418\n",
      "Make prediction for 5010 samples...\n",
      "0.6040329 No improvement since epoch  8 ; best_mse,best_ci: 0.5782133 7 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 10 [0/25046 (0%)]\tLoss: 0.631353\n",
      "Train epoch: 10 [651240/25046 (80%)]\tLoss: 0.652819\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  10 ; best_mse,best_ci: 0.5736372 9 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 11 [0/25046 (0%)]\tLoss: 0.579384\n",
      "Train epoch: 11 [653740/25046 (80%)]\tLoss: 0.617736\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  11 ; best_mse,best_ci: 0.55914885 10 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 12 [0/25046 (0%)]\tLoss: 0.610590\n",
      "Train epoch: 12 [654540/25046 (80%)]\tLoss: 0.572014\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  12 ; best_mse,best_ci: 0.55555546 11 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 13 [0/25046 (0%)]\tLoss: 0.609112\n",
      "Train epoch: 13 [657500/25046 (80%)]\tLoss: 0.577376\n",
      "Make prediction for 5010 samples...\n",
      "0.55626684 No improvement since epoch  12 ; best_mse,best_ci: 0.55555546 11 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 14 [0/25046 (0%)]\tLoss: 0.576059\n",
      "Train epoch: 14 [664440/25046 (80%)]\tLoss: 0.560027\n",
      "Make prediction for 5010 samples...\n",
      "0.56107724 No improvement since epoch  12 ; best_mse,best_ci: 0.55555546 11 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 15 [0/25046 (0%)]\tLoss: 0.578313\n",
      "Train epoch: 15 [655900/25046 (80%)]\tLoss: 0.598267\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  15 ; best_mse,best_ci: 0.5407712 14 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 16 [0/25046 (0%)]\tLoss: 0.643141\n",
      "Train epoch: 16 [654700/25046 (80%)]\tLoss: 0.568279\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  16 ; best_mse,best_ci: 0.53459847 15 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 17 [0/25046 (0%)]\tLoss: 0.554450\n",
      "Train epoch: 17 [663640/25046 (80%)]\tLoss: 0.541626\n",
      "Make prediction for 5010 samples...\n",
      "0.5703279 No improvement since epoch  16 ; best_mse,best_ci: 0.53459847 15 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 18 [0/25046 (0%)]\tLoss: 0.561959\n",
      "Train epoch: 18 [656520/25046 (80%)]\tLoss: 0.486557\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  18 ; best_mse,best_ci: 0.5257089 17 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 19 [0/25046 (0%)]\tLoss: 0.544073\n",
      "Train epoch: 19 [654000/25046 (80%)]\tLoss: 0.597705\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  19 ; best_mse,best_ci: 0.52263975 18 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 20 [0/25046 (0%)]\tLoss: 0.486452\n",
      "Train epoch: 20 [656900/25046 (80%)]\tLoss: 0.477741\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  20 ; best_mse,best_ci: 0.5170441 19 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 21 [0/25046 (0%)]\tLoss: 0.533987\n",
      "Train epoch: 21 [652580/25046 (80%)]\tLoss: 0.522214\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  21 ; best_mse,best_ci: 0.50926304 20 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 22 [0/25046 (0%)]\tLoss: 0.574526\n",
      "Train epoch: 22 [656940/25046 (80%)]\tLoss: 0.560931\n",
      "Make prediction for 5010 samples...\n",
      "0.52546775 No improvement since epoch  21 ; best_mse,best_ci: 0.50926304 20 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 23 [0/25046 (0%)]\tLoss: 0.534693\n",
      "Train epoch: 23 [659180/25046 (80%)]\tLoss: 0.507385\n",
      "Make prediction for 5010 samples...\n",
      "0.53224695 No improvement since epoch  21 ; best_mse,best_ci: 0.50926304 20 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 24 [0/25046 (0%)]\tLoss: 0.494851\n",
      "Train epoch: 24 [656180/25046 (80%)]\tLoss: 0.530324\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  24 ; best_mse,best_ci: 0.5050774 23 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 25 [0/25046 (0%)]\tLoss: 0.466736\n",
      "Train epoch: 25 [656020/25046 (80%)]\tLoss: 0.508293\n",
      "Make prediction for 5010 samples...\n",
      "0.5279507 No improvement since epoch  24 ; best_mse,best_ci: 0.5050774 23 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 26 [0/25046 (0%)]\tLoss: 0.525810\n",
      "Train epoch: 26 [656440/25046 (80%)]\tLoss: 0.531169\n",
      "Make prediction for 5010 samples...\n",
      "0.50885946 No improvement since epoch  24 ; best_mse,best_ci: 0.5050774 23 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 27 [0/25046 (0%)]\tLoss: 0.457477\n",
      "Train epoch: 27 [658420/25046 (80%)]\tLoss: 0.543260\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  27 ; best_mse,best_ci: 0.49667916 26 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 28 [0/25046 (0%)]\tLoss: 0.512974\n",
      "Train epoch: 28 [655920/25046 (80%)]\tLoss: 0.498178\n",
      "Make prediction for 5010 samples...\n",
      "0.50886613 No improvement since epoch  27 ; best_mse,best_ci: 0.49667916 26 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 29 [0/25046 (0%)]\tLoss: 0.546337\n",
      "Train epoch: 29 [652280/25046 (80%)]\tLoss: 0.560706\n",
      "Make prediction for 5010 samples...\n",
      "0.5264796 No improvement since epoch  27 ; best_mse,best_ci: 0.49667916 26 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 30 [0/25046 (0%)]\tLoss: 0.480137\n",
      "Train epoch: 30 [651780/25046 (80%)]\tLoss: 0.556912\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  30 ; best_mse,best_ci: 0.49456117 29 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 31 [0/25046 (0%)]\tLoss: 0.474298\n",
      "Train epoch: 31 [655800/25046 (80%)]\tLoss: 0.555243\n",
      "Make prediction for 5010 samples...\n",
      "0.5898922 No improvement since epoch  30 ; best_mse,best_ci: 0.49456117 29 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 32 [0/25046 (0%)]\tLoss: 0.621087\n",
      "Train epoch: 32 [654740/25046 (80%)]\tLoss: 0.601977\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  32 ; best_mse,best_ci: 0.4909902 31 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 33 [0/25046 (0%)]\tLoss: 0.490674\n",
      "Train epoch: 33 [658680/25046 (80%)]\tLoss: 0.437249\n",
      "Make prediction for 5010 samples...\n",
      "0.5129997 No improvement since epoch  32 ; best_mse,best_ci: 0.4909902 31 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 34 [0/25046 (0%)]\tLoss: 0.466659\n",
      "Train epoch: 34 [659680/25046 (80%)]\tLoss: 0.582003\n",
      "Make prediction for 5010 samples...\n",
      "0.49173567 No improvement since epoch  32 ; best_mse,best_ci: 0.4909902 31 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 35 [0/25046 (0%)]\tLoss: 0.489949\n",
      "Train epoch: 35 [660180/25046 (80%)]\tLoss: 0.552198\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 36 [0/25046 (0%)]\tLoss: 0.535428\n",
      "Train epoch: 36 [652180/25046 (80%)]\tLoss: 0.483199\n",
      "Make prediction for 5010 samples...\n",
      "0.48654595 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 37 [0/25046 (0%)]\tLoss: 0.448224\n",
      "Train epoch: 37 [653760/25046 (80%)]\tLoss: 0.479856\n",
      "Make prediction for 5010 samples...\n",
      "0.5082785 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 38 [0/25046 (0%)]\tLoss: 0.512733\n",
      "Train epoch: 38 [655780/25046 (80%)]\tLoss: 0.468742\n",
      "Make prediction for 5010 samples...\n",
      "0.55878174 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 39 [0/25046 (0%)]\tLoss: 0.573722\n",
      "Train epoch: 39 [656300/25046 (80%)]\tLoss: 0.445249\n",
      "Make prediction for 5010 samples...\n",
      "0.5000284 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 40 [0/25046 (0%)]\tLoss: 0.487595\n",
      "Train epoch: 40 [657900/25046 (80%)]\tLoss: 0.508721\n",
      "Make prediction for 5010 samples...\n",
      "0.5081361 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 41 [0/25046 (0%)]\tLoss: 0.438253\n",
      "Train epoch: 41 [659500/25046 (80%)]\tLoss: 0.502471\n",
      "Make prediction for 5010 samples...\n",
      "0.5085495 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 42 [0/25046 (0%)]\tLoss: 0.466609\n",
      "Train epoch: 42 [662540/25046 (80%)]\tLoss: 0.483758\n",
      "Make prediction for 5010 samples...\n",
      "0.49187708 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 43 [0/25046 (0%)]\tLoss: 0.513080\n",
      "Train epoch: 43 [655780/25046 (80%)]\tLoss: 0.489922\n",
      "Make prediction for 5010 samples...\n",
      "0.49006817 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 44 [0/25046 (0%)]\tLoss: 0.502550\n",
      "Train epoch: 44 [656200/25046 (80%)]\tLoss: 0.537880\n",
      "Make prediction for 5010 samples...\n",
      "0.496565 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 45 [0/25046 (0%)]\tLoss: 0.491884\n",
      "Train epoch: 45 [657200/25046 (80%)]\tLoss: 0.519275\n",
      "Make prediction for 5010 samples...\n",
      "0.5115483 No improvement since epoch  35 ; best_mse,best_ci: 0.48415458 34 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 46 [0/25046 (0%)]\tLoss: 0.552248\n",
      "Train epoch: 46 [655220/25046 (80%)]\tLoss: 0.485781\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  46 ; best_mse,best_ci: 0.47503322 45 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 47 [0/25046 (0%)]\tLoss: 0.455006\n",
      "Train epoch: 47 [652780/25046 (80%)]\tLoss: 0.446288\n",
      "Make prediction for 5010 samples...\n",
      "0.54618555 No improvement since epoch  46 ; best_mse,best_ci: 0.47503322 45 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 48 [0/25046 (0%)]\tLoss: 0.481162\n",
      "Train epoch: 48 [657280/25046 (80%)]\tLoss: 0.421731\n",
      "Make prediction for 5010 samples...\n",
      "0.47511816 No improvement since epoch  46 ; best_mse,best_ci: 0.47503322 45 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 49 [0/25046 (0%)]\tLoss: 0.479942\n",
      "Train epoch: 49 [656140/25046 (80%)]\tLoss: 0.471273\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  49 ; best_mse,best_ci: 0.46909326 48 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 50 [0/25046 (0%)]\tLoss: 0.471356\n",
      "Train epoch: 50 [650040/25046 (80%)]\tLoss: 0.493138\n",
      "Make prediction for 5010 samples...\n",
      "0.47232896 No improvement since epoch  49 ; best_mse,best_ci: 0.46909326 48 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 51 [0/25046 (0%)]\tLoss: 0.455845\n",
      "Train epoch: 51 [657720/25046 (80%)]\tLoss: 0.478451\n",
      "Make prediction for 5010 samples...\n",
      "0.49206313 No improvement since epoch  49 ; best_mse,best_ci: 0.46909326 48 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 52 [0/25046 (0%)]\tLoss: 0.593513\n",
      "Train epoch: 52 [658960/25046 (80%)]\tLoss: 0.472605\n",
      "Make prediction for 5010 samples...\n",
      "0.5408987 No improvement since epoch  49 ; best_mse,best_ci: 0.46909326 48 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 53 [0/25046 (0%)]\tLoss: 0.519335\n",
      "Train epoch: 53 [652460/25046 (80%)]\tLoss: 0.459923\n",
      "Make prediction for 5010 samples...\n",
      "0.5295109 No improvement since epoch  49 ; best_mse,best_ci: 0.46909326 48 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 54 [0/25046 (0%)]\tLoss: 0.508301\n",
      "Train epoch: 54 [660300/25046 (80%)]\tLoss: 0.416875\n",
      "Make prediction for 5010 samples...\n",
      "0.4694565 No improvement since epoch  49 ; best_mse,best_ci: 0.46909326 48 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 55 [0/25046 (0%)]\tLoss: 0.524976\n",
      "Train epoch: 55 [660980/25046 (80%)]\tLoss: 0.416975\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  55 ; best_mse,best_ci: 0.46874425 54 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 56 [0/25046 (0%)]\tLoss: 0.413235\n",
      "Train epoch: 56 [653280/25046 (80%)]\tLoss: 0.456501\n",
      "Make prediction for 5010 samples...\n",
      "0.4695734 No improvement since epoch  55 ; best_mse,best_ci: 0.46874425 54 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 57 [0/25046 (0%)]\tLoss: 0.507003\n",
      "Train epoch: 57 [660080/25046 (80%)]\tLoss: 0.520670\n",
      "Make prediction for 5010 samples...\n",
      "0.5750386 No improvement since epoch  55 ; best_mse,best_ci: 0.46874425 54 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 58 [0/25046 (0%)]\tLoss: 0.596879\n",
      "Train epoch: 58 [659520/25046 (80%)]\tLoss: 0.490914\n",
      "Make prediction for 5010 samples...\n",
      "0.4946678 No improvement since epoch  55 ; best_mse,best_ci: 0.46874425 54 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 59 [0/25046 (0%)]\tLoss: 0.456758\n",
      "Train epoch: 59 [652240/25046 (80%)]\tLoss: 0.519140\n",
      "Make prediction for 5010 samples...\n",
      "0.4818393 No improvement since epoch  55 ; best_mse,best_ci: 0.46874425 54 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 60 [0/25046 (0%)]\tLoss: 0.396204\n",
      "Train epoch: 60 [653680/25046 (80%)]\tLoss: 0.467375\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  60 ; best_mse,best_ci: 0.46054542 59 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 61 [0/25046 (0%)]\tLoss: 0.490181\n",
      "Train epoch: 61 [662880/25046 (80%)]\tLoss: 0.492564\n",
      "Make prediction for 5010 samples...\n",
      "0.50121737 No improvement since epoch  60 ; best_mse,best_ci: 0.46054542 59 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 62 [0/25046 (0%)]\tLoss: 0.500358\n",
      "Train epoch: 62 [660900/25046 (80%)]\tLoss: 0.485829\n",
      "Make prediction for 5010 samples...\n",
      "0.5019089 No improvement since epoch  60 ; best_mse,best_ci: 0.46054542 59 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 63 [0/25046 (0%)]\tLoss: 0.492996\n",
      "Train epoch: 63 [651580/25046 (80%)]\tLoss: 0.449274\n",
      "Make prediction for 5010 samples...\n",
      "0.47509015 No improvement since epoch  60 ; best_mse,best_ci: 0.46054542 59 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 64 [0/25046 (0%)]\tLoss: 0.484822\n",
      "Train epoch: 64 [654920/25046 (80%)]\tLoss: 0.508831\n",
      "Make prediction for 5010 samples...\n",
      "0.4615362 No improvement since epoch  60 ; best_mse,best_ci: 0.46054542 59 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 65 [0/25046 (0%)]\tLoss: 0.411083\n",
      "Train epoch: 65 [653100/25046 (80%)]\tLoss: 0.472957\n",
      "Make prediction for 5010 samples...\n",
      "0.46354163 No improvement since epoch  60 ; best_mse,best_ci: 0.46054542 59 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 66 [0/25046 (0%)]\tLoss: 0.461506\n",
      "Train epoch: 66 [657860/25046 (80%)]\tLoss: 0.468589\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  66 ; best_mse,best_ci: 0.43985465 65 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 67 [0/25046 (0%)]\tLoss: 0.444915\n",
      "Train epoch: 67 [656000/25046 (80%)]\tLoss: 0.382593\n",
      "Make prediction for 5010 samples...\n",
      "0.4777509 No improvement since epoch  66 ; best_mse,best_ci: 0.43985465 65 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 68 [0/25046 (0%)]\tLoss: 0.496313\n",
      "Train epoch: 68 [658320/25046 (80%)]\tLoss: 0.576773\n",
      "Make prediction for 5010 samples...\n",
      "0.5008593 No improvement since epoch  66 ; best_mse,best_ci: 0.43985465 65 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 69 [0/25046 (0%)]\tLoss: 0.434134\n",
      "Train epoch: 69 [662320/25046 (80%)]\tLoss: 0.398582\n",
      "Make prediction for 5010 samples...\n",
      "0.4456884 No improvement since epoch  66 ; best_mse,best_ci: 0.43985465 65 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 70 [0/25046 (0%)]\tLoss: 0.402277\n",
      "Train epoch: 70 [656140/25046 (80%)]\tLoss: 0.403674\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  70 ; best_mse,best_ci: 0.427332 69 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 71 [0/25046 (0%)]\tLoss: 0.382268\n",
      "Train epoch: 71 [649980/25046 (80%)]\tLoss: 0.386755\n",
      "Make prediction for 5010 samples...\n",
      "0.45708266 No improvement since epoch  70 ; best_mse,best_ci: 0.427332 69 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 72 [0/25046 (0%)]\tLoss: 0.487733\n",
      "Train epoch: 72 [657860/25046 (80%)]\tLoss: 0.429193\n",
      "Make prediction for 5010 samples...\n",
      "0.43813792 No improvement since epoch  70 ; best_mse,best_ci: 0.427332 69 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 73 [0/25046 (0%)]\tLoss: 0.386175\n",
      "Train epoch: 73 [651160/25046 (80%)]\tLoss: 0.450757\n",
      "Make prediction for 5010 samples...\n",
      "0.42964754 No improvement since epoch  70 ; best_mse,best_ci: 0.427332 69 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 74 [0/25046 (0%)]\tLoss: 0.472051\n",
      "Train epoch: 74 [651640/25046 (80%)]\tLoss: 0.483169\n",
      "Make prediction for 5010 samples...\n",
      "0.47778955 No improvement since epoch  70 ; best_mse,best_ci: 0.427332 69 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 75 [0/25046 (0%)]\tLoss: 0.468340\n",
      "Train epoch: 75 [656240/25046 (80%)]\tLoss: 0.380077\n",
      "Make prediction for 5010 samples...\n",
      "0.4795271 No improvement since epoch  70 ; best_mse,best_ci: 0.427332 69 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 76 [0/25046 (0%)]\tLoss: 0.489359\n",
      "Train epoch: 76 [653040/25046 (80%)]\tLoss: 0.485173\n",
      "Make prediction for 5010 samples...\n",
      "0.43164346 No improvement since epoch  70 ; best_mse,best_ci: 0.427332 69 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 77 [0/25046 (0%)]\tLoss: 0.385170\n",
      "Train epoch: 77 [658540/25046 (80%)]\tLoss: 0.411998\n",
      "Make prediction for 5010 samples...\n",
      "0.46793133 No improvement since epoch  70 ; best_mse,best_ci: 0.427332 69 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 78 [0/25046 (0%)]\tLoss: 0.427111\n",
      "Train epoch: 78 [661880/25046 (80%)]\tLoss: 0.410998\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  78 ; best_mse,best_ci: 0.39454108 77 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 79 [0/25046 (0%)]\tLoss: 0.359485\n",
      "Train epoch: 79 [653000/25046 (80%)]\tLoss: 0.360700\n",
      "Make prediction for 5010 samples...\n",
      "0.41455922 No improvement since epoch  78 ; best_mse,best_ci: 0.39454108 77 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 80 [0/25046 (0%)]\tLoss: 0.355417\n",
      "Train epoch: 80 [653780/25046 (80%)]\tLoss: 0.371099\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  80 ; best_mse,best_ci: 0.39351708 79 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 81 [0/25046 (0%)]\tLoss: 0.358694\n",
      "Train epoch: 81 [663860/25046 (80%)]\tLoss: 0.347279\n",
      "Make prediction for 5010 samples...\n",
      "0.39783964 No improvement since epoch  80 ; best_mse,best_ci: 0.39351708 79 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 82 [0/25046 (0%)]\tLoss: 0.427545\n",
      "Train epoch: 82 [659400/25046 (80%)]\tLoss: 0.379983\n",
      "Make prediction for 5010 samples...\n",
      "0.44664773 No improvement since epoch  80 ; best_mse,best_ci: 0.39351708 79 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 83 [0/25046 (0%)]\tLoss: 0.466682\n",
      "Train epoch: 83 [659740/25046 (80%)]\tLoss: 0.541669\n",
      "Make prediction for 5010 samples...\n",
      "0.51689845 No improvement since epoch  80 ; best_mse,best_ci: 0.39351708 79 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 84 [0/25046 (0%)]\tLoss: 0.518862\n",
      "Train epoch: 84 [650040/25046 (80%)]\tLoss: 0.340067\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  84 ; best_mse,best_ci: 0.3850755 83 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 85 [0/25046 (0%)]\tLoss: 0.393580\n",
      "Train epoch: 85 [664800/25046 (80%)]\tLoss: 0.360575\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  85 ; best_mse,best_ci: 0.3824923 84 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 86 [0/25046 (0%)]\tLoss: 0.311976\n",
      "Train epoch: 86 [659940/25046 (80%)]\tLoss: 0.455593\n",
      "Make prediction for 5010 samples...\n",
      "0.39930212 No improvement since epoch  85 ; best_mse,best_ci: 0.3824923 84 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 87 [0/25046 (0%)]\tLoss: 0.339764\n",
      "Train epoch: 87 [649800/25046 (80%)]\tLoss: 0.383089\n",
      "Make prediction for 5010 samples...\n",
      "0.3848278 No improvement since epoch  85 ; best_mse,best_ci: 0.3824923 84 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 88 [0/25046 (0%)]\tLoss: 0.313148\n",
      "Train epoch: 88 [661800/25046 (80%)]\tLoss: 0.375208\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  88 ; best_mse,best_ci: 0.37871975 87 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 89 [0/25046 (0%)]\tLoss: 0.331504\n",
      "Train epoch: 89 [657720/25046 (80%)]\tLoss: 0.393972\n",
      "Make prediction for 5010 samples...\n",
      "0.46522385 No improvement since epoch  88 ; best_mse,best_ci: 0.37871975 87 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 90 [0/25046 (0%)]\tLoss: 0.472564\n",
      "Train epoch: 90 [665200/25046 (80%)]\tLoss: 0.362619\n",
      "Make prediction for 5010 samples...\n",
      "0.37966618 No improvement since epoch  88 ; best_mse,best_ci: 0.37871975 87 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 91 [0/25046 (0%)]\tLoss: 0.356462\n",
      "Train epoch: 91 [645000/25046 (80%)]\tLoss: 0.355158\n",
      "Make prediction for 5010 samples...\n",
      "0.3831429 No improvement since epoch  88 ; best_mse,best_ci: 0.37871975 87 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 92 [0/25046 (0%)]\tLoss: 0.416789\n",
      "Train epoch: 92 [661280/25046 (80%)]\tLoss: 0.350547\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  92 ; best_mse,best_ci: 0.37419024 91 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 93 [0/25046 (0%)]\tLoss: 0.402397\n",
      "Train epoch: 93 [649000/25046 (80%)]\tLoss: 0.385736\n",
      "Make prediction for 5010 samples...\n",
      "0.41077736 No improvement since epoch  92 ; best_mse,best_ci: 0.37419024 91 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 94 [0/25046 (0%)]\tLoss: 0.351666\n",
      "Train epoch: 94 [658720/25046 (80%)]\tLoss: 0.373732\n",
      "Make prediction for 5010 samples...\n",
      "0.40108055 No improvement since epoch  92 ; best_mse,best_ci: 0.37419024 91 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 95 [0/25046 (0%)]\tLoss: 0.370757\n",
      "Train epoch: 95 [657860/25046 (80%)]\tLoss: 0.335293\n",
      "Make prediction for 5010 samples...\n",
      "0.3764392 No improvement since epoch  92 ; best_mse,best_ci: 0.37419024 91 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 96 [0/25046 (0%)]\tLoss: 0.366580\n",
      "Train epoch: 96 [661320/25046 (80%)]\tLoss: 0.312548\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  96 ; best_mse,best_ci: 0.37131524 95 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 97 [0/25046 (0%)]\tLoss: 0.325571\n",
      "Train epoch: 97 [660420/25046 (80%)]\tLoss: 0.328730\n",
      "Make prediction for 5010 samples...\n",
      "0.37400937 No improvement since epoch  96 ; best_mse,best_ci: 0.37131524 95 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 98 [0/25046 (0%)]\tLoss: 0.353146\n",
      "Train epoch: 98 [656040/25046 (80%)]\tLoss: 0.353001\n",
      "Make prediction for 5010 samples...\n",
      "0.38865247 No improvement since epoch  96 ; best_mse,best_ci: 0.37131524 95 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 99 [0/25046 (0%)]\tLoss: 0.315055\n",
      "Train epoch: 99 [658420/25046 (80%)]\tLoss: 0.300446\n",
      "Make prediction for 5010 samples...\n",
      "0.39796734 No improvement since epoch  96 ; best_mse,best_ci: 0.37131524 95 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 100 [0/25046 (0%)]\tLoss: 0.392309\n",
      "Train epoch: 100 [656940/25046 (80%)]\tLoss: 0.309942\n",
      "Make prediction for 5010 samples...\n",
      "0.37259367 No improvement since epoch  96 ; best_mse,best_ci: 0.37131524 95 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 101 [0/25046 (0%)]\tLoss: 0.331709\n",
      "Train epoch: 101 [651940/25046 (80%)]\tLoss: 0.350091\n",
      "Make prediction for 5010 samples...\n",
      "0.41140828 No improvement since epoch  96 ; best_mse,best_ci: 0.37131524 95 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 102 [0/25046 (0%)]\tLoss: 0.337858\n",
      "Train epoch: 102 [650420/25046 (80%)]\tLoss: 0.326392\n",
      "Make prediction for 5010 samples...\n",
      "0.37169275 No improvement since epoch  96 ; best_mse,best_ci: 0.37131524 95 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 103 [0/25046 (0%)]\tLoss: 0.352207\n",
      "Train epoch: 103 [650040/25046 (80%)]\tLoss: 0.385823\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  103 ; best_mse,best_ci: 0.3584216 102 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 104 [0/25046 (0%)]\tLoss: 0.343011\n",
      "Train epoch: 104 [652220/25046 (80%)]\tLoss: 0.338295\n",
      "Make prediction for 5010 samples...\n",
      "0.3639252 No improvement since epoch  103 ; best_mse,best_ci: 0.3584216 102 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 105 [0/25046 (0%)]\tLoss: 0.290047\n",
      "Train epoch: 105 [659680/25046 (80%)]\tLoss: 0.365973\n",
      "Make prediction for 5010 samples...\n",
      "0.387894 No improvement since epoch  103 ; best_mse,best_ci: 0.3584216 102 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 106 [0/25046 (0%)]\tLoss: 0.350341\n",
      "Train epoch: 106 [651940/25046 (80%)]\tLoss: 0.393485\n",
      "Make prediction for 5010 samples...\n",
      "0.36022678 No improvement since epoch  103 ; best_mse,best_ci: 0.3584216 102 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 107 [0/25046 (0%)]\tLoss: 0.376975\n",
      "Train epoch: 107 [659680/25046 (80%)]\tLoss: 0.368683\n",
      "Make prediction for 5010 samples...\n",
      "0.41258836 No improvement since epoch  103 ; best_mse,best_ci: 0.3584216 102 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 108 [0/25046 (0%)]\tLoss: 0.359049\n",
      "Train epoch: 108 [657160/25046 (80%)]\tLoss: 0.332453\n",
      "Make prediction for 5010 samples...\n",
      "0.38734204 No improvement since epoch  103 ; best_mse,best_ci: 0.3584216 102 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 109 [0/25046 (0%)]\tLoss: 0.366139\n",
      "Train epoch: 109 [652260/25046 (80%)]\tLoss: 0.341083\n",
      "Make prediction for 5010 samples...\n",
      "0.36120066 No improvement since epoch  103 ; best_mse,best_ci: 0.3584216 102 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 110 [0/25046 (0%)]\tLoss: 0.350770\n",
      "Train epoch: 110 [656580/25046 (80%)]\tLoss: 0.321104\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  110 ; best_mse,best_ci: 0.35791078 109 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 111 [0/25046 (0%)]\tLoss: 0.303310\n",
      "Train epoch: 111 [654940/25046 (80%)]\tLoss: 0.319523\n",
      "Make prediction for 5010 samples...\n",
      "0.40400577 No improvement since epoch  110 ; best_mse,best_ci: 0.35791078 109 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 112 [0/25046 (0%)]\tLoss: 0.347237\n",
      "Train epoch: 112 [659340/25046 (80%)]\tLoss: 0.357428\n",
      "Make prediction for 5010 samples...\n",
      "0.3692779 No improvement since epoch  110 ; best_mse,best_ci: 0.35791078 109 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 113 [0/25046 (0%)]\tLoss: 0.340471\n",
      "Train epoch: 113 [652660/25046 (80%)]\tLoss: 0.360578\n",
      "Make prediction for 5010 samples...\n",
      "0.3797671 No improvement since epoch  110 ; best_mse,best_ci: 0.35791078 109 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 114 [0/25046 (0%)]\tLoss: 0.363441\n",
      "Train epoch: 114 [661800/25046 (80%)]\tLoss: 0.325634\n",
      "Make prediction for 5010 samples...\n",
      "0.36439598 No improvement since epoch  110 ; best_mse,best_ci: 0.35791078 109 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 115 [0/25046 (0%)]\tLoss: 0.307442\n",
      "Train epoch: 115 [653800/25046 (80%)]\tLoss: 0.393655\n",
      "Make prediction for 5010 samples...\n",
      "0.36973786 No improvement since epoch  110 ; best_mse,best_ci: 0.35791078 109 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 116 [0/25046 (0%)]\tLoss: 0.342527\n",
      "Train epoch: 116 [656480/25046 (80%)]\tLoss: 0.279147\n",
      "Make prediction for 5010 samples...\n",
      "0.36836693 No improvement since epoch  110 ; best_mse,best_ci: 0.35791078 109 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 117 [0/25046 (0%)]\tLoss: 0.303310\n",
      "Train epoch: 117 [656740/25046 (80%)]\tLoss: 0.351585\n",
      "Make prediction for 5010 samples...\n",
      "0.37247172 No improvement since epoch  110 ; best_mse,best_ci: 0.35791078 109 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 118 [0/25046 (0%)]\tLoss: 0.335172\n",
      "Train epoch: 118 [655160/25046 (80%)]\tLoss: 0.345979\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  118 ; best_mse,best_ci: 0.35381922 117 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 119 [0/25046 (0%)]\tLoss: 0.312229\n",
      "Train epoch: 119 [655840/25046 (80%)]\tLoss: 0.354323\n",
      "Make prediction for 5010 samples...\n",
      "0.39703426 No improvement since epoch  118 ; best_mse,best_ci: 0.35381922 117 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 120 [0/25046 (0%)]\tLoss: 0.334252\n",
      "Train epoch: 120 [662480/25046 (80%)]\tLoss: 0.291845\n",
      "Make prediction for 5010 samples...\n",
      "0.35676485 No improvement since epoch  118 ; best_mse,best_ci: 0.35381922 117 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 121 [0/25046 (0%)]\tLoss: 0.277592\n",
      "Train epoch: 121 [655360/25046 (80%)]\tLoss: 0.336579\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  121 ; best_mse,best_ci: 0.35087717 120 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 122 [0/25046 (0%)]\tLoss: 0.308396\n",
      "Train epoch: 122 [655540/25046 (80%)]\tLoss: 0.340777\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 123 [0/25046 (0%)]\tLoss: 0.320968\n",
      "Train epoch: 123 [661800/25046 (80%)]\tLoss: 0.336243\n",
      "Make prediction for 5010 samples...\n",
      "0.35567284 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 124 [0/25046 (0%)]\tLoss: 0.300057\n",
      "Train epoch: 124 [659900/25046 (80%)]\tLoss: 0.292035\n",
      "Make prediction for 5010 samples...\n",
      "0.35309923 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 125 [0/25046 (0%)]\tLoss: 0.282594\n",
      "Train epoch: 125 [661600/25046 (80%)]\tLoss: 0.346367\n",
      "Make prediction for 5010 samples...\n",
      "0.3649511 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 126 [0/25046 (0%)]\tLoss: 0.349821\n",
      "Train epoch: 126 [653200/25046 (80%)]\tLoss: 0.400490\n",
      "Make prediction for 5010 samples...\n",
      "0.4074605 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 127 [0/25046 (0%)]\tLoss: 0.405497\n",
      "Train epoch: 127 [652800/25046 (80%)]\tLoss: 0.350952\n",
      "Make prediction for 5010 samples...\n",
      "0.4046582 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 128 [0/25046 (0%)]\tLoss: 0.307324\n",
      "Train epoch: 128 [660880/25046 (80%)]\tLoss: 0.292917\n",
      "Make prediction for 5010 samples...\n",
      "0.39087865 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 129 [0/25046 (0%)]\tLoss: 0.313817\n",
      "Train epoch: 129 [655920/25046 (80%)]\tLoss: 0.336080\n",
      "Make prediction for 5010 samples...\n",
      "0.38751945 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 130 [0/25046 (0%)]\tLoss: 0.300301\n",
      "Train epoch: 130 [655720/25046 (80%)]\tLoss: 0.311690\n",
      "Make prediction for 5010 samples...\n",
      "0.36280844 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 131 [0/25046 (0%)]\tLoss: 0.312405\n",
      "Train epoch: 131 [657340/25046 (80%)]\tLoss: 0.336836\n",
      "Make prediction for 5010 samples...\n",
      "0.35821912 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 132 [0/25046 (0%)]\tLoss: 0.347332\n",
      "Train epoch: 132 [654700/25046 (80%)]\tLoss: 0.299712\n",
      "Make prediction for 5010 samples...\n",
      "0.3589302 No improvement since epoch  122 ; best_mse,best_ci: 0.34934184 121 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 133 [0/25046 (0%)]\tLoss: 0.317594\n",
      "Train epoch: 133 [659820/25046 (80%)]\tLoss: 0.282705\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  133 ; best_mse,best_ci: 0.34577492 132 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 134 [0/25046 (0%)]\tLoss: 0.263854\n",
      "Train epoch: 134 [659000/25046 (80%)]\tLoss: 0.233382\n",
      "Make prediction for 5010 samples...\n",
      "0.35812044 No improvement since epoch  133 ; best_mse,best_ci: 0.34577492 132 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 135 [0/25046 (0%)]\tLoss: 0.272090\n",
      "Train epoch: 135 [662380/25046 (80%)]\tLoss: 0.274466\n",
      "Make prediction for 5010 samples...\n",
      "0.35321492 No improvement since epoch  133 ; best_mse,best_ci: 0.34577492 132 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 136 [0/25046 (0%)]\tLoss: 0.310306\n",
      "Train epoch: 136 [660340/25046 (80%)]\tLoss: 0.335779\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 137 [0/25046 (0%)]\tLoss: 0.274311\n",
      "Train epoch: 137 [655440/25046 (80%)]\tLoss: 0.275673\n",
      "Make prediction for 5010 samples...\n",
      "0.35317197 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 138 [0/25046 (0%)]\tLoss: 0.297472\n",
      "Train epoch: 138 [665400/25046 (80%)]\tLoss: 0.327026\n",
      "Make prediction for 5010 samples...\n",
      "0.35142562 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 139 [0/25046 (0%)]\tLoss: 0.271669\n",
      "Train epoch: 139 [654060/25046 (80%)]\tLoss: 0.289308\n",
      "Make prediction for 5010 samples...\n",
      "0.41630203 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 140 [0/25046 (0%)]\tLoss: 0.326502\n",
      "Train epoch: 140 [656620/25046 (80%)]\tLoss: 0.318344\n",
      "Make prediction for 5010 samples...\n",
      "0.4204614 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 141 [0/25046 (0%)]\tLoss: 0.330864\n",
      "Train epoch: 141 [654640/25046 (80%)]\tLoss: 0.311626\n",
      "Make prediction for 5010 samples...\n",
      "0.35121208 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 142 [0/25046 (0%)]\tLoss: 0.306106\n",
      "Train epoch: 142 [655600/25046 (80%)]\tLoss: 0.315782\n",
      "Make prediction for 5010 samples...\n",
      "0.34212226 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 143 [0/25046 (0%)]\tLoss: 0.270622\n",
      "Train epoch: 143 [652380/25046 (80%)]\tLoss: 0.307942\n",
      "Make prediction for 5010 samples...\n",
      "0.359557 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 144 [0/25046 (0%)]\tLoss: 0.284597\n",
      "Train epoch: 144 [659760/25046 (80%)]\tLoss: 0.258966\n",
      "Make prediction for 5010 samples...\n",
      "0.3432575 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 145 [0/25046 (0%)]\tLoss: 0.277718\n",
      "Train epoch: 145 [662420/25046 (80%)]\tLoss: 0.286011\n",
      "Make prediction for 5010 samples...\n",
      "0.36399344 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 146 [0/25046 (0%)]\tLoss: 0.316241\n",
      "Train epoch: 146 [664460/25046 (80%)]\tLoss: 0.305292\n",
      "Make prediction for 5010 samples...\n",
      "0.3433081 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 147 [0/25046 (0%)]\tLoss: 0.283947\n",
      "Train epoch: 147 [658220/25046 (80%)]\tLoss: 0.287164\n",
      "Make prediction for 5010 samples...\n",
      "0.34346464 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 148 [0/25046 (0%)]\tLoss: 0.291974\n",
      "Train epoch: 148 [651780/25046 (80%)]\tLoss: 0.308735\n",
      "Make prediction for 5010 samples...\n",
      "0.34805432 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 149 [0/25046 (0%)]\tLoss: 0.266474\n",
      "Train epoch: 149 [651560/25046 (80%)]\tLoss: 0.292768\n",
      "Make prediction for 5010 samples...\n",
      "0.34625626 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 150 [0/25046 (0%)]\tLoss: 0.258695\n",
      "Train epoch: 150 [656260/25046 (80%)]\tLoss: 0.322703\n",
      "Make prediction for 5010 samples...\n",
      "0.37798858 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 151 [0/25046 (0%)]\tLoss: 0.260891\n",
      "Train epoch: 151 [671740/25046 (80%)]\tLoss: 0.296475\n",
      "Make prediction for 5010 samples...\n",
      "0.35470673 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 152 [0/25046 (0%)]\tLoss: 0.222383\n",
      "Train epoch: 152 [654360/25046 (80%)]\tLoss: 0.337375\n",
      "Make prediction for 5010 samples...\n",
      "0.3432324 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 153 [0/25046 (0%)]\tLoss: 0.314872\n",
      "Train epoch: 153 [662760/25046 (80%)]\tLoss: 0.289628\n",
      "Make prediction for 5010 samples...\n",
      "0.35897 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 154 [0/25046 (0%)]\tLoss: 0.322865\n",
      "Train epoch: 154 [653660/25046 (80%)]\tLoss: 0.319393\n",
      "Make prediction for 5010 samples...\n",
      "0.40060398 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 155 [0/25046 (0%)]\tLoss: 0.332886\n",
      "Train epoch: 155 [652780/25046 (80%)]\tLoss: 0.299918\n",
      "Make prediction for 5010 samples...\n",
      "0.35820574 No improvement since epoch  136 ; best_mse,best_ci: 0.3370165 135 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 156 [0/25046 (0%)]\tLoss: 0.231768\n",
      "Train epoch: 156 [654420/25046 (80%)]\tLoss: 0.248882\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  156 ; best_mse,best_ci: 0.33363572 155 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 157 [0/25046 (0%)]\tLoss: 0.277500\n",
      "Train epoch: 157 [661600/25046 (80%)]\tLoss: 0.267162\n",
      "Make prediction for 5010 samples...\n",
      "0.34249967 No improvement since epoch  156 ; best_mse,best_ci: 0.33363572 155 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 158 [0/25046 (0%)]\tLoss: 0.297956\n",
      "Train epoch: 158 [654060/25046 (80%)]\tLoss: 0.256670\n",
      "Make prediction for 5010 samples...\n",
      "0.33849528 No improvement since epoch  156 ; best_mse,best_ci: 0.33363572 155 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 159 [0/25046 (0%)]\tLoss: 0.285666\n",
      "Train epoch: 159 [647720/25046 (80%)]\tLoss: 0.265453\n",
      "Make prediction for 5010 samples...\n",
      "0.3360774 No improvement since epoch  156 ; best_mse,best_ci: 0.33363572 155 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 160 [0/25046 (0%)]\tLoss: 0.257608\n",
      "Train epoch: 160 [656900/25046 (80%)]\tLoss: 0.288415\n",
      "Make prediction for 5010 samples...\n",
      "0.37665337 No improvement since epoch  156 ; best_mse,best_ci: 0.33363572 155 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 161 [0/25046 (0%)]\tLoss: 0.247204\n",
      "Train epoch: 161 [650500/25046 (80%)]\tLoss: 0.271978\n",
      "Make prediction for 5010 samples...\n",
      "0.37281352 No improvement since epoch  156 ; best_mse,best_ci: 0.33363572 155 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 162 [0/25046 (0%)]\tLoss: 0.263468\n",
      "Train epoch: 162 [655020/25046 (80%)]\tLoss: 0.283055\n",
      "Make prediction for 5010 samples...\n",
      "0.35023186 No improvement since epoch  156 ; best_mse,best_ci: 0.33363572 155 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 163 [0/25046 (0%)]\tLoss: 0.277964\n",
      "Train epoch: 163 [655400/25046 (80%)]\tLoss: 0.313080\n",
      "Make prediction for 5010 samples...\n",
      "0.35257205 No improvement since epoch  156 ; best_mse,best_ci: 0.33363572 155 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 164 [0/25046 (0%)]\tLoss: 0.238151\n",
      "Train epoch: 164 [656880/25046 (80%)]\tLoss: 0.218060\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  164 ; best_mse,best_ci: 0.3312275 163 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 165 [0/25046 (0%)]\tLoss: 0.246039\n",
      "Train epoch: 165 [660520/25046 (80%)]\tLoss: 0.244805\n",
      "Make prediction for 5010 samples...\n",
      "0.35326636 No improvement since epoch  164 ; best_mse,best_ci: 0.3312275 163 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 166 [0/25046 (0%)]\tLoss: 0.279888\n",
      "Train epoch: 166 [657940/25046 (80%)]\tLoss: 0.256311\n",
      "Make prediction for 5010 samples...\n",
      "0.34416133 No improvement since epoch  164 ; best_mse,best_ci: 0.3312275 163 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 167 [0/25046 (0%)]\tLoss: 0.226671\n",
      "Train epoch: 167 [659220/25046 (80%)]\tLoss: 0.251156\n",
      "Make prediction for 5010 samples...\n",
      "0.36530542 No improvement since epoch  164 ; best_mse,best_ci: 0.3312275 163 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 168 [0/25046 (0%)]\tLoss: 0.297975\n",
      "Train epoch: 168 [653700/25046 (80%)]\tLoss: 0.275530\n",
      "Make prediction for 5010 samples...\n",
      "0.34007072 No improvement since epoch  164 ; best_mse,best_ci: 0.3312275 163 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 169 [0/25046 (0%)]\tLoss: 0.245178\n",
      "Train epoch: 169 [653800/25046 (80%)]\tLoss: 0.267931\n",
      "Make prediction for 5010 samples...\n",
      "0.33222184 No improvement since epoch  164 ; best_mse,best_ci: 0.3312275 163 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 170 [0/25046 (0%)]\tLoss: 0.285745\n",
      "Train epoch: 170 [661960/25046 (80%)]\tLoss: 0.292501\n",
      "Make prediction for 5010 samples...\n",
      "0.33561355 No improvement since epoch  164 ; best_mse,best_ci: 0.3312275 163 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 171 [0/25046 (0%)]\tLoss: 0.245352\n",
      "Train epoch: 171 [654060/25046 (80%)]\tLoss: 0.252304\n",
      "Make prediction for 5010 samples...\n",
      "0.33746755 No improvement since epoch  164 ; best_mse,best_ci: 0.3312275 163 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 172 [0/25046 (0%)]\tLoss: 0.265638\n",
      "Train epoch: 172 [655300/25046 (80%)]\tLoss: 0.225032\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  172 ; best_mse,best_ci: 0.3306155 171 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 173 [0/25046 (0%)]\tLoss: 0.273005\n",
      "Train epoch: 173 [653780/25046 (80%)]\tLoss: 0.255580\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 174 [0/25046 (0%)]\tLoss: 0.235185\n",
      "Train epoch: 174 [665420/25046 (80%)]\tLoss: 0.241802\n",
      "Make prediction for 5010 samples...\n",
      "0.32898936 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 175 [0/25046 (0%)]\tLoss: 0.244202\n",
      "Train epoch: 175 [655860/25046 (80%)]\tLoss: 0.271577\n",
      "Make prediction for 5010 samples...\n",
      "0.331699 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 176 [0/25046 (0%)]\tLoss: 0.235603\n",
      "Train epoch: 176 [659080/25046 (80%)]\tLoss: 0.305511\n",
      "Make prediction for 5010 samples...\n",
      "0.37875542 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 177 [0/25046 (0%)]\tLoss: 0.277553\n",
      "Train epoch: 177 [652580/25046 (80%)]\tLoss: 0.244126\n",
      "Make prediction for 5010 samples...\n",
      "0.33253777 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 178 [0/25046 (0%)]\tLoss: 0.230381\n",
      "Train epoch: 178 [654080/25046 (80%)]\tLoss: 0.233988\n",
      "Make prediction for 5010 samples...\n",
      "0.34162158 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 179 [0/25046 (0%)]\tLoss: 0.230419\n",
      "Train epoch: 179 [665600/25046 (80%)]\tLoss: 0.219051\n",
      "Make prediction for 5010 samples...\n",
      "0.36419833 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 180 [0/25046 (0%)]\tLoss: 0.264489\n",
      "Train epoch: 180 [656500/25046 (80%)]\tLoss: 0.250927\n",
      "Make prediction for 5010 samples...\n",
      "0.38469636 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 181 [0/25046 (0%)]\tLoss: 0.258723\n",
      "Train epoch: 181 [653080/25046 (80%)]\tLoss: 0.289509\n",
      "Make prediction for 5010 samples...\n",
      "0.35790256 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 182 [0/25046 (0%)]\tLoss: 0.234659\n",
      "Train epoch: 182 [658400/25046 (80%)]\tLoss: 0.251765\n",
      "Make prediction for 5010 samples...\n",
      "0.33786097 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 183 [0/25046 (0%)]\tLoss: 0.232691\n",
      "Train epoch: 183 [654460/25046 (80%)]\tLoss: 0.289019\n",
      "Make prediction for 5010 samples...\n",
      "0.34491414 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 184 [0/25046 (0%)]\tLoss: 0.261823\n",
      "Train epoch: 184 [658960/25046 (80%)]\tLoss: 0.258431\n",
      "Make prediction for 5010 samples...\n",
      "0.3921582 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 185 [0/25046 (0%)]\tLoss: 0.248947\n",
      "Train epoch: 185 [655960/25046 (80%)]\tLoss: 0.245274\n",
      "Make prediction for 5010 samples...\n",
      "0.3378455 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 186 [0/25046 (0%)]\tLoss: 0.253301\n",
      "Train epoch: 186 [656760/25046 (80%)]\tLoss: 0.237369\n",
      "Make prediction for 5010 samples...\n",
      "0.33829564 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 187 [0/25046 (0%)]\tLoss: 0.274884\n",
      "Train epoch: 187 [659680/25046 (80%)]\tLoss: 0.250340\n",
      "Make prediction for 5010 samples...\n",
      "0.35100192 No improvement since epoch  173 ; best_mse,best_ci: 0.3270346 172 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 188 [0/25046 (0%)]\tLoss: 0.242805\n",
      "Train epoch: 188 [650400/25046 (80%)]\tLoss: 0.244104\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  188 ; best_mse,best_ci: 0.32666644 187 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 189 [0/25046 (0%)]\tLoss: 0.221149\n",
      "Train epoch: 189 [648240/25046 (80%)]\tLoss: 0.240477\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 190 [0/25046 (0%)]\tLoss: 0.236773\n",
      "Train epoch: 190 [658580/25046 (80%)]\tLoss: 0.226473\n",
      "Make prediction for 5010 samples...\n",
      "0.3499288 No improvement since epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 191 [0/25046 (0%)]\tLoss: 0.225746\n",
      "Train epoch: 191 [656240/25046 (80%)]\tLoss: 0.255120\n",
      "Make prediction for 5010 samples...\n",
      "0.32919326 No improvement since epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 192 [0/25046 (0%)]\tLoss: 0.218677\n",
      "Train epoch: 192 [652320/25046 (80%)]\tLoss: 0.207483\n",
      "Make prediction for 5010 samples...\n",
      "0.3756732 No improvement since epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 193 [0/25046 (0%)]\tLoss: 0.224575\n",
      "Train epoch: 193 [663200/25046 (80%)]\tLoss: 0.342454\n",
      "Make prediction for 5010 samples...\n",
      "0.42791915 No improvement since epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 194 [0/25046 (0%)]\tLoss: 0.405791\n",
      "Train epoch: 194 [658960/25046 (80%)]\tLoss: 0.271408\n",
      "Make prediction for 5010 samples...\n",
      "0.35624278 No improvement since epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 195 [0/25046 (0%)]\tLoss: 0.286535\n",
      "Train epoch: 195 [659520/25046 (80%)]\tLoss: 0.225778\n",
      "Make prediction for 5010 samples...\n",
      "0.37149668 No improvement since epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 196 [0/25046 (0%)]\tLoss: 0.218329\n",
      "Train epoch: 196 [659560/25046 (80%)]\tLoss: 0.253413\n",
      "Make prediction for 5010 samples...\n",
      "0.33810535 No improvement since epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 197 [0/25046 (0%)]\tLoss: 0.219346\n",
      "Train epoch: 197 [655340/25046 (80%)]\tLoss: 0.206406\n",
      "Make prediction for 5010 samples...\n",
      "0.38629928 No improvement since epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 198 [0/25046 (0%)]\tLoss: 0.275072\n",
      "Train epoch: 198 [663460/25046 (80%)]\tLoss: 0.225794\n",
      "Make prediction for 5010 samples...\n",
      "0.33251175 No improvement since epoch  189 ; best_mse,best_ci: 0.32420817 188 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 199 [0/25046 (0%)]\tLoss: 0.225126\n",
      "Train epoch: 199 [658160/25046 (80%)]\tLoss: 0.237026\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 200 [0/25046 (0%)]\tLoss: 0.252708\n",
      "Train epoch: 200 [659140/25046 (80%)]\tLoss: 0.282969\n",
      "Make prediction for 5010 samples...\n",
      "0.3311045 No improvement since epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 201 [0/25046 (0%)]\tLoss: 0.193854\n",
      "Train epoch: 201 [661700/25046 (80%)]\tLoss: 0.239191\n",
      "Make prediction for 5010 samples...\n",
      "0.34524837 No improvement since epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 202 [0/25046 (0%)]\tLoss: 0.207961\n",
      "Train epoch: 202 [655320/25046 (80%)]\tLoss: 0.225082\n",
      "Make prediction for 5010 samples...\n",
      "0.32641047 No improvement since epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 203 [0/25046 (0%)]\tLoss: 0.214707\n",
      "Train epoch: 203 [652500/25046 (80%)]\tLoss: 0.211775\n",
      "Make prediction for 5010 samples...\n",
      "0.3453653 No improvement since epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 204 [0/25046 (0%)]\tLoss: 0.258176\n",
      "Train epoch: 204 [657940/25046 (80%)]\tLoss: 0.214624\n",
      "Make prediction for 5010 samples...\n",
      "0.34315175 No improvement since epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 205 [0/25046 (0%)]\tLoss: 0.311840\n",
      "Train epoch: 205 [658500/25046 (80%)]\tLoss: 0.224332\n",
      "Make prediction for 5010 samples...\n",
      "0.3853692 No improvement since epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 206 [0/25046 (0%)]\tLoss: 0.228675\n",
      "Train epoch: 206 [656680/25046 (80%)]\tLoss: 0.248245\n",
      "Make prediction for 5010 samples...\n",
      "0.33210382 No improvement since epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 207 [0/25046 (0%)]\tLoss: 0.243759\n",
      "Train epoch: 207 [654940/25046 (80%)]\tLoss: 0.226073\n",
      "Make prediction for 5010 samples...\n",
      "0.33458644 No improvement since epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 208 [0/25046 (0%)]\tLoss: 0.235640\n",
      "Train epoch: 208 [653460/25046 (80%)]\tLoss: 0.239312\n",
      "Make prediction for 5010 samples...\n",
      "0.33524305 No improvement since epoch  199 ; best_mse,best_ci: 0.32297662 198 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 209 [0/25046 (0%)]\tLoss: 0.219168\n",
      "Train epoch: 209 [652640/25046 (80%)]\tLoss: 0.265039\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 210 [0/25046 (0%)]\tLoss: 0.190667\n",
      "Train epoch: 210 [653900/25046 (80%)]\tLoss: 0.256697\n",
      "Make prediction for 5010 samples...\n",
      "0.3220063 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 211 [0/25046 (0%)]\tLoss: 0.214454\n",
      "Train epoch: 211 [653560/25046 (80%)]\tLoss: 0.230181\n",
      "Make prediction for 5010 samples...\n",
      "0.33399498 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 212 [0/25046 (0%)]\tLoss: 0.265398\n",
      "Train epoch: 212 [658840/25046 (80%)]\tLoss: 0.214009\n",
      "Make prediction for 5010 samples...\n",
      "0.33738506 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 213 [0/25046 (0%)]\tLoss: 0.227825\n",
      "Train epoch: 213 [654920/25046 (80%)]\tLoss: 0.263491\n",
      "Make prediction for 5010 samples...\n",
      "0.33799902 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 214 [0/25046 (0%)]\tLoss: 0.202654\n",
      "Train epoch: 214 [656000/25046 (80%)]\tLoss: 0.224050\n",
      "Make prediction for 5010 samples...\n",
      "0.3402804 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 215 [0/25046 (0%)]\tLoss: 0.206013\n",
      "Train epoch: 215 [650780/25046 (80%)]\tLoss: 0.202191\n",
      "Make prediction for 5010 samples...\n",
      "0.32130772 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 216 [0/25046 (0%)]\tLoss: 0.221102\n",
      "Train epoch: 216 [656820/25046 (80%)]\tLoss: 0.244219\n",
      "Make prediction for 5010 samples...\n",
      "0.37320355 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 217 [0/25046 (0%)]\tLoss: 0.234779\n",
      "Train epoch: 217 [657620/25046 (80%)]\tLoss: 0.215548\n",
      "Make prediction for 5010 samples...\n",
      "0.32354763 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 218 [0/25046 (0%)]\tLoss: 0.204243\n",
      "Train epoch: 218 [660900/25046 (80%)]\tLoss: 0.252333\n",
      "Make prediction for 5010 samples...\n",
      "0.32707724 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 219 [0/25046 (0%)]\tLoss: 0.218867\n",
      "Train epoch: 219 [655580/25046 (80%)]\tLoss: 0.204412\n",
      "Make prediction for 5010 samples...\n",
      "0.33958468 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 220 [0/25046 (0%)]\tLoss: 0.191401\n",
      "Train epoch: 220 [653420/25046 (80%)]\tLoss: 0.239084\n",
      "Make prediction for 5010 samples...\n",
      "0.32800764 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 221 [0/25046 (0%)]\tLoss: 0.174993\n",
      "Train epoch: 221 [656160/25046 (80%)]\tLoss: 0.229443\n",
      "Make prediction for 5010 samples...\n",
      "0.37438223 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 222 [0/25046 (0%)]\tLoss: 0.224342\n",
      "Train epoch: 222 [655900/25046 (80%)]\tLoss: 0.242659\n",
      "Make prediction for 5010 samples...\n",
      "0.39963835 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 223 [0/25046 (0%)]\tLoss: 0.222519\n",
      "Train epoch: 223 [657900/25046 (80%)]\tLoss: 0.218447\n",
      "Make prediction for 5010 samples...\n",
      "0.32227424 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 224 [0/25046 (0%)]\tLoss: 0.200216\n",
      "Train epoch: 224 [658540/25046 (80%)]\tLoss: 0.218722\n",
      "Make prediction for 5010 samples...\n",
      "0.32480955 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 225 [0/25046 (0%)]\tLoss: 0.181515\n",
      "Train epoch: 225 [653920/25046 (80%)]\tLoss: 0.208356\n",
      "Make prediction for 5010 samples...\n",
      "0.38370764 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 226 [0/25046 (0%)]\tLoss: 0.230957\n",
      "Train epoch: 226 [657940/25046 (80%)]\tLoss: 0.214384\n",
      "Make prediction for 5010 samples...\n",
      "0.3820007 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 227 [0/25046 (0%)]\tLoss: 0.254495\n",
      "Train epoch: 227 [652420/25046 (80%)]\tLoss: 0.243743\n",
      "Make prediction for 5010 samples...\n",
      "0.35774776 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 228 [0/25046 (0%)]\tLoss: 0.218764\n",
      "Train epoch: 228 [659260/25046 (80%)]\tLoss: 0.230005\n",
      "Make prediction for 5010 samples...\n",
      "0.31706464 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 229 [0/25046 (0%)]\tLoss: 0.203927\n",
      "Train epoch: 229 [657500/25046 (80%)]\tLoss: 0.198994\n",
      "Make prediction for 5010 samples...\n",
      "0.33846885 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 230 [0/25046 (0%)]\tLoss: 0.220930\n",
      "Train epoch: 230 [653800/25046 (80%)]\tLoss: 0.278070\n",
      "Make prediction for 5010 samples...\n",
      "0.35678792 No improvement since epoch  209 ; best_mse,best_ci: 0.3167604 208 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 231 [0/25046 (0%)]\tLoss: 0.280717\n",
      "Train epoch: 231 [651220/25046 (80%)]\tLoss: 0.230533\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 232 [0/25046 (0%)]\tLoss: 0.204291\n",
      "Train epoch: 232 [658340/25046 (80%)]\tLoss: 0.215213\n",
      "Make prediction for 5010 samples...\n",
      "0.3132195 No improvement since epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 233 [0/25046 (0%)]\tLoss: 0.201132\n",
      "Train epoch: 233 [655580/25046 (80%)]\tLoss: 0.234598\n",
      "Make prediction for 5010 samples...\n",
      "0.35326025 No improvement since epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 234 [0/25046 (0%)]\tLoss: 0.191387\n",
      "Train epoch: 234 [664500/25046 (80%)]\tLoss: 0.229425\n",
      "Make prediction for 5010 samples...\n",
      "0.32909408 No improvement since epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 235 [0/25046 (0%)]\tLoss: 0.210669\n",
      "Train epoch: 235 [647380/25046 (80%)]\tLoss: 0.206588\n",
      "Make prediction for 5010 samples...\n",
      "0.3279347 No improvement since epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 236 [0/25046 (0%)]\tLoss: 0.200295\n",
      "Train epoch: 236 [656020/25046 (80%)]\tLoss: 0.234744\n",
      "Make prediction for 5010 samples...\n",
      "0.31669694 No improvement since epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 237 [0/25046 (0%)]\tLoss: 0.227873\n",
      "Train epoch: 237 [661640/25046 (80%)]\tLoss: 0.204271\n",
      "Make prediction for 5010 samples...\n",
      "0.38193554 No improvement since epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 238 [0/25046 (0%)]\tLoss: 0.184316\n",
      "Train epoch: 238 [657740/25046 (80%)]\tLoss: 0.218045\n",
      "Make prediction for 5010 samples...\n",
      "0.34101474 No improvement since epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 239 [0/25046 (0%)]\tLoss: 0.192151\n",
      "Train epoch: 239 [655140/25046 (80%)]\tLoss: 0.213965\n",
      "Make prediction for 5010 samples...\n",
      "0.3422408 No improvement since epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 240 [0/25046 (0%)]\tLoss: 0.180880\n",
      "Train epoch: 240 [653100/25046 (80%)]\tLoss: 0.207367\n",
      "Make prediction for 5010 samples...\n",
      "0.31860927 No improvement since epoch  231 ; best_mse,best_ci: 0.3127447 230 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 241 [0/25046 (0%)]\tLoss: 0.194284\n",
      "Train epoch: 241 [658360/25046 (80%)]\tLoss: 0.219003\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 242 [0/25046 (0%)]\tLoss: 0.185990\n",
      "Train epoch: 242 [665300/25046 (80%)]\tLoss: 0.183333\n",
      "Make prediction for 5010 samples...\n",
      "0.31473312 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 243 [0/25046 (0%)]\tLoss: 0.218695\n",
      "Train epoch: 243 [654660/25046 (80%)]\tLoss: 0.189565\n",
      "Make prediction for 5010 samples...\n",
      "0.3345783 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 244 [0/25046 (0%)]\tLoss: 0.205123\n",
      "Train epoch: 244 [651720/25046 (80%)]\tLoss: 0.219776\n",
      "Make prediction for 5010 samples...\n",
      "0.3429341 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 245 [0/25046 (0%)]\tLoss: 0.179275\n",
      "Train epoch: 245 [659300/25046 (80%)]\tLoss: 0.197633\n",
      "Make prediction for 5010 samples...\n",
      "0.36698532 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 246 [0/25046 (0%)]\tLoss: 0.217076\n",
      "Train epoch: 246 [657860/25046 (80%)]\tLoss: 0.207408\n",
      "Make prediction for 5010 samples...\n",
      "0.33458143 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 247 [0/25046 (0%)]\tLoss: 0.184557\n",
      "Train epoch: 247 [656660/25046 (80%)]\tLoss: 0.249402\n",
      "Make prediction for 5010 samples...\n",
      "0.32444507 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 248 [0/25046 (0%)]\tLoss: 0.236175\n",
      "Train epoch: 248 [657540/25046 (80%)]\tLoss: 0.215351\n",
      "Make prediction for 5010 samples...\n",
      "0.32544047 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 249 [0/25046 (0%)]\tLoss: 0.220702\n",
      "Train epoch: 249 [657960/25046 (80%)]\tLoss: 0.244357\n",
      "Make prediction for 5010 samples...\n",
      "0.3390587 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 250 [0/25046 (0%)]\tLoss: 0.191866\n",
      "Train epoch: 250 [648940/25046 (80%)]\tLoss: 0.204296\n",
      "Make prediction for 5010 samples...\n",
      "0.34560835 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 251 [0/25046 (0%)]\tLoss: 0.180520\n",
      "Train epoch: 251 [659660/25046 (80%)]\tLoss: 0.244934\n",
      "Make prediction for 5010 samples...\n",
      "0.34877205 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 252 [0/25046 (0%)]\tLoss: 0.185226\n",
      "Train epoch: 252 [661360/25046 (80%)]\tLoss: 0.216562\n",
      "Make prediction for 5010 samples...\n",
      "0.32356733 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 253 [0/25046 (0%)]\tLoss: 0.186645\n",
      "Train epoch: 253 [650860/25046 (80%)]\tLoss: 0.218715\n",
      "Make prediction for 5010 samples...\n",
      "0.3162085 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 254 [0/25046 (0%)]\tLoss: 0.258101\n",
      "Train epoch: 254 [659860/25046 (80%)]\tLoss: 0.185075\n",
      "Make prediction for 5010 samples...\n",
      "0.3126133 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 255 [0/25046 (0%)]\tLoss: 0.162781\n",
      "Train epoch: 255 [653860/25046 (80%)]\tLoss: 0.196239\n",
      "Make prediction for 5010 samples...\n",
      "0.4087591 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 256 [0/25046 (0%)]\tLoss: 0.221028\n",
      "Train epoch: 256 [655380/25046 (80%)]\tLoss: 0.250714\n",
      "Make prediction for 5010 samples...\n",
      "0.339554 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 257 [0/25046 (0%)]\tLoss: 0.191010\n",
      "Train epoch: 257 [659160/25046 (80%)]\tLoss: 0.258082\n",
      "Make prediction for 5010 samples...\n",
      "0.3668147 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 258 [0/25046 (0%)]\tLoss: 0.208493\n",
      "Train epoch: 258 [658060/25046 (80%)]\tLoss: 0.205549\n",
      "Make prediction for 5010 samples...\n",
      "0.3222164 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 259 [0/25046 (0%)]\tLoss: 0.159679\n",
      "Train epoch: 259 [655120/25046 (80%)]\tLoss: 0.199481\n",
      "Make prediction for 5010 samples...\n",
      "0.3281522 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 260 [0/25046 (0%)]\tLoss: 0.190932\n",
      "Train epoch: 260 [655220/25046 (80%)]\tLoss: 0.227224\n",
      "Make prediction for 5010 samples...\n",
      "0.31626436 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 261 [0/25046 (0%)]\tLoss: 0.192862\n",
      "Train epoch: 261 [657560/25046 (80%)]\tLoss: 0.191448\n",
      "Make prediction for 5010 samples...\n",
      "0.37160072 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 262 [0/25046 (0%)]\tLoss: 0.179185\n",
      "Train epoch: 262 [656460/25046 (80%)]\tLoss: 0.294284\n",
      "Make prediction for 5010 samples...\n",
      "0.32987872 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 263 [0/25046 (0%)]\tLoss: 0.220149\n",
      "Train epoch: 263 [659360/25046 (80%)]\tLoss: 0.210958\n",
      "Make prediction for 5010 samples...\n",
      "0.3640542 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 264 [0/25046 (0%)]\tLoss: 0.162742\n",
      "Train epoch: 264 [659320/25046 (80%)]\tLoss: 0.222595\n",
      "Make prediction for 5010 samples...\n",
      "0.32658395 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 265 [0/25046 (0%)]\tLoss: 0.215875\n",
      "Train epoch: 265 [662560/25046 (80%)]\tLoss: 0.202893\n",
      "Make prediction for 5010 samples...\n",
      "0.30945107 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 266 [0/25046 (0%)]\tLoss: 0.188492\n",
      "Train epoch: 266 [658980/25046 (80%)]\tLoss: 0.199492\n",
      "Make prediction for 5010 samples...\n",
      "0.31651163 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 267 [0/25046 (0%)]\tLoss: 0.199085\n",
      "Train epoch: 267 [655520/25046 (80%)]\tLoss: 0.186639\n",
      "Make prediction for 5010 samples...\n",
      "0.31208152 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 268 [0/25046 (0%)]\tLoss: 0.164082\n",
      "Train epoch: 268 [654640/25046 (80%)]\tLoss: 0.222843\n",
      "Make prediction for 5010 samples...\n",
      "0.35661325 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 269 [0/25046 (0%)]\tLoss: 0.188610\n",
      "Train epoch: 269 [662360/25046 (80%)]\tLoss: 0.181470\n",
      "Make prediction for 5010 samples...\n",
      "0.30990916 No improvement since epoch  241 ; best_mse,best_ci: 0.30775136 240 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 270 [0/25046 (0%)]\tLoss: 0.156669\n",
      "Train epoch: 270 [657780/25046 (80%)]\tLoss: 0.197276\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  270 ; best_mse,best_ci: 0.3060282 269 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 271 [0/25046 (0%)]\tLoss: 0.178453\n",
      "Train epoch: 271 [657060/25046 (80%)]\tLoss: 0.187083\n",
      "Make prediction for 5010 samples...\n",
      "0.3159743 No improvement since epoch  270 ; best_mse,best_ci: 0.3060282 269 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 272 [0/25046 (0%)]\tLoss: 0.170408\n",
      "Train epoch: 272 [655520/25046 (80%)]\tLoss: 0.219719\n",
      "Make prediction for 5010 samples...\n",
      "0.3199978 No improvement since epoch  270 ; best_mse,best_ci: 0.3060282 269 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 273 [0/25046 (0%)]\tLoss: 0.217925\n",
      "Train epoch: 273 [652380/25046 (80%)]\tLoss: 0.192226\n",
      "Make prediction for 5010 samples...\n",
      "0.3306341 No improvement since epoch  270 ; best_mse,best_ci: 0.3060282 269 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 274 [0/25046 (0%)]\tLoss: 0.199154\n",
      "Train epoch: 274 [657580/25046 (80%)]\tLoss: 0.222590\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 275 [0/25046 (0%)]\tLoss: 0.170554\n",
      "Train epoch: 275 [660140/25046 (80%)]\tLoss: 0.208467\n",
      "Make prediction for 5010 samples...\n",
      "0.38831574 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 276 [0/25046 (0%)]\tLoss: 0.213135\n",
      "Train epoch: 276 [661680/25046 (80%)]\tLoss: 0.182366\n",
      "Make prediction for 5010 samples...\n",
      "0.32285756 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 277 [0/25046 (0%)]\tLoss: 0.179025\n",
      "Train epoch: 277 [659200/25046 (80%)]\tLoss: 0.200058\n",
      "Make prediction for 5010 samples...\n",
      "0.31689152 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 278 [0/25046 (0%)]\tLoss: 0.209083\n",
      "Train epoch: 278 [662160/25046 (80%)]\tLoss: 0.204667\n",
      "Make prediction for 5010 samples...\n",
      "0.33610258 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 279 [0/25046 (0%)]\tLoss: 0.175476\n",
      "Train epoch: 279 [654740/25046 (80%)]\tLoss: 0.209838\n",
      "Make prediction for 5010 samples...\n",
      "0.32026935 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 280 [0/25046 (0%)]\tLoss: 0.198894\n",
      "Train epoch: 280 [657080/25046 (80%)]\tLoss: 0.206176\n",
      "Make prediction for 5010 samples...\n",
      "0.4661161 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 281 [0/25046 (0%)]\tLoss: 0.296346\n",
      "Train epoch: 281 [665300/25046 (80%)]\tLoss: 0.176459\n",
      "Make prediction for 5010 samples...\n",
      "0.3476532 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 282 [0/25046 (0%)]\tLoss: 0.179647\n",
      "Train epoch: 282 [661300/25046 (80%)]\tLoss: 0.182327\n",
      "Make prediction for 5010 samples...\n",
      "0.33900455 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 283 [0/25046 (0%)]\tLoss: 0.190459\n",
      "Train epoch: 283 [660640/25046 (80%)]\tLoss: 0.203079\n",
      "Make prediction for 5010 samples...\n",
      "0.3099904 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 284 [0/25046 (0%)]\tLoss: 0.191029\n",
      "Train epoch: 284 [652300/25046 (80%)]\tLoss: 0.178419\n",
      "Make prediction for 5010 samples...\n",
      "0.31771764 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 285 [0/25046 (0%)]\tLoss: 0.169471\n",
      "Train epoch: 285 [653840/25046 (80%)]\tLoss: 0.165279\n",
      "Make prediction for 5010 samples...\n",
      "0.34593624 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 286 [0/25046 (0%)]\tLoss: 0.193985\n",
      "Train epoch: 286 [657200/25046 (80%)]\tLoss: 0.200711\n",
      "Make prediction for 5010 samples...\n",
      "0.3213937 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 287 [0/25046 (0%)]\tLoss: 0.161147\n",
      "Train epoch: 287 [658080/25046 (80%)]\tLoss: 0.157105\n",
      "Make prediction for 5010 samples...\n",
      "0.32240403 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 288 [0/25046 (0%)]\tLoss: 0.172836\n",
      "Train epoch: 288 [660100/25046 (80%)]\tLoss: 0.159548\n",
      "Make prediction for 5010 samples...\n",
      "0.31375897 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 289 [0/25046 (0%)]\tLoss: 0.188280\n",
      "Train epoch: 289 [651380/25046 (80%)]\tLoss: 0.162231\n",
      "Make prediction for 5010 samples...\n",
      "0.33957162 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 290 [0/25046 (0%)]\tLoss: 0.180174\n",
      "Train epoch: 290 [661220/25046 (80%)]\tLoss: 0.162390\n",
      "Make prediction for 5010 samples...\n",
      "0.3125405 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 291 [0/25046 (0%)]\tLoss: 0.176706\n",
      "Train epoch: 291 [654780/25046 (80%)]\tLoss: 0.179314\n",
      "Make prediction for 5010 samples...\n",
      "0.31805125 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 292 [0/25046 (0%)]\tLoss: 0.153215\n",
      "Train epoch: 292 [649660/25046 (80%)]\tLoss: 0.183189\n",
      "Make prediction for 5010 samples...\n",
      "0.3234924 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 293 [0/25046 (0%)]\tLoss: 0.176756\n",
      "Train epoch: 293 [660740/25046 (80%)]\tLoss: 0.173853\n",
      "Make prediction for 5010 samples...\n",
      "0.3299414 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 294 [0/25046 (0%)]\tLoss: 0.163824\n",
      "Train epoch: 294 [658300/25046 (80%)]\tLoss: 0.199042\n",
      "Make prediction for 5010 samples...\n",
      "0.33200845 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 295 [0/25046 (0%)]\tLoss: 0.177545\n",
      "Train epoch: 295 [656040/25046 (80%)]\tLoss: 0.179812\n",
      "Make prediction for 5010 samples...\n",
      "0.32968444 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 296 [0/25046 (0%)]\tLoss: 0.184380\n",
      "Train epoch: 296 [650600/25046 (80%)]\tLoss: 0.199883\n",
      "Make prediction for 5010 samples...\n",
      "0.45727715 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 297 [0/25046 (0%)]\tLoss: 0.244569\n",
      "Train epoch: 297 [659800/25046 (80%)]\tLoss: 0.241108\n",
      "Make prediction for 5010 samples...\n",
      "0.31319535 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 298 [0/25046 (0%)]\tLoss: 0.200414\n",
      "Train epoch: 298 [658040/25046 (80%)]\tLoss: 0.170235\n",
      "Make prediction for 5010 samples...\n",
      "0.31793594 No improvement since epoch  274 ; best_mse,best_ci: 0.30509228 273 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 299 [0/25046 (0%)]\tLoss: 0.213689\n",
      "Train epoch: 299 [654760/25046 (80%)]\tLoss: 0.167618\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  299 ; best_mse,best_ci: 0.3040269 298 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 300 [0/25046 (0%)]\tLoss: 0.189930\n",
      "Train epoch: 300 [663080/25046 (80%)]\tLoss: 0.204011\n",
      "Make prediction for 5010 samples...\n",
      "0.33594358 No improvement since epoch  299 ; best_mse,best_ci: 0.3040269 298 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 301 [0/25046 (0%)]\tLoss: 0.154213\n",
      "Train epoch: 301 [655820/25046 (80%)]\tLoss: 0.174395\n",
      "Make prediction for 5010 samples...\n",
      "0.3189053 No improvement since epoch  299 ; best_mse,best_ci: 0.3040269 298 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 302 [0/25046 (0%)]\tLoss: 0.172041\n",
      "Train epoch: 302 [653800/25046 (80%)]\tLoss: 0.177321\n",
      "Make prediction for 5010 samples...\n",
      "0.30689833 No improvement since epoch  299 ; best_mse,best_ci: 0.3040269 298 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 303 [0/25046 (0%)]\tLoss: 0.190304\n",
      "Train epoch: 303 [649700/25046 (80%)]\tLoss: 0.181399\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 304 [0/25046 (0%)]\tLoss: 0.167501\n",
      "Train epoch: 304 [654520/25046 (80%)]\tLoss: 0.156980\n",
      "Make prediction for 5010 samples...\n",
      "0.34001902 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 305 [0/25046 (0%)]\tLoss: 0.184129\n",
      "Train epoch: 305 [656260/25046 (80%)]\tLoss: 0.196317\n",
      "Make prediction for 5010 samples...\n",
      "0.3098954 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 306 [0/25046 (0%)]\tLoss: 0.148980\n",
      "Train epoch: 306 [659480/25046 (80%)]\tLoss: 0.196692\n",
      "Make prediction for 5010 samples...\n",
      "0.30504677 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 307 [0/25046 (0%)]\tLoss: 0.148137\n",
      "Train epoch: 307 [659800/25046 (80%)]\tLoss: 0.202387\n",
      "Make prediction for 5010 samples...\n",
      "0.32195213 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 308 [0/25046 (0%)]\tLoss: 0.147616\n",
      "Train epoch: 308 [657480/25046 (80%)]\tLoss: 0.181234\n",
      "Make prediction for 5010 samples...\n",
      "0.3089654 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 309 [0/25046 (0%)]\tLoss: 0.153029\n",
      "Train epoch: 309 [659980/25046 (80%)]\tLoss: 0.164677\n",
      "Make prediction for 5010 samples...\n",
      "0.3193512 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 310 [0/25046 (0%)]\tLoss: 0.171389\n",
      "Train epoch: 310 [654940/25046 (80%)]\tLoss: 0.168500\n",
      "Make prediction for 5010 samples...\n",
      "0.3038281 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 311 [0/25046 (0%)]\tLoss: 0.149062\n",
      "Train epoch: 311 [656860/25046 (80%)]\tLoss: 0.168197\n",
      "Make prediction for 5010 samples...\n",
      "0.34251964 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 312 [0/25046 (0%)]\tLoss: 0.190910\n",
      "Train epoch: 312 [656180/25046 (80%)]\tLoss: 0.189612\n",
      "Make prediction for 5010 samples...\n",
      "0.3093737 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 313 [0/25046 (0%)]\tLoss: 0.203005\n",
      "Train epoch: 313 [657760/25046 (80%)]\tLoss: 0.232594\n",
      "Make prediction for 5010 samples...\n",
      "0.39078996 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 314 [0/25046 (0%)]\tLoss: 0.213702\n",
      "Train epoch: 314 [660040/25046 (80%)]\tLoss: 0.219843\n",
      "Make prediction for 5010 samples...\n",
      "0.30941907 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 315 [0/25046 (0%)]\tLoss: 0.203512\n",
      "Train epoch: 315 [668640/25046 (80%)]\tLoss: 0.172132\n",
      "Make prediction for 5010 samples...\n",
      "0.3143313 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 316 [0/25046 (0%)]\tLoss: 0.162744\n",
      "Train epoch: 316 [648020/25046 (80%)]\tLoss: 0.178903\n",
      "Make prediction for 5010 samples...\n",
      "0.31683916 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 317 [0/25046 (0%)]\tLoss: 0.180888\n",
      "Train epoch: 317 [652540/25046 (80%)]\tLoss: 0.166756\n",
      "Make prediction for 5010 samples...\n",
      "0.35330632 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 318 [0/25046 (0%)]\tLoss: 0.165880\n",
      "Train epoch: 318 [653780/25046 (80%)]\tLoss: 0.184085\n",
      "Make prediction for 5010 samples...\n",
      "0.36504754 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 319 [0/25046 (0%)]\tLoss: 0.169376\n",
      "Train epoch: 319 [651420/25046 (80%)]\tLoss: 0.209150\n",
      "Make prediction for 5010 samples...\n",
      "0.32494652 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 320 [0/25046 (0%)]\tLoss: 0.191579\n",
      "Train epoch: 320 [653180/25046 (80%)]\tLoss: 0.186556\n",
      "Make prediction for 5010 samples...\n",
      "0.34968564 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 321 [0/25046 (0%)]\tLoss: 0.260022\n",
      "Train epoch: 321 [658460/25046 (80%)]\tLoss: 0.185469\n",
      "Make prediction for 5010 samples...\n",
      "0.33327174 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 322 [0/25046 (0%)]\tLoss: 0.159170\n",
      "Train epoch: 322 [655440/25046 (80%)]\tLoss: 0.214178\n",
      "Make prediction for 5010 samples...\n",
      "0.30740544 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 323 [0/25046 (0%)]\tLoss: 0.164035\n",
      "Train epoch: 323 [652180/25046 (80%)]\tLoss: 0.189550\n",
      "Make prediction for 5010 samples...\n",
      "0.32325545 No improvement since epoch  303 ; best_mse,best_ci: 0.3029737 302 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 324 [0/25046 (0%)]\tLoss: 0.195248\n",
      "Train epoch: 324 [653700/25046 (80%)]\tLoss: 0.214829\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  324 ; best_mse,best_ci: 0.29856387 323 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 325 [0/25046 (0%)]\tLoss: 0.177589\n",
      "Train epoch: 325 [649620/25046 (80%)]\tLoss: 0.183597\n",
      "Make prediction for 5010 samples...\n",
      "0.30650797 No improvement since epoch  324 ; best_mse,best_ci: 0.29856387 323 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 326 [0/25046 (0%)]\tLoss: 0.163072\n",
      "Train epoch: 326 [661640/25046 (80%)]\tLoss: 0.186596\n",
      "Make prediction for 5010 samples...\n",
      "0.30613002 No improvement since epoch  324 ; best_mse,best_ci: 0.29856387 323 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 327 [0/25046 (0%)]\tLoss: 0.172318\n",
      "Train epoch: 327 [659880/25046 (80%)]\tLoss: 0.158262\n",
      "Make prediction for 5010 samples...\n",
      "0.32301864 No improvement since epoch  324 ; best_mse,best_ci: 0.29856387 323 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 328 [0/25046 (0%)]\tLoss: 0.152006\n",
      "Train epoch: 328 [657840/25046 (80%)]\tLoss: 0.159516\n",
      "Make prediction for 5010 samples...\n",
      "0.3008447 No improvement since epoch  324 ; best_mse,best_ci: 0.29856387 323 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 329 [0/25046 (0%)]\tLoss: 0.179839\n",
      "Train epoch: 329 [659040/25046 (80%)]\tLoss: 0.179296\n",
      "Make prediction for 5010 samples...\n",
      "0.31247866 No improvement since epoch  324 ; best_mse,best_ci: 0.29856387 323 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 330 [0/25046 (0%)]\tLoss: 0.192538\n",
      "Train epoch: 330 [646520/25046 (80%)]\tLoss: 0.141927\n",
      "Make prediction for 5010 samples...\n",
      "0.32203218 No improvement since epoch  324 ; best_mse,best_ci: 0.29856387 323 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 331 [0/25046 (0%)]\tLoss: 0.192809\n",
      "Train epoch: 331 [658940/25046 (80%)]\tLoss: 0.177649\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 332 [0/25046 (0%)]\tLoss: 0.167869\n",
      "Train epoch: 332 [659040/25046 (80%)]\tLoss: 0.182636\n",
      "Make prediction for 5010 samples...\n",
      "0.30426887 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 333 [0/25046 (0%)]\tLoss: 0.173915\n",
      "Train epoch: 333 [651920/25046 (80%)]\tLoss: 0.201217\n",
      "Make prediction for 5010 samples...\n",
      "0.32160994 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 334 [0/25046 (0%)]\tLoss: 0.178129\n",
      "Train epoch: 334 [659960/25046 (80%)]\tLoss: 0.152150\n",
      "Make prediction for 5010 samples...\n",
      "0.33824515 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 335 [0/25046 (0%)]\tLoss: 0.144044\n",
      "Train epoch: 335 [650020/25046 (80%)]\tLoss: 0.184997\n",
      "Make prediction for 5010 samples...\n",
      "0.35541183 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 336 [0/25046 (0%)]\tLoss: 0.166401\n",
      "Train epoch: 336 [655380/25046 (80%)]\tLoss: 0.183393\n",
      "Make prediction for 5010 samples...\n",
      "0.30115995 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 337 [0/25046 (0%)]\tLoss: 0.160530\n",
      "Train epoch: 337 [652440/25046 (80%)]\tLoss: 0.203602\n",
      "Make prediction for 5010 samples...\n",
      "0.32417455 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 338 [0/25046 (0%)]\tLoss: 0.252928\n",
      "Train epoch: 338 [655100/25046 (80%)]\tLoss: 0.169426\n",
      "Make prediction for 5010 samples...\n",
      "0.3217558 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 339 [0/25046 (0%)]\tLoss: 0.176894\n",
      "Train epoch: 339 [660860/25046 (80%)]\tLoss: 0.163431\n",
      "Make prediction for 5010 samples...\n",
      "0.30394122 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 340 [0/25046 (0%)]\tLoss: 0.146010\n",
      "Train epoch: 340 [658020/25046 (80%)]\tLoss: 0.173982\n",
      "Make prediction for 5010 samples...\n",
      "0.29755598 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 341 [0/25046 (0%)]\tLoss: 0.164624\n",
      "Train epoch: 341 [659720/25046 (80%)]\tLoss: 0.164701\n",
      "Make prediction for 5010 samples...\n",
      "0.29833055 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 342 [0/25046 (0%)]\tLoss: 0.156981\n",
      "Train epoch: 342 [657240/25046 (80%)]\tLoss: 0.188805\n",
      "Make prediction for 5010 samples...\n",
      "0.3106263 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 343 [0/25046 (0%)]\tLoss: 0.145829\n",
      "Train epoch: 343 [665320/25046 (80%)]\tLoss: 0.165979\n",
      "Make prediction for 5010 samples...\n",
      "0.30264297 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 344 [0/25046 (0%)]\tLoss: 0.179621\n",
      "Train epoch: 344 [659240/25046 (80%)]\tLoss: 0.145412\n",
      "Make prediction for 5010 samples...\n",
      "0.29650337 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 345 [0/25046 (0%)]\tLoss: 0.156417\n",
      "Train epoch: 345 [658980/25046 (80%)]\tLoss: 0.218474\n",
      "Make prediction for 5010 samples...\n",
      "0.2957744 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 346 [0/25046 (0%)]\tLoss: 0.147593\n",
      "Train epoch: 346 [649080/25046 (80%)]\tLoss: 0.183967\n",
      "Make prediction for 5010 samples...\n",
      "0.3269895 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 347 [0/25046 (0%)]\tLoss: 0.163138\n",
      "Train epoch: 347 [653040/25046 (80%)]\tLoss: 0.164199\n",
      "Make prediction for 5010 samples...\n",
      "0.3141821 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 348 [0/25046 (0%)]\tLoss: 0.153966\n",
      "Train epoch: 348 [658820/25046 (80%)]\tLoss: 0.170685\n",
      "Make prediction for 5010 samples...\n",
      "0.30984992 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 349 [0/25046 (0%)]\tLoss: 0.156392\n",
      "Train epoch: 349 [656540/25046 (80%)]\tLoss: 0.179945\n",
      "Make prediction for 5010 samples...\n",
      "0.34748733 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 350 [0/25046 (0%)]\tLoss: 0.158661\n",
      "Train epoch: 350 [651120/25046 (80%)]\tLoss: 0.186525\n",
      "Make prediction for 5010 samples...\n",
      "0.32533285 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 351 [0/25046 (0%)]\tLoss: 0.166736\n",
      "Train epoch: 351 [663100/25046 (80%)]\tLoss: 0.176742\n",
      "Make prediction for 5010 samples...\n",
      "0.32891265 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 352 [0/25046 (0%)]\tLoss: 0.162382\n",
      "Train epoch: 352 [659540/25046 (80%)]\tLoss: 0.155441\n",
      "Make prediction for 5010 samples...\n",
      "0.32320404 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 353 [0/25046 (0%)]\tLoss: 0.133604\n",
      "Train epoch: 353 [658460/25046 (80%)]\tLoss: 0.186707\n",
      "Make prediction for 5010 samples...\n",
      "0.34739798 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 354 [0/25046 (0%)]\tLoss: 0.151857\n",
      "Train epoch: 354 [659120/25046 (80%)]\tLoss: 0.210521\n",
      "Make prediction for 5010 samples...\n",
      "0.36941138 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 355 [0/25046 (0%)]\tLoss: 0.207264\n",
      "Train epoch: 355 [653600/25046 (80%)]\tLoss: 0.198737\n",
      "Make prediction for 5010 samples...\n",
      "0.3102828 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 356 [0/25046 (0%)]\tLoss: 0.197981\n",
      "Train epoch: 356 [653840/25046 (80%)]\tLoss: 0.171657\n",
      "Make prediction for 5010 samples...\n",
      "0.3073741 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 357 [0/25046 (0%)]\tLoss: 0.149530\n",
      "Train epoch: 357 [656580/25046 (80%)]\tLoss: 0.150126\n",
      "Make prediction for 5010 samples...\n",
      "0.31083718 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 358 [0/25046 (0%)]\tLoss: 0.157934\n",
      "Train epoch: 358 [663500/25046 (80%)]\tLoss: 0.157228\n",
      "Make prediction for 5010 samples...\n",
      "0.32351133 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 359 [0/25046 (0%)]\tLoss: 0.210971\n",
      "Train epoch: 359 [658580/25046 (80%)]\tLoss: 0.165301\n",
      "Make prediction for 5010 samples...\n",
      "0.30709353 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 360 [0/25046 (0%)]\tLoss: 0.153161\n",
      "Train epoch: 360 [647620/25046 (80%)]\tLoss: 0.162866\n",
      "Make prediction for 5010 samples...\n",
      "0.2993799 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 361 [0/25046 (0%)]\tLoss: 0.156658\n",
      "Train epoch: 361 [656940/25046 (80%)]\tLoss: 0.177079\n",
      "Make prediction for 5010 samples...\n",
      "0.2995125 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 362 [0/25046 (0%)]\tLoss: 0.179228\n",
      "Train epoch: 362 [654880/25046 (80%)]\tLoss: 0.151605\n",
      "Make prediction for 5010 samples...\n",
      "0.29803404 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 363 [0/25046 (0%)]\tLoss: 0.166628\n",
      "Train epoch: 363 [654280/25046 (80%)]\tLoss: 0.178707\n",
      "Make prediction for 5010 samples...\n",
      "0.30821064 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 364 [0/25046 (0%)]\tLoss: 0.173046\n",
      "Train epoch: 364 [661400/25046 (80%)]\tLoss: 0.158673\n",
      "Make prediction for 5010 samples...\n",
      "0.30400035 No improvement since epoch  331 ; best_mse,best_ci: 0.29399395 330 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 365 [0/25046 (0%)]\tLoss: 0.161544\n",
      "Train epoch: 365 [664440/25046 (80%)]\tLoss: 0.169917\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  365 ; best_mse,best_ci: 0.29355347 364 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 366 [0/25046 (0%)]\tLoss: 0.178674\n",
      "Train epoch: 366 [661600/25046 (80%)]\tLoss: 0.170214\n",
      "Make prediction for 5010 samples...\n",
      "0.30640614 No improvement since epoch  365 ; best_mse,best_ci: 0.29355347 364 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 367 [0/25046 (0%)]\tLoss: 0.155843\n",
      "Train epoch: 367 [653220/25046 (80%)]\tLoss: 0.162600\n",
      "Make prediction for 5010 samples...\n",
      "0.32953262 No improvement since epoch  365 ; best_mse,best_ci: 0.29355347 364 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 368 [0/25046 (0%)]\tLoss: 0.154951\n",
      "Train epoch: 368 [662360/25046 (80%)]\tLoss: 0.153219\n",
      "Make prediction for 5010 samples...\n",
      "0.32518333 No improvement since epoch  365 ; best_mse,best_ci: 0.29355347 364 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 369 [0/25046 (0%)]\tLoss: 0.175067\n",
      "Train epoch: 369 [655700/25046 (80%)]\tLoss: 0.167526\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  369 ; best_mse,best_ci: 0.29329842 368 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 370 [0/25046 (0%)]\tLoss: 0.164658\n",
      "Train epoch: 370 [656260/25046 (80%)]\tLoss: 0.190934\n",
      "Make prediction for 5010 samples...\n",
      "0.30944175 No improvement since epoch  369 ; best_mse,best_ci: 0.29329842 368 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 371 [0/25046 (0%)]\tLoss: 0.184072\n",
      "Train epoch: 371 [661360/25046 (80%)]\tLoss: 0.176518\n",
      "Make prediction for 5010 samples...\n",
      "0.30819675 No improvement since epoch  369 ; best_mse,best_ci: 0.29329842 368 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 372 [0/25046 (0%)]\tLoss: 0.139463\n",
      "Train epoch: 372 [658820/25046 (80%)]\tLoss: 0.162138\n",
      "Make prediction for 5010 samples...\n",
      "0.29688329 No improvement since epoch  369 ; best_mse,best_ci: 0.29329842 368 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 373 [0/25046 (0%)]\tLoss: 0.138028\n",
      "Train epoch: 373 [664060/25046 (80%)]\tLoss: 0.179986\n",
      "Make prediction for 5010 samples...\n",
      "0.3075666 No improvement since epoch  369 ; best_mse,best_ci: 0.29329842 368 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 374 [0/25046 (0%)]\tLoss: 0.150821\n",
      "Train epoch: 374 [657460/25046 (80%)]\tLoss: 0.183115\n",
      "Make prediction for 5010 samples...\n",
      "0.31757405 No improvement since epoch  369 ; best_mse,best_ci: 0.29329842 368 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 375 [0/25046 (0%)]\tLoss: 0.165837\n",
      "Train epoch: 375 [656700/25046 (80%)]\tLoss: 0.157904\n",
      "Make prediction for 5010 samples...\n",
      "0.3579352 No improvement since epoch  369 ; best_mse,best_ci: 0.29329842 368 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 376 [0/25046 (0%)]\tLoss: 0.183021\n",
      "Train epoch: 376 [653840/25046 (80%)]\tLoss: 0.177603\n",
      "Make prediction for 5010 samples...\n",
      "0.3184668 No improvement since epoch  369 ; best_mse,best_ci: 0.29329842 368 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 377 [0/25046 (0%)]\tLoss: 0.144865\n",
      "Train epoch: 377 [657260/25046 (80%)]\tLoss: 0.187473\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  377 ; best_mse,best_ci: 0.29263362 376 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 378 [0/25046 (0%)]\tLoss: 0.169322\n",
      "Train epoch: 378 [650880/25046 (80%)]\tLoss: 0.219292\n",
      "Make prediction for 5010 samples...\n",
      "0.3423098 No improvement since epoch  377 ; best_mse,best_ci: 0.29263362 376 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 379 [0/25046 (0%)]\tLoss: 0.174684\n",
      "Train epoch: 379 [655680/25046 (80%)]\tLoss: 0.173706\n",
      "Make prediction for 5010 samples...\n",
      "0.32847986 No improvement since epoch  377 ; best_mse,best_ci: 0.29263362 376 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 380 [0/25046 (0%)]\tLoss: 0.168848\n",
      "Train epoch: 380 [653340/25046 (80%)]\tLoss: 0.183604\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  380 ; best_mse,best_ci: 0.2921169 379 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 381 [0/25046 (0%)]\tLoss: 0.162887\n",
      "Train epoch: 381 [657040/25046 (80%)]\tLoss: 0.200221\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 382 [0/25046 (0%)]\tLoss: 0.145128\n",
      "Train epoch: 382 [660520/25046 (80%)]\tLoss: 0.155117\n",
      "Make prediction for 5010 samples...\n",
      "0.3278674 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 383 [0/25046 (0%)]\tLoss: 0.180399\n",
      "Train epoch: 383 [661160/25046 (80%)]\tLoss: 0.186517\n",
      "Make prediction for 5010 samples...\n",
      "0.3325029 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 384 [0/25046 (0%)]\tLoss: 0.178871\n",
      "Train epoch: 384 [657080/25046 (80%)]\tLoss: 0.156494\n",
      "Make prediction for 5010 samples...\n",
      "0.34691638 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 385 [0/25046 (0%)]\tLoss: 0.177205\n",
      "Train epoch: 385 [659720/25046 (80%)]\tLoss: 0.210096\n",
      "Make prediction for 5010 samples...\n",
      "0.3637678 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 386 [0/25046 (0%)]\tLoss: 0.235980\n",
      "Train epoch: 386 [665260/25046 (80%)]\tLoss: 0.186117\n",
      "Make prediction for 5010 samples...\n",
      "0.29995835 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 387 [0/25046 (0%)]\tLoss: 0.175395\n",
      "Train epoch: 387 [666200/25046 (80%)]\tLoss: 0.150120\n",
      "Make prediction for 5010 samples...\n",
      "0.30401662 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 388 [0/25046 (0%)]\tLoss: 0.199408\n",
      "Train epoch: 388 [657120/25046 (80%)]\tLoss: 0.158387\n",
      "Make prediction for 5010 samples...\n",
      "0.29967317 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 389 [0/25046 (0%)]\tLoss: 0.134102\n",
      "Train epoch: 389 [651900/25046 (80%)]\tLoss: 0.165200\n",
      "Make prediction for 5010 samples...\n",
      "0.30222222 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 390 [0/25046 (0%)]\tLoss: 0.144250\n",
      "Train epoch: 390 [661580/25046 (80%)]\tLoss: 0.177091\n",
      "Make prediction for 5010 samples...\n",
      "0.2972322 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 391 [0/25046 (0%)]\tLoss: 0.150702\n",
      "Train epoch: 391 [658900/25046 (80%)]\tLoss: 0.150289\n",
      "Make prediction for 5010 samples...\n",
      "0.32648084 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 392 [0/25046 (0%)]\tLoss: 0.146456\n",
      "Train epoch: 392 [656220/25046 (80%)]\tLoss: 0.145735\n",
      "Make prediction for 5010 samples...\n",
      "0.34675387 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 393 [0/25046 (0%)]\tLoss: 0.189717\n",
      "Train epoch: 393 [656840/25046 (80%)]\tLoss: 0.132216\n",
      "Make prediction for 5010 samples...\n",
      "0.3046144 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 394 [0/25046 (0%)]\tLoss: 0.132631\n",
      "Train epoch: 394 [660760/25046 (80%)]\tLoss: 0.147799\n",
      "Make prediction for 5010 samples...\n",
      "0.31739202 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 395 [0/25046 (0%)]\tLoss: 0.142697\n",
      "Train epoch: 395 [655060/25046 (80%)]\tLoss: 0.172597\n",
      "Make prediction for 5010 samples...\n",
      "0.30657804 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 396 [0/25046 (0%)]\tLoss: 0.131392\n",
      "Train epoch: 396 [659360/25046 (80%)]\tLoss: 0.172647\n",
      "Make prediction for 5010 samples...\n",
      "0.3024594 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 397 [0/25046 (0%)]\tLoss: 0.146728\n",
      "Train epoch: 397 [659660/25046 (80%)]\tLoss: 0.148126\n",
      "Make prediction for 5010 samples...\n",
      "0.29302666 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 398 [0/25046 (0%)]\tLoss: 0.151325\n",
      "Train epoch: 398 [660480/25046 (80%)]\tLoss: 0.133888\n",
      "Make prediction for 5010 samples...\n",
      "0.2987598 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 399 [0/25046 (0%)]\tLoss: 0.171054\n",
      "Train epoch: 399 [657760/25046 (80%)]\tLoss: 0.174692\n",
      "Make prediction for 5010 samples...\n",
      "0.30191976 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 400 [0/25046 (0%)]\tLoss: 0.151934\n",
      "Train epoch: 400 [663680/25046 (80%)]\tLoss: 0.177824\n",
      "Make prediction for 5010 samples...\n",
      "0.322076 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 401 [0/25046 (0%)]\tLoss: 0.157683\n",
      "Train epoch: 401 [656580/25046 (80%)]\tLoss: 0.200861\n",
      "Make prediction for 5010 samples...\n",
      "0.31978276 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 402 [0/25046 (0%)]\tLoss: 0.168669\n",
      "Train epoch: 402 [659880/25046 (80%)]\tLoss: 0.145646\n",
      "Make prediction for 5010 samples...\n",
      "0.29687065 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 403 [0/25046 (0%)]\tLoss: 0.157512\n",
      "Train epoch: 403 [653000/25046 (80%)]\tLoss: 0.152082\n",
      "Make prediction for 5010 samples...\n",
      "0.34531182 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 404 [0/25046 (0%)]\tLoss: 0.164816\n",
      "Train epoch: 404 [649720/25046 (80%)]\tLoss: 0.162529\n",
      "Make prediction for 5010 samples...\n",
      "0.29719692 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 405 [0/25046 (0%)]\tLoss: 0.152902\n",
      "Train epoch: 405 [659880/25046 (80%)]\tLoss: 0.159192\n",
      "Make prediction for 5010 samples...\n",
      "0.29948142 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 406 [0/25046 (0%)]\tLoss: 0.189430\n",
      "Train epoch: 406 [656340/25046 (80%)]\tLoss: 0.159350\n",
      "Make prediction for 5010 samples...\n",
      "0.3568612 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 407 [0/25046 (0%)]\tLoss: 0.153423\n",
      "Train epoch: 407 [659320/25046 (80%)]\tLoss: 0.183049\n",
      "Make prediction for 5010 samples...\n",
      "0.32276 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 408 [0/25046 (0%)]\tLoss: 0.152465\n",
      "Train epoch: 408 [664420/25046 (80%)]\tLoss: 0.154175\n",
      "Make prediction for 5010 samples...\n",
      "0.29984728 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 409 [0/25046 (0%)]\tLoss: 0.161196\n",
      "Train epoch: 409 [655020/25046 (80%)]\tLoss: 0.151703\n",
      "Make prediction for 5010 samples...\n",
      "0.3017942 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 410 [0/25046 (0%)]\tLoss: 0.158134\n",
      "Train epoch: 410 [653460/25046 (80%)]\tLoss: 0.167532\n",
      "Make prediction for 5010 samples...\n",
      "0.29780352 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 411 [0/25046 (0%)]\tLoss: 0.147066\n",
      "Train epoch: 411 [656200/25046 (80%)]\tLoss: 0.146177\n",
      "Make prediction for 5010 samples...\n",
      "0.29838687 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 412 [0/25046 (0%)]\tLoss: 0.144785\n",
      "Train epoch: 412 [663040/25046 (80%)]\tLoss: 0.168170\n",
      "Make prediction for 5010 samples...\n",
      "0.2974584 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 413 [0/25046 (0%)]\tLoss: 0.153613\n",
      "Train epoch: 413 [664640/25046 (80%)]\tLoss: 0.147013\n",
      "Make prediction for 5010 samples...\n",
      "0.30634046 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 414 [0/25046 (0%)]\tLoss: 0.117618\n",
      "Train epoch: 414 [663200/25046 (80%)]\tLoss: 0.173056\n",
      "Make prediction for 5010 samples...\n",
      "0.33649474 No improvement since epoch  381 ; best_mse,best_ci: 0.2910514 380 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 415 [0/25046 (0%)]\tLoss: 0.168867\n",
      "Train epoch: 415 [651160/25046 (80%)]\tLoss: 0.132803\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 416 [0/25046 (0%)]\tLoss: 0.173192\n",
      "Train epoch: 416 [656560/25046 (80%)]\tLoss: 0.146049\n",
      "Make prediction for 5010 samples...\n",
      "0.33018854 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 417 [0/25046 (0%)]\tLoss: 0.137366\n",
      "Train epoch: 417 [659520/25046 (80%)]\tLoss: 0.163693\n",
      "Make prediction for 5010 samples...\n",
      "0.30365196 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 418 [0/25046 (0%)]\tLoss: 0.146295\n",
      "Train epoch: 418 [660500/25046 (80%)]\tLoss: 0.175489\n",
      "Make prediction for 5010 samples...\n",
      "0.29860038 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 419 [0/25046 (0%)]\tLoss: 0.150096\n",
      "Train epoch: 419 [656380/25046 (80%)]\tLoss: 0.179315\n",
      "Make prediction for 5010 samples...\n",
      "0.33338743 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 420 [0/25046 (0%)]\tLoss: 0.174697\n",
      "Train epoch: 420 [659380/25046 (80%)]\tLoss: 0.144187\n",
      "Make prediction for 5010 samples...\n",
      "0.30376562 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 421 [0/25046 (0%)]\tLoss: 0.164729\n",
      "Train epoch: 421 [661860/25046 (80%)]\tLoss: 0.151744\n",
      "Make prediction for 5010 samples...\n",
      "0.3527756 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 422 [0/25046 (0%)]\tLoss: 0.181756\n",
      "Train epoch: 422 [657400/25046 (80%)]\tLoss: 0.191080\n",
      "Make prediction for 5010 samples...\n",
      "0.3718826 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 423 [0/25046 (0%)]\tLoss: 0.167737\n",
      "Train epoch: 423 [658160/25046 (80%)]\tLoss: 0.165500\n",
      "Make prediction for 5010 samples...\n",
      "0.3233955 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 424 [0/25046 (0%)]\tLoss: 0.153850\n",
      "Train epoch: 424 [653220/25046 (80%)]\tLoss: 0.161867\n",
      "Make prediction for 5010 samples...\n",
      "0.34536663 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 425 [0/25046 (0%)]\tLoss: 0.177211\n",
      "Train epoch: 425 [651640/25046 (80%)]\tLoss: 0.159312\n",
      "Make prediction for 5010 samples...\n",
      "0.2890944 No improvement since epoch  415 ; best_mse,best_ci: 0.28804514 414 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 426 [0/25046 (0%)]\tLoss: 0.118403\n",
      "Train epoch: 426 [648120/25046 (80%)]\tLoss: 0.134657\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  426 ; best_mse,best_ci: 0.2879664 425 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 427 [0/25046 (0%)]\tLoss: 0.142486\n",
      "Train epoch: 427 [659580/25046 (80%)]\tLoss: 0.189776\n",
      "Make prediction for 5010 samples...\n",
      "0.31465262 No improvement since epoch  426 ; best_mse,best_ci: 0.2879664 425 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 428 [0/25046 (0%)]\tLoss: 0.145359\n",
      "Train epoch: 428 [652240/25046 (80%)]\tLoss: 0.175565\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 429 [0/25046 (0%)]\tLoss: 0.145684\n",
      "Train epoch: 429 [653080/25046 (80%)]\tLoss: 0.164987\n",
      "Make prediction for 5010 samples...\n",
      "0.30758706 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 430 [0/25046 (0%)]\tLoss: 0.111183\n",
      "Train epoch: 430 [656540/25046 (80%)]\tLoss: 0.147194\n",
      "Make prediction for 5010 samples...\n",
      "0.32707396 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 431 [0/25046 (0%)]\tLoss: 0.147361\n",
      "Train epoch: 431 [651960/25046 (80%)]\tLoss: 0.148004\n",
      "Make prediction for 5010 samples...\n",
      "0.30063745 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 432 [0/25046 (0%)]\tLoss: 0.168118\n",
      "Train epoch: 432 [656920/25046 (80%)]\tLoss: 0.142667\n",
      "Make prediction for 5010 samples...\n",
      "0.29453543 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 433 [0/25046 (0%)]\tLoss: 0.173190\n",
      "Train epoch: 433 [652440/25046 (80%)]\tLoss: 0.169701\n",
      "Make prediction for 5010 samples...\n",
      "0.30055112 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 434 [0/25046 (0%)]\tLoss: 0.182108\n",
      "Train epoch: 434 [649880/25046 (80%)]\tLoss: 0.141889\n",
      "Make prediction for 5010 samples...\n",
      "0.2877122 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 435 [0/25046 (0%)]\tLoss: 0.124497\n",
      "Train epoch: 435 [655920/25046 (80%)]\tLoss: 0.151939\n",
      "Make prediction for 5010 samples...\n",
      "0.30167654 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 436 [0/25046 (0%)]\tLoss: 0.152960\n",
      "Train epoch: 436 [658920/25046 (80%)]\tLoss: 0.161993\n",
      "Make prediction for 5010 samples...\n",
      "0.31752652 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 437 [0/25046 (0%)]\tLoss: 0.139723\n",
      "Train epoch: 437 [652280/25046 (80%)]\tLoss: 0.170540\n",
      "Make prediction for 5010 samples...\n",
      "0.3386799 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 438 [0/25046 (0%)]\tLoss: 0.163704\n",
      "Train epoch: 438 [654500/25046 (80%)]\tLoss: 0.156934\n",
      "Make prediction for 5010 samples...\n",
      "0.3022665 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 439 [0/25046 (0%)]\tLoss: 0.155609\n",
      "Train epoch: 439 [654860/25046 (80%)]\tLoss: 0.160811\n",
      "Make prediction for 5010 samples...\n",
      "0.28814787 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 440 [0/25046 (0%)]\tLoss: 0.193617\n",
      "Train epoch: 440 [655900/25046 (80%)]\tLoss: 0.172307\n",
      "Make prediction for 5010 samples...\n",
      "0.36077327 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 441 [0/25046 (0%)]\tLoss: 0.192068\n",
      "Train epoch: 441 [659900/25046 (80%)]\tLoss: 0.144483\n",
      "Make prediction for 5010 samples...\n",
      "0.2938824 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 442 [0/25046 (0%)]\tLoss: 0.134002\n",
      "Train epoch: 442 [659420/25046 (80%)]\tLoss: 0.131484\n",
      "Make prediction for 5010 samples...\n",
      "0.30511296 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 443 [0/25046 (0%)]\tLoss: 0.143885\n",
      "Train epoch: 443 [656260/25046 (80%)]\tLoss: 0.152066\n",
      "Make prediction for 5010 samples...\n",
      "0.29737765 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 444 [0/25046 (0%)]\tLoss: 0.163265\n",
      "Train epoch: 444 [661360/25046 (80%)]\tLoss: 0.189573\n",
      "Make prediction for 5010 samples...\n",
      "0.29184353 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 445 [0/25046 (0%)]\tLoss: 0.159278\n",
      "Train epoch: 445 [662260/25046 (80%)]\tLoss: 0.136807\n",
      "Make prediction for 5010 samples...\n",
      "0.30508006 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 446 [0/25046 (0%)]\tLoss: 0.139935\n",
      "Train epoch: 446 [659860/25046 (80%)]\tLoss: 0.130729\n",
      "Make prediction for 5010 samples...\n",
      "0.3011187 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 447 [0/25046 (0%)]\tLoss: 0.162770\n",
      "Train epoch: 447 [659540/25046 (80%)]\tLoss: 0.126015\n",
      "Make prediction for 5010 samples...\n",
      "0.30230922 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 448 [0/25046 (0%)]\tLoss: 0.152977\n",
      "Train epoch: 448 [661680/25046 (80%)]\tLoss: 0.148214\n",
      "Make prediction for 5010 samples...\n",
      "0.2934588 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 449 [0/25046 (0%)]\tLoss: 0.156832\n",
      "Train epoch: 449 [650640/25046 (80%)]\tLoss: 0.143114\n",
      "Make prediction for 5010 samples...\n",
      "0.2915465 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 450 [0/25046 (0%)]\tLoss: 0.129072\n",
      "Train epoch: 450 [656740/25046 (80%)]\tLoss: 0.160465\n",
      "Make prediction for 5010 samples...\n",
      "0.30924124 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 451 [0/25046 (0%)]\tLoss: 0.150016\n",
      "Train epoch: 451 [657520/25046 (80%)]\tLoss: 0.153631\n",
      "Make prediction for 5010 samples...\n",
      "0.28798035 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 452 [0/25046 (0%)]\tLoss: 0.141425\n",
      "Train epoch: 452 [656800/25046 (80%)]\tLoss: 0.149168\n",
      "Make prediction for 5010 samples...\n",
      "0.2977036 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 453 [0/25046 (0%)]\tLoss: 0.111932\n",
      "Train epoch: 453 [663080/25046 (80%)]\tLoss: 0.162348\n",
      "Make prediction for 5010 samples...\n",
      "0.3350004 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 454 [0/25046 (0%)]\tLoss: 0.154361\n",
      "Train epoch: 454 [655800/25046 (80%)]\tLoss: 0.150590\n",
      "Make prediction for 5010 samples...\n",
      "0.31919846 No improvement since epoch  428 ; best_mse,best_ci: 0.2870006 427 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 455 [0/25046 (0%)]\tLoss: 0.131931\n",
      "Train epoch: 455 [658000/25046 (80%)]\tLoss: 0.128859\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 456 [0/25046 (0%)]\tLoss: 0.135288\n",
      "Train epoch: 456 [658400/25046 (80%)]\tLoss: 0.179237\n",
      "Make prediction for 5010 samples...\n",
      "0.31012276 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 457 [0/25046 (0%)]\tLoss: 0.138992\n",
      "Train epoch: 457 [657460/25046 (80%)]\tLoss: 0.182847\n",
      "Make prediction for 5010 samples...\n",
      "0.30866152 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 458 [0/25046 (0%)]\tLoss: 0.176262\n",
      "Train epoch: 458 [653800/25046 (80%)]\tLoss: 0.143428\n",
      "Make prediction for 5010 samples...\n",
      "0.296354 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 459 [0/25046 (0%)]\tLoss: 0.140710\n",
      "Train epoch: 459 [660420/25046 (80%)]\tLoss: 0.132497\n",
      "Make prediction for 5010 samples...\n",
      "0.29841477 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 460 [0/25046 (0%)]\tLoss: 0.130542\n",
      "Train epoch: 460 [651900/25046 (80%)]\tLoss: 0.108215\n",
      "Make prediction for 5010 samples...\n",
      "0.29732007 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 461 [0/25046 (0%)]\tLoss: 0.138613\n",
      "Train epoch: 461 [658680/25046 (80%)]\tLoss: 0.152677\n",
      "Make prediction for 5010 samples...\n",
      "0.29751417 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 462 [0/25046 (0%)]\tLoss: 0.142098\n",
      "Train epoch: 462 [653060/25046 (80%)]\tLoss: 0.161873\n",
      "Make prediction for 5010 samples...\n",
      "0.36041468 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 463 [0/25046 (0%)]\tLoss: 0.149816\n",
      "Train epoch: 463 [655260/25046 (80%)]\tLoss: 0.147329\n",
      "Make prediction for 5010 samples...\n",
      "0.2991688 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 464 [0/25046 (0%)]\tLoss: 0.148932\n",
      "Train epoch: 464 [660440/25046 (80%)]\tLoss: 0.200692\n",
      "Make prediction for 5010 samples...\n",
      "0.3159555 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 465 [0/25046 (0%)]\tLoss: 0.145249\n",
      "Train epoch: 465 [657640/25046 (80%)]\tLoss: 0.138678\n",
      "Make prediction for 5010 samples...\n",
      "0.30821523 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 466 [0/25046 (0%)]\tLoss: 0.127363\n",
      "Train epoch: 466 [654200/25046 (80%)]\tLoss: 0.158428\n",
      "Make prediction for 5010 samples...\n",
      "0.2966354 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 467 [0/25046 (0%)]\tLoss: 0.135848\n",
      "Train epoch: 467 [654740/25046 (80%)]\tLoss: 0.173020\n",
      "Make prediction for 5010 samples...\n",
      "0.28866774 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 468 [0/25046 (0%)]\tLoss: 0.145445\n",
      "Train epoch: 468 [654500/25046 (80%)]\tLoss: 0.146867\n",
      "Make prediction for 5010 samples...\n",
      "0.32802534 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 469 [0/25046 (0%)]\tLoss: 0.140907\n",
      "Train epoch: 469 [658340/25046 (80%)]\tLoss: 0.140346\n",
      "Make prediction for 5010 samples...\n",
      "0.30052075 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 470 [0/25046 (0%)]\tLoss: 0.125172\n",
      "Train epoch: 470 [652560/25046 (80%)]\tLoss: 0.122767\n",
      "Make prediction for 5010 samples...\n",
      "0.3192767 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 471 [0/25046 (0%)]\tLoss: 0.150244\n",
      "Train epoch: 471 [646760/25046 (80%)]\tLoss: 0.163917\n",
      "Make prediction for 5010 samples...\n",
      "0.36011815 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 472 [0/25046 (0%)]\tLoss: 0.162474\n",
      "Train epoch: 472 [653980/25046 (80%)]\tLoss: 0.138312\n",
      "Make prediction for 5010 samples...\n",
      "0.29890987 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 473 [0/25046 (0%)]\tLoss: 0.138266\n",
      "Train epoch: 473 [663740/25046 (80%)]\tLoss: 0.157521\n",
      "Make prediction for 5010 samples...\n",
      "0.28783688 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 474 [0/25046 (0%)]\tLoss: 0.130976\n",
      "Train epoch: 474 [654560/25046 (80%)]\tLoss: 0.164967\n",
      "Make prediction for 5010 samples...\n",
      "0.3189173 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 475 [0/25046 (0%)]\tLoss: 0.182065\n",
      "Train epoch: 475 [654680/25046 (80%)]\tLoss: 0.146495\n",
      "Make prediction for 5010 samples...\n",
      "0.29886562 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 476 [0/25046 (0%)]\tLoss: 0.136133\n",
      "Train epoch: 476 [654460/25046 (80%)]\tLoss: 0.150366\n",
      "Make prediction for 5010 samples...\n",
      "0.34857625 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 477 [0/25046 (0%)]\tLoss: 0.147668\n",
      "Train epoch: 477 [651240/25046 (80%)]\tLoss: 0.142991\n",
      "Make prediction for 5010 samples...\n",
      "0.28847826 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 478 [0/25046 (0%)]\tLoss: 0.155138\n",
      "Train epoch: 478 [657500/25046 (80%)]\tLoss: 0.162659\n",
      "Make prediction for 5010 samples...\n",
      "0.3348418 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 479 [0/25046 (0%)]\tLoss: 0.149898\n",
      "Train epoch: 479 [657760/25046 (80%)]\tLoss: 0.136038\n",
      "Make prediction for 5010 samples...\n",
      "0.29673856 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 480 [0/25046 (0%)]\tLoss: 0.144597\n",
      "Train epoch: 480 [657500/25046 (80%)]\tLoss: 0.165082\n",
      "Make prediction for 5010 samples...\n",
      "0.29344782 No improvement since epoch  455 ; best_mse,best_ci: 0.28531316 454 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 481 [0/25046 (0%)]\tLoss: 0.118008\n",
      "Train epoch: 481 [662400/25046 (80%)]\tLoss: 0.115384\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 482 [0/25046 (0%)]\tLoss: 0.124922\n",
      "Train epoch: 482 [651640/25046 (80%)]\tLoss: 0.145861\n",
      "Make prediction for 5010 samples...\n",
      "0.2860141 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 483 [0/25046 (0%)]\tLoss: 0.159057\n",
      "Train epoch: 483 [656480/25046 (80%)]\tLoss: 0.146709\n",
      "Make prediction for 5010 samples...\n",
      "0.35901934 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 484 [0/25046 (0%)]\tLoss: 0.167955\n",
      "Train epoch: 484 [659240/25046 (80%)]\tLoss: 0.154218\n",
      "Make prediction for 5010 samples...\n",
      "0.29568142 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 485 [0/25046 (0%)]\tLoss: 0.140819\n",
      "Train epoch: 485 [652840/25046 (80%)]\tLoss: 0.150437\n",
      "Make prediction for 5010 samples...\n",
      "0.3189487 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 486 [0/25046 (0%)]\tLoss: 0.142027\n",
      "Train epoch: 486 [651640/25046 (80%)]\tLoss: 0.176769\n",
      "Make prediction for 5010 samples...\n",
      "0.30856466 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 487 [0/25046 (0%)]\tLoss: 0.132021\n",
      "Train epoch: 487 [656480/25046 (80%)]\tLoss: 0.154101\n",
      "Make prediction for 5010 samples...\n",
      "0.30582112 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 488 [0/25046 (0%)]\tLoss: 0.159973\n",
      "Train epoch: 488 [656680/25046 (80%)]\tLoss: 0.138860\n",
      "Make prediction for 5010 samples...\n",
      "0.28774962 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 489 [0/25046 (0%)]\tLoss: 0.130111\n",
      "Train epoch: 489 [656720/25046 (80%)]\tLoss: 0.170446\n",
      "Make prediction for 5010 samples...\n",
      "0.36412117 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 490 [0/25046 (0%)]\tLoss: 0.163118\n",
      "Train epoch: 490 [649020/25046 (80%)]\tLoss: 0.138653\n",
      "Make prediction for 5010 samples...\n",
      "0.29219767 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 491 [0/25046 (0%)]\tLoss: 0.126518\n",
      "Train epoch: 491 [656880/25046 (80%)]\tLoss: 0.139733\n",
      "Make prediction for 5010 samples...\n",
      "0.29410422 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 492 [0/25046 (0%)]\tLoss: 0.148225\n",
      "Train epoch: 492 [662320/25046 (80%)]\tLoss: 0.146050\n",
      "Make prediction for 5010 samples...\n",
      "0.29319406 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 493 [0/25046 (0%)]\tLoss: 0.141996\n",
      "Train epoch: 493 [658880/25046 (80%)]\tLoss: 0.134523\n",
      "Make prediction for 5010 samples...\n",
      "0.29113322 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 494 [0/25046 (0%)]\tLoss: 0.133756\n",
      "Train epoch: 494 [662200/25046 (80%)]\tLoss: 0.158346\n",
      "Make prediction for 5010 samples...\n",
      "0.32856223 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 495 [0/25046 (0%)]\tLoss: 0.144762\n",
      "Train epoch: 495 [652200/25046 (80%)]\tLoss: 0.197710\n",
      "Make prediction for 5010 samples...\n",
      "0.33115172 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 496 [0/25046 (0%)]\tLoss: 0.154546\n",
      "Train epoch: 496 [657840/25046 (80%)]\tLoss: 0.161462\n",
      "Make prediction for 5010 samples...\n",
      "0.28889835 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 497 [0/25046 (0%)]\tLoss: 0.134171\n",
      "Train epoch: 497 [662060/25046 (80%)]\tLoss: 0.123264\n",
      "Make prediction for 5010 samples...\n",
      "0.3078502 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 498 [0/25046 (0%)]\tLoss: 0.139972\n",
      "Train epoch: 498 [655560/25046 (80%)]\tLoss: 0.129160\n",
      "Make prediction for 5010 samples...\n",
      "0.2840603 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 499 [0/25046 (0%)]\tLoss: 0.156845\n",
      "Train epoch: 499 [656660/25046 (80%)]\tLoss: 0.141061\n",
      "Make prediction for 5010 samples...\n",
      "0.2945642 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 500 [0/25046 (0%)]\tLoss: 0.138649\n",
      "Train epoch: 500 [656060/25046 (80%)]\tLoss: 0.139840\n",
      "Make prediction for 5010 samples...\n",
      "0.30227968 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 501 [0/25046 (0%)]\tLoss: 0.141523\n",
      "Train epoch: 501 [656720/25046 (80%)]\tLoss: 0.141931\n",
      "Make prediction for 5010 samples...\n",
      "0.31290358 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 502 [0/25046 (0%)]\tLoss: 0.120272\n",
      "Train epoch: 502 [655700/25046 (80%)]\tLoss: 0.150108\n",
      "Make prediction for 5010 samples...\n",
      "0.28851405 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 503 [0/25046 (0%)]\tLoss: 0.137933\n",
      "Train epoch: 503 [657960/25046 (80%)]\tLoss: 0.139515\n",
      "Make prediction for 5010 samples...\n",
      "0.29746222 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 504 [0/25046 (0%)]\tLoss: 0.124965\n",
      "Train epoch: 504 [658360/25046 (80%)]\tLoss: 0.133395\n",
      "Make prediction for 5010 samples...\n",
      "0.28455842 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 505 [0/25046 (0%)]\tLoss: 0.155222\n",
      "Train epoch: 505 [662000/25046 (80%)]\tLoss: 0.157791\n",
      "Make prediction for 5010 samples...\n",
      "0.2930523 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 506 [0/25046 (0%)]\tLoss: 0.133917\n",
      "Train epoch: 506 [654560/25046 (80%)]\tLoss: 0.138440\n",
      "Make prediction for 5010 samples...\n",
      "0.3096369 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 507 [0/25046 (0%)]\tLoss: 0.159406\n",
      "Train epoch: 507 [658960/25046 (80%)]\tLoss: 0.145604\n",
      "Make prediction for 5010 samples...\n",
      "0.2870181 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 508 [0/25046 (0%)]\tLoss: 0.147114\n",
      "Train epoch: 508 [657600/25046 (80%)]\tLoss: 0.164851\n",
      "Make prediction for 5010 samples...\n",
      "0.31681567 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 509 [0/25046 (0%)]\tLoss: 0.156175\n",
      "Train epoch: 509 [653400/25046 (80%)]\tLoss: 0.135658\n",
      "Make prediction for 5010 samples...\n",
      "0.2903375 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 510 [0/25046 (0%)]\tLoss: 0.137722\n",
      "Train epoch: 510 [657120/25046 (80%)]\tLoss: 0.155365\n",
      "Make prediction for 5010 samples...\n",
      "0.30127802 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 511 [0/25046 (0%)]\tLoss: 0.122784\n",
      "Train epoch: 511 [658540/25046 (80%)]\tLoss: 0.135133\n",
      "Make prediction for 5010 samples...\n",
      "0.2842423 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 512 [0/25046 (0%)]\tLoss: 0.129966\n",
      "Train epoch: 512 [659460/25046 (80%)]\tLoss: 0.166167\n",
      "Make prediction for 5010 samples...\n",
      "0.28740498 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 513 [0/25046 (0%)]\tLoss: 0.129338\n",
      "Train epoch: 513 [658680/25046 (80%)]\tLoss: 0.148947\n",
      "Make prediction for 5010 samples...\n",
      "0.2847762 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 514 [0/25046 (0%)]\tLoss: 0.126012\n",
      "Train epoch: 514 [659400/25046 (80%)]\tLoss: 0.153529\n",
      "Make prediction for 5010 samples...\n",
      "0.29790407 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 515 [0/25046 (0%)]\tLoss: 0.137016\n",
      "Train epoch: 515 [655940/25046 (80%)]\tLoss: 0.122821\n",
      "Make prediction for 5010 samples...\n",
      "0.3157988 No improvement since epoch  481 ; best_mse,best_ci: 0.28253308 480 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 516 [0/25046 (0%)]\tLoss: 0.161300\n",
      "Train epoch: 516 [654300/25046 (80%)]\tLoss: 0.145944\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 517 [0/25046 (0%)]\tLoss: 0.131119\n",
      "Train epoch: 517 [653640/25046 (80%)]\tLoss: 0.141820\n",
      "Make prediction for 5010 samples...\n",
      "0.27994436 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 518 [0/25046 (0%)]\tLoss: 0.140729\n",
      "Train epoch: 518 [660440/25046 (80%)]\tLoss: 0.152540\n",
      "Make prediction for 5010 samples...\n",
      "0.28223446 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 519 [0/25046 (0%)]\tLoss: 0.117883\n",
      "Train epoch: 519 [662500/25046 (80%)]\tLoss: 0.137787\n",
      "Make prediction for 5010 samples...\n",
      "0.29597265 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 520 [0/25046 (0%)]\tLoss: 0.116007\n",
      "Train epoch: 520 [656200/25046 (80%)]\tLoss: 0.138680\n",
      "Make prediction for 5010 samples...\n",
      "0.3170599 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 521 [0/25046 (0%)]\tLoss: 0.139279\n",
      "Train epoch: 521 [657960/25046 (80%)]\tLoss: 0.131828\n",
      "Make prediction for 5010 samples...\n",
      "0.29211947 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 522 [0/25046 (0%)]\tLoss: 0.137529\n",
      "Train epoch: 522 [652740/25046 (80%)]\tLoss: 0.129993\n",
      "Make prediction for 5010 samples...\n",
      "0.28651193 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 523 [0/25046 (0%)]\tLoss: 0.149097\n",
      "Train epoch: 523 [657080/25046 (80%)]\tLoss: 0.129705\n",
      "Make prediction for 5010 samples...\n",
      "0.31170762 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 524 [0/25046 (0%)]\tLoss: 0.131953\n",
      "Train epoch: 524 [661140/25046 (80%)]\tLoss: 0.136716\n",
      "Make prediction for 5010 samples...\n",
      "0.32320088 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 525 [0/25046 (0%)]\tLoss: 0.135597\n",
      "Train epoch: 525 [659940/25046 (80%)]\tLoss: 0.141358\n",
      "Make prediction for 5010 samples...\n",
      "0.28930497 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 526 [0/25046 (0%)]\tLoss: 0.124905\n",
      "Train epoch: 526 [655400/25046 (80%)]\tLoss: 0.145484\n",
      "Make prediction for 5010 samples...\n",
      "0.29597533 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 527 [0/25046 (0%)]\tLoss: 0.128715\n",
      "Train epoch: 527 [658960/25046 (80%)]\tLoss: 0.113999\n",
      "Make prediction for 5010 samples...\n",
      "0.40228233 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 528 [0/25046 (0%)]\tLoss: 0.217446\n",
      "Train epoch: 528 [657900/25046 (80%)]\tLoss: 0.137848\n",
      "Make prediction for 5010 samples...\n",
      "0.3408684 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 529 [0/25046 (0%)]\tLoss: 0.155764\n",
      "Train epoch: 529 [656800/25046 (80%)]\tLoss: 0.133770\n",
      "Make prediction for 5010 samples...\n",
      "0.30635488 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 530 [0/25046 (0%)]\tLoss: 0.136126\n",
      "Train epoch: 530 [655400/25046 (80%)]\tLoss: 0.136528\n",
      "Make prediction for 5010 samples...\n",
      "0.29172045 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 531 [0/25046 (0%)]\tLoss: 0.119902\n",
      "Train epoch: 531 [655420/25046 (80%)]\tLoss: 0.138841\n",
      "Make prediction for 5010 samples...\n",
      "0.2943343 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 532 [0/25046 (0%)]\tLoss: 0.131825\n",
      "Train epoch: 532 [660540/25046 (80%)]\tLoss: 0.139119\n",
      "Make prediction for 5010 samples...\n",
      "0.29970855 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 533 [0/25046 (0%)]\tLoss: 0.138070\n",
      "Train epoch: 533 [656400/25046 (80%)]\tLoss: 0.189161\n",
      "Make prediction for 5010 samples...\n",
      "0.3225022 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 534 [0/25046 (0%)]\tLoss: 0.141310\n",
      "Train epoch: 534 [644840/25046 (80%)]\tLoss: 0.118332\n",
      "Make prediction for 5010 samples...\n",
      "0.29585448 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 535 [0/25046 (0%)]\tLoss: 0.141701\n",
      "Train epoch: 535 [660000/25046 (80%)]\tLoss: 0.133586\n",
      "Make prediction for 5010 samples...\n",
      "0.29912975 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 536 [0/25046 (0%)]\tLoss: 0.137209\n",
      "Train epoch: 536 [654380/25046 (80%)]\tLoss: 0.155384\n",
      "Make prediction for 5010 samples...\n",
      "0.29989624 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 537 [0/25046 (0%)]\tLoss: 0.133000\n",
      "Train epoch: 537 [655660/25046 (80%)]\tLoss: 0.124681\n",
      "Make prediction for 5010 samples...\n",
      "0.28540093 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 538 [0/25046 (0%)]\tLoss: 0.153916\n",
      "Train epoch: 538 [664020/25046 (80%)]\tLoss: 0.182608\n",
      "Make prediction for 5010 samples...\n",
      "0.33426258 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 539 [0/25046 (0%)]\tLoss: 0.204421\n",
      "Train epoch: 539 [658620/25046 (80%)]\tLoss: 0.123290\n",
      "Make prediction for 5010 samples...\n",
      "0.29384798 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 540 [0/25046 (0%)]\tLoss: 0.143541\n",
      "Train epoch: 540 [665020/25046 (80%)]\tLoss: 0.139892\n",
      "Make prediction for 5010 samples...\n",
      "0.2854288 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 541 [0/25046 (0%)]\tLoss: 0.125999\n",
      "Train epoch: 541 [658720/25046 (80%)]\tLoss: 0.178059\n",
      "Make prediction for 5010 samples...\n",
      "0.35814738 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 542 [0/25046 (0%)]\tLoss: 0.152208\n",
      "Train epoch: 542 [654560/25046 (80%)]\tLoss: 0.132201\n",
      "Make prediction for 5010 samples...\n",
      "0.29517975 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 543 [0/25046 (0%)]\tLoss: 0.122419\n",
      "Train epoch: 543 [654920/25046 (80%)]\tLoss: 0.144354\n",
      "Make prediction for 5010 samples...\n",
      "0.2984903 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 544 [0/25046 (0%)]\tLoss: 0.132285\n",
      "Train epoch: 544 [657080/25046 (80%)]\tLoss: 0.136063\n",
      "Make prediction for 5010 samples...\n",
      "0.29500005 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 545 [0/25046 (0%)]\tLoss: 0.166957\n",
      "Train epoch: 545 [658440/25046 (80%)]\tLoss: 0.135266\n",
      "Make prediction for 5010 samples...\n",
      "0.28564122 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 546 [0/25046 (0%)]\tLoss: 0.148767\n",
      "Train epoch: 546 [660160/25046 (80%)]\tLoss: 0.152128\n",
      "Make prediction for 5010 samples...\n",
      "0.33322182 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 547 [0/25046 (0%)]\tLoss: 0.137202\n",
      "Train epoch: 547 [654860/25046 (80%)]\tLoss: 0.127014\n",
      "Make prediction for 5010 samples...\n",
      "0.28323987 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 548 [0/25046 (0%)]\tLoss: 0.162088\n",
      "Train epoch: 548 [654880/25046 (80%)]\tLoss: 0.132137\n",
      "Make prediction for 5010 samples...\n",
      "0.28158483 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 549 [0/25046 (0%)]\tLoss: 0.145509\n",
      "Train epoch: 549 [659380/25046 (80%)]\tLoss: 0.131135\n",
      "Make prediction for 5010 samples...\n",
      "0.3341113 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 550 [0/25046 (0%)]\tLoss: 0.139218\n",
      "Train epoch: 550 [652300/25046 (80%)]\tLoss: 0.121429\n",
      "Make prediction for 5010 samples...\n",
      "0.2952843 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 551 [0/25046 (0%)]\tLoss: 0.120658\n",
      "Train epoch: 551 [657320/25046 (80%)]\tLoss: 0.130193\n",
      "Make prediction for 5010 samples...\n",
      "0.28105912 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 552 [0/25046 (0%)]\tLoss: 0.149265\n",
      "Train epoch: 552 [658120/25046 (80%)]\tLoss: 0.132760\n",
      "Make prediction for 5010 samples...\n",
      "0.31427413 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 553 [0/25046 (0%)]\tLoss: 0.153107\n",
      "Train epoch: 553 [659740/25046 (80%)]\tLoss: 0.120305\n",
      "Make prediction for 5010 samples...\n",
      "0.28885153 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 554 [0/25046 (0%)]\tLoss: 0.109701\n",
      "Train epoch: 554 [657200/25046 (80%)]\tLoss: 0.130796\n",
      "Make prediction for 5010 samples...\n",
      "0.2862855 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 555 [0/25046 (0%)]\tLoss: 0.112056\n",
      "Train epoch: 555 [648920/25046 (80%)]\tLoss: 0.166664\n",
      "Make prediction for 5010 samples...\n",
      "0.28174067 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 556 [0/25046 (0%)]\tLoss: 0.121871\n",
      "Train epoch: 556 [660560/25046 (80%)]\tLoss: 0.129262\n",
      "Make prediction for 5010 samples...\n",
      "0.28796473 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 557 [0/25046 (0%)]\tLoss: 0.113428\n",
      "Train epoch: 557 [653980/25046 (80%)]\tLoss: 0.137584\n",
      "Make prediction for 5010 samples...\n",
      "0.28884292 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 558 [0/25046 (0%)]\tLoss: 0.124184\n",
      "Train epoch: 558 [658400/25046 (80%)]\tLoss: 0.179252\n",
      "Make prediction for 5010 samples...\n",
      "0.32313654 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 559 [0/25046 (0%)]\tLoss: 0.138204\n",
      "Train epoch: 559 [660080/25046 (80%)]\tLoss: 0.181774\n",
      "Make prediction for 5010 samples...\n",
      "0.30661678 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 560 [0/25046 (0%)]\tLoss: 0.141874\n",
      "Train epoch: 560 [652640/25046 (80%)]\tLoss: 0.135930\n",
      "Make prediction for 5010 samples...\n",
      "0.31600037 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 561 [0/25046 (0%)]\tLoss: 0.139100\n",
      "Train epoch: 561 [654720/25046 (80%)]\tLoss: 0.131719\n",
      "Make prediction for 5010 samples...\n",
      "0.30337197 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 562 [0/25046 (0%)]\tLoss: 0.126128\n",
      "Train epoch: 562 [646420/25046 (80%)]\tLoss: 0.172969\n",
      "Make prediction for 5010 samples...\n",
      "0.33686018 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 563 [0/25046 (0%)]\tLoss: 0.142230\n",
      "Train epoch: 563 [649500/25046 (80%)]\tLoss: 0.136579\n",
      "Make prediction for 5010 samples...\n",
      "0.30145684 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 564 [0/25046 (0%)]\tLoss: 0.149102\n",
      "Train epoch: 564 [655660/25046 (80%)]\tLoss: 0.121988\n",
      "Make prediction for 5010 samples...\n",
      "0.3139318 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 565 [0/25046 (0%)]\tLoss: 0.145567\n",
      "Train epoch: 565 [655880/25046 (80%)]\tLoss: 0.152994\n",
      "Make prediction for 5010 samples...\n",
      "0.31048515 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 566 [0/25046 (0%)]\tLoss: 0.107771\n",
      "Train epoch: 566 [655660/25046 (80%)]\tLoss: 0.116723\n",
      "Make prediction for 5010 samples...\n",
      "0.2886509 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 567 [0/25046 (0%)]\tLoss: 0.121162\n",
      "Train epoch: 567 [660040/25046 (80%)]\tLoss: 0.165097\n",
      "Make prediction for 5010 samples...\n",
      "0.3159766 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 568 [0/25046 (0%)]\tLoss: 0.140012\n",
      "Train epoch: 568 [655000/25046 (80%)]\tLoss: 0.119157\n",
      "Make prediction for 5010 samples...\n",
      "0.30809096 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 569 [0/25046 (0%)]\tLoss: 0.116069\n",
      "Train epoch: 569 [649700/25046 (80%)]\tLoss: 0.120054\n",
      "Make prediction for 5010 samples...\n",
      "0.32221276 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 570 [0/25046 (0%)]\tLoss: 0.121169\n",
      "Train epoch: 570 [663200/25046 (80%)]\tLoss: 0.153676\n",
      "Make prediction for 5010 samples...\n",
      "0.35258073 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 571 [0/25046 (0%)]\tLoss: 0.150494\n",
      "Train epoch: 571 [656240/25046 (80%)]\tLoss: 0.119948\n",
      "Make prediction for 5010 samples...\n",
      "0.3061205 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 572 [0/25046 (0%)]\tLoss: 0.121701\n",
      "Train epoch: 572 [656220/25046 (80%)]\tLoss: 0.113624\n",
      "Make prediction for 5010 samples...\n",
      "0.28600943 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 573 [0/25046 (0%)]\tLoss: 0.120441\n",
      "Train epoch: 573 [655840/25046 (80%)]\tLoss: 0.141172\n",
      "Make prediction for 5010 samples...\n",
      "0.28870288 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 574 [0/25046 (0%)]\tLoss: 0.125600\n",
      "Train epoch: 574 [650360/25046 (80%)]\tLoss: 0.131985\n",
      "Make prediction for 5010 samples...\n",
      "0.30667514 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 575 [0/25046 (0%)]\tLoss: 0.126918\n",
      "Train epoch: 575 [659060/25046 (80%)]\tLoss: 0.124175\n",
      "Make prediction for 5010 samples...\n",
      "0.30225664 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 576 [0/25046 (0%)]\tLoss: 0.139587\n",
      "Train epoch: 576 [657860/25046 (80%)]\tLoss: 0.133426\n",
      "Make prediction for 5010 samples...\n",
      "0.27643067 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 577 [0/25046 (0%)]\tLoss: 0.121141\n",
      "Train epoch: 577 [658120/25046 (80%)]\tLoss: 0.149323\n",
      "Make prediction for 5010 samples...\n",
      "0.2836658 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 578 [0/25046 (0%)]\tLoss: 0.118890\n",
      "Train epoch: 578 [663260/25046 (80%)]\tLoss: 0.117953\n",
      "Make prediction for 5010 samples...\n",
      "0.2954857 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 579 [0/25046 (0%)]\tLoss: 0.131791\n",
      "Train epoch: 579 [649020/25046 (80%)]\tLoss: 0.119020\n",
      "Make prediction for 5010 samples...\n",
      "0.3240461 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 580 [0/25046 (0%)]\tLoss: 0.116420\n",
      "Train epoch: 580 [658200/25046 (80%)]\tLoss: 0.145522\n",
      "Make prediction for 5010 samples...\n",
      "0.3426421 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 581 [0/25046 (0%)]\tLoss: 0.164307\n",
      "Train epoch: 581 [652980/25046 (80%)]\tLoss: 0.126135\n",
      "Make prediction for 5010 samples...\n",
      "0.28775227 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 582 [0/25046 (0%)]\tLoss: 0.151874\n",
      "Train epoch: 582 [661880/25046 (80%)]\tLoss: 0.197878\n",
      "Make prediction for 5010 samples...\n",
      "0.29136857 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 583 [0/25046 (0%)]\tLoss: 0.135462\n",
      "Train epoch: 583 [658040/25046 (80%)]\tLoss: 0.153668\n",
      "Make prediction for 5010 samples...\n",
      "0.32285225 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 584 [0/25046 (0%)]\tLoss: 0.142194\n",
      "Train epoch: 584 [651960/25046 (80%)]\tLoss: 0.149315\n",
      "Make prediction for 5010 samples...\n",
      "0.28830796 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 585 [0/25046 (0%)]\tLoss: 0.136305\n",
      "Train epoch: 585 [657000/25046 (80%)]\tLoss: 0.107558\n",
      "Make prediction for 5010 samples...\n",
      "0.29125834 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 586 [0/25046 (0%)]\tLoss: 0.130906\n",
      "Train epoch: 586 [655580/25046 (80%)]\tLoss: 0.143793\n",
      "Make prediction for 5010 samples...\n",
      "0.28837648 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 587 [0/25046 (0%)]\tLoss: 0.151771\n",
      "Train epoch: 587 [647340/25046 (80%)]\tLoss: 0.170205\n",
      "Make prediction for 5010 samples...\n",
      "0.32165617 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 588 [0/25046 (0%)]\tLoss: 0.118318\n",
      "Train epoch: 588 [661720/25046 (80%)]\tLoss: 0.136036\n",
      "Make prediction for 5010 samples...\n",
      "0.28017545 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 589 [0/25046 (0%)]\tLoss: 0.128702\n",
      "Train epoch: 589 [658260/25046 (80%)]\tLoss: 0.119195\n",
      "Make prediction for 5010 samples...\n",
      "0.27820206 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 590 [0/25046 (0%)]\tLoss: 0.135022\n",
      "Train epoch: 590 [658080/25046 (80%)]\tLoss: 0.165071\n",
      "Make prediction for 5010 samples...\n",
      "0.279885 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 591 [0/25046 (0%)]\tLoss: 0.123762\n",
      "Train epoch: 591 [652300/25046 (80%)]\tLoss: 0.141127\n",
      "Make prediction for 5010 samples...\n",
      "0.31744695 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 592 [0/25046 (0%)]\tLoss: 0.108311\n",
      "Train epoch: 592 [660940/25046 (80%)]\tLoss: 0.115606\n",
      "Make prediction for 5010 samples...\n",
      "0.30938756 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 593 [0/25046 (0%)]\tLoss: 0.127704\n",
      "Train epoch: 593 [654240/25046 (80%)]\tLoss: 0.124236\n",
      "Make prediction for 5010 samples...\n",
      "0.32122445 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 594 [0/25046 (0%)]\tLoss: 0.135935\n",
      "Train epoch: 594 [664000/25046 (80%)]\tLoss: 0.122100\n",
      "Make prediction for 5010 samples...\n",
      "0.2924373 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 595 [0/25046 (0%)]\tLoss: 0.130153\n",
      "Train epoch: 595 [653500/25046 (80%)]\tLoss: 0.124210\n",
      "Make prediction for 5010 samples...\n",
      "0.28171682 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 596 [0/25046 (0%)]\tLoss: 0.135979\n",
      "Train epoch: 596 [657080/25046 (80%)]\tLoss: 0.125627\n",
      "Make prediction for 5010 samples...\n",
      "0.28910872 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 597 [0/25046 (0%)]\tLoss: 0.127956\n",
      "Train epoch: 597 [659040/25046 (80%)]\tLoss: 0.121257\n",
      "Make prediction for 5010 samples...\n",
      "0.3484898 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 598 [0/25046 (0%)]\tLoss: 0.177648\n",
      "Train epoch: 598 [651300/25046 (80%)]\tLoss: 0.102465\n",
      "Make prediction for 5010 samples...\n",
      "0.28711075 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 599 [0/25046 (0%)]\tLoss: 0.122421\n",
      "Train epoch: 599 [658100/25046 (80%)]\tLoss: 0.144326\n",
      "Make prediction for 5010 samples...\n",
      "0.30225778 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 600 [0/25046 (0%)]\tLoss: 0.129475\n",
      "Train epoch: 600 [652300/25046 (80%)]\tLoss: 0.125399\n",
      "Make prediction for 5010 samples...\n",
      "0.31503972 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 601 [0/25046 (0%)]\tLoss: 0.131998\n",
      "Train epoch: 601 [662180/25046 (80%)]\tLoss: 0.135474\n",
      "Make prediction for 5010 samples...\n",
      "0.31069547 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 602 [0/25046 (0%)]\tLoss: 0.112512\n",
      "Train epoch: 602 [655820/25046 (80%)]\tLoss: 0.138362\n",
      "Make prediction for 5010 samples...\n",
      "0.29553935 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 603 [0/25046 (0%)]\tLoss: 0.126141\n",
      "Train epoch: 603 [652140/25046 (80%)]\tLoss: 0.150299\n",
      "Make prediction for 5010 samples...\n",
      "0.28980482 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 604 [0/25046 (0%)]\tLoss: 0.126076\n",
      "Train epoch: 604 [658320/25046 (80%)]\tLoss: 0.130657\n",
      "Make prediction for 5010 samples...\n",
      "0.29410154 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 605 [0/25046 (0%)]\tLoss: 0.118211\n",
      "Train epoch: 605 [653320/25046 (80%)]\tLoss: 0.140373\n",
      "Make prediction for 5010 samples...\n",
      "0.28246066 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 606 [0/25046 (0%)]\tLoss: 0.108940\n",
      "Train epoch: 606 [660980/25046 (80%)]\tLoss: 0.114594\n",
      "Make prediction for 5010 samples...\n",
      "0.2905737 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 607 [0/25046 (0%)]\tLoss: 0.142942\n",
      "Train epoch: 607 [650480/25046 (80%)]\tLoss: 0.132001\n",
      "Make prediction for 5010 samples...\n",
      "0.28428152 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 608 [0/25046 (0%)]\tLoss: 0.117430\n",
      "Train epoch: 608 [663660/25046 (80%)]\tLoss: 0.105783\n",
      "Make prediction for 5010 samples...\n",
      "0.30143577 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 609 [0/25046 (0%)]\tLoss: 0.112913\n",
      "Train epoch: 609 [661620/25046 (80%)]\tLoss: 0.166121\n",
      "Make prediction for 5010 samples...\n",
      "0.28495383 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 610 [0/25046 (0%)]\tLoss: 0.138951\n",
      "Train epoch: 610 [660040/25046 (80%)]\tLoss: 0.125508\n",
      "Make prediction for 5010 samples...\n",
      "0.30217043 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 611 [0/25046 (0%)]\tLoss: 0.121885\n",
      "Train epoch: 611 [657320/25046 (80%)]\tLoss: 0.122266\n",
      "Make prediction for 5010 samples...\n",
      "0.30233547 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 612 [0/25046 (0%)]\tLoss: 0.116483\n",
      "Train epoch: 612 [656100/25046 (80%)]\tLoss: 0.159681\n",
      "Make prediction for 5010 samples...\n",
      "0.28487635 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 613 [0/25046 (0%)]\tLoss: 0.131260\n",
      "Train epoch: 613 [658500/25046 (80%)]\tLoss: 0.117913\n",
      "Make prediction for 5010 samples...\n",
      "0.30783615 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 614 [0/25046 (0%)]\tLoss: 0.122409\n",
      "Train epoch: 614 [657980/25046 (80%)]\tLoss: 0.142183\n",
      "Make prediction for 5010 samples...\n",
      "0.28023508 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 615 [0/25046 (0%)]\tLoss: 0.115717\n",
      "Train epoch: 615 [657580/25046 (80%)]\tLoss: 0.112042\n",
      "Make prediction for 5010 samples...\n",
      "0.28949258 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 616 [0/25046 (0%)]\tLoss: 0.121737\n",
      "Train epoch: 616 [654160/25046 (80%)]\tLoss: 0.143678\n",
      "Make prediction for 5010 samples...\n",
      "0.2784631 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 617 [0/25046 (0%)]\tLoss: 0.108104\n",
      "Train epoch: 617 [653640/25046 (80%)]\tLoss: 0.120827\n",
      "Make prediction for 5010 samples...\n",
      "0.30210194 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 618 [0/25046 (0%)]\tLoss: 0.121514\n",
      "Train epoch: 618 [653420/25046 (80%)]\tLoss: 0.129987\n",
      "Make prediction for 5010 samples...\n",
      "0.29401964 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 619 [0/25046 (0%)]\tLoss: 0.102499\n",
      "Train epoch: 619 [653700/25046 (80%)]\tLoss: 0.140113\n",
      "Make prediction for 5010 samples...\n",
      "0.29283753 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 620 [0/25046 (0%)]\tLoss: 0.131898\n",
      "Train epoch: 620 [654460/25046 (80%)]\tLoss: 0.119328\n",
      "Make prediction for 5010 samples...\n",
      "0.28871712 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 621 [0/25046 (0%)]\tLoss: 0.126049\n",
      "Train epoch: 621 [654860/25046 (80%)]\tLoss: 0.180091\n",
      "Make prediction for 5010 samples...\n",
      "0.29590032 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 622 [0/25046 (0%)]\tLoss: 0.114519\n",
      "Train epoch: 622 [662880/25046 (80%)]\tLoss: 0.152234\n",
      "Make prediction for 5010 samples...\n",
      "0.28417027 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 623 [0/25046 (0%)]\tLoss: 0.128189\n",
      "Train epoch: 623 [655260/25046 (80%)]\tLoss: 0.130984\n",
      "Make prediction for 5010 samples...\n",
      "0.34747577 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 624 [0/25046 (0%)]\tLoss: 0.135669\n",
      "Train epoch: 624 [656180/25046 (80%)]\tLoss: 0.138297\n",
      "Make prediction for 5010 samples...\n",
      "0.29917663 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 625 [0/25046 (0%)]\tLoss: 0.121468\n",
      "Train epoch: 625 [650120/25046 (80%)]\tLoss: 0.138690\n",
      "Make prediction for 5010 samples...\n",
      "0.27909172 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 626 [0/25046 (0%)]\tLoss: 0.118801\n",
      "Train epoch: 626 [659580/25046 (80%)]\tLoss: 0.125718\n",
      "Make prediction for 5010 samples...\n",
      "0.293143 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 627 [0/25046 (0%)]\tLoss: 0.116738\n",
      "Train epoch: 627 [657500/25046 (80%)]\tLoss: 0.153808\n",
      "Make prediction for 5010 samples...\n",
      "0.289736 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 628 [0/25046 (0%)]\tLoss: 0.103249\n",
      "Train epoch: 628 [661060/25046 (80%)]\tLoss: 0.158732\n",
      "Make prediction for 5010 samples...\n",
      "0.2794437 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 629 [0/25046 (0%)]\tLoss: 0.121555\n",
      "Train epoch: 629 [661100/25046 (80%)]\tLoss: 0.122223\n",
      "Make prediction for 5010 samples...\n",
      "0.29050726 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 630 [0/25046 (0%)]\tLoss: 0.120548\n",
      "Train epoch: 630 [653640/25046 (80%)]\tLoss: 0.135356\n",
      "Make prediction for 5010 samples...\n",
      "0.2922644 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 631 [0/25046 (0%)]\tLoss: 0.120232\n",
      "Train epoch: 631 [650620/25046 (80%)]\tLoss: 0.130348\n",
      "Make prediction for 5010 samples...\n",
      "0.28793696 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 632 [0/25046 (0%)]\tLoss: 0.117140\n",
      "Train epoch: 632 [654320/25046 (80%)]\tLoss: 0.155349\n",
      "Make prediction for 5010 samples...\n",
      "0.2899219 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 633 [0/25046 (0%)]\tLoss: 0.127730\n",
      "Train epoch: 633 [659580/25046 (80%)]\tLoss: 0.130562\n",
      "Make prediction for 5010 samples...\n",
      "0.28480685 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 634 [0/25046 (0%)]\tLoss: 0.145094\n",
      "Train epoch: 634 [657600/25046 (80%)]\tLoss: 0.135461\n",
      "Make prediction for 5010 samples...\n",
      "0.29361594 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 635 [0/25046 (0%)]\tLoss: 0.127583\n",
      "Train epoch: 635 [661800/25046 (80%)]\tLoss: 0.134759\n",
      "Make prediction for 5010 samples...\n",
      "0.31924403 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 636 [0/25046 (0%)]\tLoss: 0.126447\n",
      "Train epoch: 636 [654600/25046 (80%)]\tLoss: 0.149361\n",
      "Make prediction for 5010 samples...\n",
      "0.30331543 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 637 [0/25046 (0%)]\tLoss: 0.126633\n",
      "Train epoch: 637 [654360/25046 (80%)]\tLoss: 0.097383\n",
      "Make prediction for 5010 samples...\n",
      "0.28105626 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 638 [0/25046 (0%)]\tLoss: 0.127048\n",
      "Train epoch: 638 [662120/25046 (80%)]\tLoss: 0.114138\n",
      "Make prediction for 5010 samples...\n",
      "0.27746895 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 639 [0/25046 (0%)]\tLoss: 0.147271\n",
      "Train epoch: 639 [657920/25046 (80%)]\tLoss: 0.110100\n",
      "Make prediction for 5010 samples...\n",
      "0.29406276 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 640 [0/25046 (0%)]\tLoss: 0.115895\n",
      "Train epoch: 640 [655560/25046 (80%)]\tLoss: 0.149301\n",
      "Make prediction for 5010 samples...\n",
      "0.2999482 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 641 [0/25046 (0%)]\tLoss: 0.093768\n",
      "Train epoch: 641 [647460/25046 (80%)]\tLoss: 0.112843\n",
      "Make prediction for 5010 samples...\n",
      "0.2840181 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 642 [0/25046 (0%)]\tLoss: 0.109035\n",
      "Train epoch: 642 [652300/25046 (80%)]\tLoss: 0.135706\n",
      "Make prediction for 5010 samples...\n",
      "0.29249257 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 643 [0/25046 (0%)]\tLoss: 0.139259\n",
      "Train epoch: 643 [655480/25046 (80%)]\tLoss: 0.133749\n",
      "Make prediction for 5010 samples...\n",
      "0.33105317 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 644 [0/25046 (0%)]\tLoss: 0.128997\n",
      "Train epoch: 644 [654720/25046 (80%)]\tLoss: 0.131241\n",
      "Make prediction for 5010 samples...\n",
      "0.27892846 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 645 [0/25046 (0%)]\tLoss: 0.130784\n",
      "Train epoch: 645 [656000/25046 (80%)]\tLoss: 0.141740\n",
      "Make prediction for 5010 samples...\n",
      "0.29568353 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 646 [0/25046 (0%)]\tLoss: 0.126488\n",
      "Train epoch: 646 [658320/25046 (80%)]\tLoss: 0.118784\n",
      "Make prediction for 5010 samples...\n",
      "0.28167385 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 647 [0/25046 (0%)]\tLoss: 0.137258\n",
      "Train epoch: 647 [644960/25046 (80%)]\tLoss: 0.119388\n",
      "Make prediction for 5010 samples...\n",
      "0.2976846 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 648 [0/25046 (0%)]\tLoss: 0.144882\n",
      "Train epoch: 648 [659000/25046 (80%)]\tLoss: 0.161217\n",
      "Make prediction for 5010 samples...\n",
      "0.28275383 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 649 [0/25046 (0%)]\tLoss: 0.135463\n",
      "Train epoch: 649 [654080/25046 (80%)]\tLoss: 0.131853\n",
      "Make prediction for 5010 samples...\n",
      "0.28120378 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 650 [0/25046 (0%)]\tLoss: 0.109344\n",
      "Train epoch: 650 [663940/25046 (80%)]\tLoss: 0.151878\n",
      "Make prediction for 5010 samples...\n",
      "0.29471427 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 651 [0/25046 (0%)]\tLoss: 0.125768\n",
      "Train epoch: 651 [656080/25046 (80%)]\tLoss: 0.120228\n",
      "Make prediction for 5010 samples...\n",
      "0.29009962 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 652 [0/25046 (0%)]\tLoss: 0.144803\n",
      "Train epoch: 652 [657740/25046 (80%)]\tLoss: 0.110895\n",
      "Make prediction for 5010 samples...\n",
      "0.27938384 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 653 [0/25046 (0%)]\tLoss: 0.113038\n",
      "Train epoch: 653 [657960/25046 (80%)]\tLoss: 0.162511\n",
      "Make prediction for 5010 samples...\n",
      "0.33383852 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 654 [0/25046 (0%)]\tLoss: 0.136236\n",
      "Train epoch: 654 [654280/25046 (80%)]\tLoss: 0.106965\n",
      "Make prediction for 5010 samples...\n",
      "0.2803141 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 655 [0/25046 (0%)]\tLoss: 0.122570\n",
      "Train epoch: 655 [657040/25046 (80%)]\tLoss: 0.158459\n",
      "Make prediction for 5010 samples...\n",
      "0.28309503 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 656 [0/25046 (0%)]\tLoss: 0.108630\n",
      "Train epoch: 656 [656480/25046 (80%)]\tLoss: 0.127627\n",
      "Make prediction for 5010 samples...\n",
      "0.28262562 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 657 [0/25046 (0%)]\tLoss: 0.117213\n",
      "Train epoch: 657 [659140/25046 (80%)]\tLoss: 0.115160\n",
      "Make prediction for 5010 samples...\n",
      "0.29818538 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 658 [0/25046 (0%)]\tLoss: 0.116256\n",
      "Train epoch: 658 [651220/25046 (80%)]\tLoss: 0.125120\n",
      "Make prediction for 5010 samples...\n",
      "0.3014164 No improvement since epoch  516 ; best_mse,best_ci: 0.27435258 515 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 659 [0/25046 (0%)]\tLoss: 0.120830\n",
      "Train epoch: 659 [655760/25046 (80%)]\tLoss: 0.127376\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 660 [0/25046 (0%)]\tLoss: 0.112505\n",
      "Train epoch: 660 [653820/25046 (80%)]\tLoss: 0.168448\n",
      "Make prediction for 5010 samples...\n",
      "0.27979007 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 661 [0/25046 (0%)]\tLoss: 0.129433\n",
      "Train epoch: 661 [660500/25046 (80%)]\tLoss: 0.141858\n",
      "Make prediction for 5010 samples...\n",
      "0.29494694 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 662 [0/25046 (0%)]\tLoss: 0.127967\n",
      "Train epoch: 662 [660580/25046 (80%)]\tLoss: 0.139998\n",
      "Make prediction for 5010 samples...\n",
      "0.28247905 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 663 [0/25046 (0%)]\tLoss: 0.158541\n",
      "Train epoch: 663 [657440/25046 (80%)]\tLoss: 0.107077\n",
      "Make prediction for 5010 samples...\n",
      "0.30660367 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 664 [0/25046 (0%)]\tLoss: 0.118433\n",
      "Train epoch: 664 [658900/25046 (80%)]\tLoss: 0.112600\n",
      "Make prediction for 5010 samples...\n",
      "0.28246868 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 665 [0/25046 (0%)]\tLoss: 0.112463\n",
      "Train epoch: 665 [654960/25046 (80%)]\tLoss: 0.125665\n",
      "Make prediction for 5010 samples...\n",
      "0.27345398 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 666 [0/25046 (0%)]\tLoss: 0.121625\n",
      "Train epoch: 666 [655580/25046 (80%)]\tLoss: 0.131176\n",
      "Make prediction for 5010 samples...\n",
      "0.3341255 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 667 [0/25046 (0%)]\tLoss: 0.128893\n",
      "Train epoch: 667 [662940/25046 (80%)]\tLoss: 0.107646\n",
      "Make prediction for 5010 samples...\n",
      "0.3148946 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 668 [0/25046 (0%)]\tLoss: 0.130876\n",
      "Train epoch: 668 [654420/25046 (80%)]\tLoss: 0.133421\n",
      "Make prediction for 5010 samples...\n",
      "0.28835914 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 669 [0/25046 (0%)]\tLoss: 0.132251\n",
      "Train epoch: 669 [658300/25046 (80%)]\tLoss: 0.190379\n",
      "Make prediction for 5010 samples...\n",
      "0.31172293 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 670 [0/25046 (0%)]\tLoss: 0.159764\n",
      "Train epoch: 670 [661620/25046 (80%)]\tLoss: 0.117564\n",
      "Make prediction for 5010 samples...\n",
      "0.28465748 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 671 [0/25046 (0%)]\tLoss: 0.102564\n",
      "Train epoch: 671 [657080/25046 (80%)]\tLoss: 0.113082\n",
      "Make prediction for 5010 samples...\n",
      "0.29439622 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 672 [0/25046 (0%)]\tLoss: 0.138541\n",
      "Train epoch: 672 [656060/25046 (80%)]\tLoss: 0.146338\n",
      "Make prediction for 5010 samples...\n",
      "0.3769759 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 673 [0/25046 (0%)]\tLoss: 0.144703\n",
      "Train epoch: 673 [651740/25046 (80%)]\tLoss: 0.158115\n",
      "Make prediction for 5010 samples...\n",
      "0.2869697 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 674 [0/25046 (0%)]\tLoss: 0.139391\n",
      "Train epoch: 674 [654660/25046 (80%)]\tLoss: 0.123125\n",
      "Make prediction for 5010 samples...\n",
      "0.3006069 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 675 [0/25046 (0%)]\tLoss: 0.130634\n",
      "Train epoch: 675 [661620/25046 (80%)]\tLoss: 0.126350\n",
      "Make prediction for 5010 samples...\n",
      "0.27918592 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 676 [0/25046 (0%)]\tLoss: 0.123151\n",
      "Train epoch: 676 [656220/25046 (80%)]\tLoss: 0.107179\n",
      "Make prediction for 5010 samples...\n",
      "0.28128305 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 677 [0/25046 (0%)]\tLoss: 0.130082\n",
      "Train epoch: 677 [647520/25046 (80%)]\tLoss: 0.126140\n",
      "Make prediction for 5010 samples...\n",
      "0.2837254 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 678 [0/25046 (0%)]\tLoss: 0.115275\n",
      "Train epoch: 678 [656580/25046 (80%)]\tLoss: 0.127166\n",
      "Make prediction for 5010 samples...\n",
      "0.30369064 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 679 [0/25046 (0%)]\tLoss: 0.128698\n",
      "Train epoch: 679 [652060/25046 (80%)]\tLoss: 0.131148\n",
      "Make prediction for 5010 samples...\n",
      "0.29979944 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 680 [0/25046 (0%)]\tLoss: 0.123951\n",
      "Train epoch: 680 [660540/25046 (80%)]\tLoss: 0.122577\n",
      "Make prediction for 5010 samples...\n",
      "0.2972647 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 681 [0/25046 (0%)]\tLoss: 0.120660\n",
      "Train epoch: 681 [662680/25046 (80%)]\tLoss: 0.122513\n",
      "Make prediction for 5010 samples...\n",
      "0.28940248 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 682 [0/25046 (0%)]\tLoss: 0.160996\n",
      "Train epoch: 682 [662240/25046 (80%)]\tLoss: 0.121472\n",
      "Make prediction for 5010 samples...\n",
      "0.2822308 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 683 [0/25046 (0%)]\tLoss: 0.119963\n",
      "Train epoch: 683 [652480/25046 (80%)]\tLoss: 0.120065\n",
      "Make prediction for 5010 samples...\n",
      "0.27842146 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 684 [0/25046 (0%)]\tLoss: 0.111997\n",
      "Train epoch: 684 [659600/25046 (80%)]\tLoss: 0.112996\n",
      "Make prediction for 5010 samples...\n",
      "0.31961593 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 685 [0/25046 (0%)]\tLoss: 0.128833\n",
      "Train epoch: 685 [656420/25046 (80%)]\tLoss: 0.127911\n",
      "Make prediction for 5010 samples...\n",
      "0.2896677 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 686 [0/25046 (0%)]\tLoss: 0.118529\n",
      "Train epoch: 686 [652820/25046 (80%)]\tLoss: 0.109031\n",
      "Make prediction for 5010 samples...\n",
      "0.2999629 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 687 [0/25046 (0%)]\tLoss: 0.124679\n",
      "Train epoch: 687 [656160/25046 (80%)]\tLoss: 0.110406\n",
      "Make prediction for 5010 samples...\n",
      "0.2877907 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 688 [0/25046 (0%)]\tLoss: 0.101139\n",
      "Train epoch: 688 [655420/25046 (80%)]\tLoss: 0.144756\n",
      "Make prediction for 5010 samples...\n",
      "0.30532593 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 689 [0/25046 (0%)]\tLoss: 0.119363\n",
      "Train epoch: 689 [653300/25046 (80%)]\tLoss: 0.125592\n",
      "Make prediction for 5010 samples...\n",
      "0.2967042 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 690 [0/25046 (0%)]\tLoss: 0.124070\n",
      "Train epoch: 690 [655020/25046 (80%)]\tLoss: 0.125698\n",
      "Make prediction for 5010 samples...\n",
      "0.28740093 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 691 [0/25046 (0%)]\tLoss: 0.128449\n",
      "Train epoch: 691 [659140/25046 (80%)]\tLoss: 0.111767\n",
      "Make prediction for 5010 samples...\n",
      "0.2810469 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 692 [0/25046 (0%)]\tLoss: 0.116147\n",
      "Train epoch: 692 [646780/25046 (80%)]\tLoss: 0.163416\n",
      "Make prediction for 5010 samples...\n",
      "0.3170282 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 693 [0/25046 (0%)]\tLoss: 0.126890\n",
      "Train epoch: 693 [654540/25046 (80%)]\tLoss: 0.114048\n",
      "Make prediction for 5010 samples...\n",
      "0.2957994 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 694 [0/25046 (0%)]\tLoss: 0.119618\n",
      "Train epoch: 694 [660620/25046 (80%)]\tLoss: 0.135544\n",
      "Make prediction for 5010 samples...\n",
      "0.2804898 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 695 [0/25046 (0%)]\tLoss: 0.112045\n",
      "Train epoch: 695 [658020/25046 (80%)]\tLoss: 0.118163\n",
      "Make prediction for 5010 samples...\n",
      "0.2782816 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 696 [0/25046 (0%)]\tLoss: 0.108936\n",
      "Train epoch: 696 [660040/25046 (80%)]\tLoss: 0.113953\n",
      "Make prediction for 5010 samples...\n",
      "0.28236586 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 697 [0/25046 (0%)]\tLoss: 0.115856\n",
      "Train epoch: 697 [659500/25046 (80%)]\tLoss: 0.128350\n",
      "Make prediction for 5010 samples...\n",
      "0.3294672 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 698 [0/25046 (0%)]\tLoss: 0.138062\n",
      "Train epoch: 698 [658580/25046 (80%)]\tLoss: 0.128845\n",
      "Make prediction for 5010 samples...\n",
      "0.2920951 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 699 [0/25046 (0%)]\tLoss: 0.119259\n",
      "Train epoch: 699 [655060/25046 (80%)]\tLoss: 0.128511\n",
      "Make prediction for 5010 samples...\n",
      "0.278814 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 700 [0/25046 (0%)]\tLoss: 0.111902\n",
      "Train epoch: 700 [651540/25046 (80%)]\tLoss: 0.130408\n",
      "Make prediction for 5010 samples...\n",
      "0.2873358 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 701 [0/25046 (0%)]\tLoss: 0.146141\n",
      "Train epoch: 701 [658020/25046 (80%)]\tLoss: 0.165548\n",
      "Make prediction for 5010 samples...\n",
      "0.2812269 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 702 [0/25046 (0%)]\tLoss: 0.134028\n",
      "Train epoch: 702 [654940/25046 (80%)]\tLoss: 0.119575\n",
      "Make prediction for 5010 samples...\n",
      "0.3353391 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 703 [0/25046 (0%)]\tLoss: 0.139154\n",
      "Train epoch: 703 [660280/25046 (80%)]\tLoss: 0.146690\n",
      "Make prediction for 5010 samples...\n",
      "0.32034442 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 704 [0/25046 (0%)]\tLoss: 0.132652\n",
      "Train epoch: 704 [657320/25046 (80%)]\tLoss: 0.116067\n",
      "Make prediction for 5010 samples...\n",
      "0.30011672 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 705 [0/25046 (0%)]\tLoss: 0.125083\n",
      "Train epoch: 705 [656300/25046 (80%)]\tLoss: 0.128067\n",
      "Make prediction for 5010 samples...\n",
      "0.3487142 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 706 [0/25046 (0%)]\tLoss: 0.156988\n",
      "Train epoch: 706 [655900/25046 (80%)]\tLoss: 0.144766\n",
      "Make prediction for 5010 samples...\n",
      "0.31128612 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 707 [0/25046 (0%)]\tLoss: 0.137544\n",
      "Train epoch: 707 [659120/25046 (80%)]\tLoss: 0.127146\n",
      "Make prediction for 5010 samples...\n",
      "0.28434777 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 708 [0/25046 (0%)]\tLoss: 0.112189\n",
      "Train epoch: 708 [660900/25046 (80%)]\tLoss: 0.130973\n",
      "Make prediction for 5010 samples...\n",
      "0.28620303 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 709 [0/25046 (0%)]\tLoss: 0.147839\n",
      "Train epoch: 709 [657680/25046 (80%)]\tLoss: 0.137112\n",
      "Make prediction for 5010 samples...\n",
      "0.3086229 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 710 [0/25046 (0%)]\tLoss: 0.124245\n",
      "Train epoch: 710 [659080/25046 (80%)]\tLoss: 0.118202\n",
      "Make prediction for 5010 samples...\n",
      "0.28446287 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 711 [0/25046 (0%)]\tLoss: 0.123598\n",
      "Train epoch: 711 [664160/25046 (80%)]\tLoss: 0.113277\n",
      "Make prediction for 5010 samples...\n",
      "0.29864126 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 712 [0/25046 (0%)]\tLoss: 0.145678\n",
      "Train epoch: 712 [657940/25046 (80%)]\tLoss: 0.119320\n",
      "Make prediction for 5010 samples...\n",
      "0.27457565 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 713 [0/25046 (0%)]\tLoss: 0.121366\n",
      "Train epoch: 713 [657440/25046 (80%)]\tLoss: 0.108736\n",
      "Make prediction for 5010 samples...\n",
      "0.28145012 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 714 [0/25046 (0%)]\tLoss: 0.108448\n",
      "Train epoch: 714 [661060/25046 (80%)]\tLoss: 0.145879\n",
      "Make prediction for 5010 samples...\n",
      "0.28369284 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 715 [0/25046 (0%)]\tLoss: 0.119186\n",
      "Train epoch: 715 [660440/25046 (80%)]\tLoss: 0.135573\n",
      "Make prediction for 5010 samples...\n",
      "0.2885694 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 716 [0/25046 (0%)]\tLoss: 0.147209\n",
      "Train epoch: 716 [659620/25046 (80%)]\tLoss: 0.127640\n",
      "Make prediction for 5010 samples...\n",
      "0.29055104 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 717 [0/25046 (0%)]\tLoss: 0.098312\n",
      "Train epoch: 717 [660200/25046 (80%)]\tLoss: 0.115221\n",
      "Make prediction for 5010 samples...\n",
      "0.27448598 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 718 [0/25046 (0%)]\tLoss: 0.115921\n",
      "Train epoch: 718 [655540/25046 (80%)]\tLoss: 0.108481\n",
      "Make prediction for 5010 samples...\n",
      "0.29833516 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 719 [0/25046 (0%)]\tLoss: 0.122312\n",
      "Train epoch: 719 [655080/25046 (80%)]\tLoss: 0.136027\n",
      "Make prediction for 5010 samples...\n",
      "0.27186087 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 720 [0/25046 (0%)]\tLoss: 0.115092\n",
      "Train epoch: 720 [653760/25046 (80%)]\tLoss: 0.130989\n",
      "Make prediction for 5010 samples...\n",
      "0.28095278 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 721 [0/25046 (0%)]\tLoss: 0.120882\n",
      "Train epoch: 721 [666760/25046 (80%)]\tLoss: 0.131790\n",
      "Make prediction for 5010 samples...\n",
      "0.28604594 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 722 [0/25046 (0%)]\tLoss: 0.112505\n",
      "Train epoch: 722 [656860/25046 (80%)]\tLoss: 0.129653\n",
      "Make prediction for 5010 samples...\n",
      "0.2834099 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 723 [0/25046 (0%)]\tLoss: 0.103300\n",
      "Train epoch: 723 [653140/25046 (80%)]\tLoss: 0.122481\n",
      "Make prediction for 5010 samples...\n",
      "0.2970693 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 724 [0/25046 (0%)]\tLoss: 0.125086\n",
      "Train epoch: 724 [652060/25046 (80%)]\tLoss: 0.121088\n",
      "Make prediction for 5010 samples...\n",
      "0.284659 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 725 [0/25046 (0%)]\tLoss: 0.120022\n",
      "Train epoch: 725 [656560/25046 (80%)]\tLoss: 0.130877\n",
      "Make prediction for 5010 samples...\n",
      "0.35357922 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 726 [0/25046 (0%)]\tLoss: 0.131623\n",
      "Train epoch: 726 [650700/25046 (80%)]\tLoss: 0.131815\n",
      "Make prediction for 5010 samples...\n",
      "0.28110522 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 727 [0/25046 (0%)]\tLoss: 0.116207\n",
      "Train epoch: 727 [652240/25046 (80%)]\tLoss: 0.108986\n",
      "Make prediction for 5010 samples...\n",
      "0.27589637 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 728 [0/25046 (0%)]\tLoss: 0.106219\n",
      "Train epoch: 728 [655780/25046 (80%)]\tLoss: 0.143550\n",
      "Make prediction for 5010 samples...\n",
      "0.28068492 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 729 [0/25046 (0%)]\tLoss: 0.119713\n",
      "Train epoch: 729 [655660/25046 (80%)]\tLoss: 0.179714\n",
      "Make prediction for 5010 samples...\n",
      "0.2834336 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 730 [0/25046 (0%)]\tLoss: 0.152278\n",
      "Train epoch: 730 [659780/25046 (80%)]\tLoss: 0.154922\n",
      "Make prediction for 5010 samples...\n",
      "0.30417407 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 731 [0/25046 (0%)]\tLoss: 0.106075\n",
      "Train epoch: 731 [656600/25046 (80%)]\tLoss: 0.155234\n",
      "Make prediction for 5010 samples...\n",
      "0.31546673 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 732 [0/25046 (0%)]\tLoss: 0.121159\n",
      "Train epoch: 732 [654760/25046 (80%)]\tLoss: 0.117934\n",
      "Make prediction for 5010 samples...\n",
      "0.274736 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 733 [0/25046 (0%)]\tLoss: 0.119459\n",
      "Train epoch: 733 [656740/25046 (80%)]\tLoss: 0.123561\n",
      "Make prediction for 5010 samples...\n",
      "0.3301066 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 734 [0/25046 (0%)]\tLoss: 0.130488\n",
      "Train epoch: 734 [658520/25046 (80%)]\tLoss: 0.123660\n",
      "Make prediction for 5010 samples...\n",
      "0.35083762 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 735 [0/25046 (0%)]\tLoss: 0.147588\n",
      "Train epoch: 735 [653360/25046 (80%)]\tLoss: 0.107983\n",
      "Make prediction for 5010 samples...\n",
      "0.275184 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 736 [0/25046 (0%)]\tLoss: 0.134468\n",
      "Train epoch: 736 [651720/25046 (80%)]\tLoss: 0.120300\n",
      "Make prediction for 5010 samples...\n",
      "0.28762004 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 737 [0/25046 (0%)]\tLoss: 0.104775\n",
      "Train epoch: 737 [655740/25046 (80%)]\tLoss: 0.111631\n",
      "Make prediction for 5010 samples...\n",
      "0.28963625 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 738 [0/25046 (0%)]\tLoss: 0.141466\n",
      "Train epoch: 738 [653700/25046 (80%)]\tLoss: 0.107673\n",
      "Make prediction for 5010 samples...\n",
      "0.2760658 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 739 [0/25046 (0%)]\tLoss: 0.140141\n",
      "Train epoch: 739 [653680/25046 (80%)]\tLoss: 0.151701\n",
      "Make prediction for 5010 samples...\n",
      "0.30291146 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 740 [0/25046 (0%)]\tLoss: 0.113880\n",
      "Train epoch: 740 [654260/25046 (80%)]\tLoss: 0.118570\n",
      "Make prediction for 5010 samples...\n",
      "0.2739125 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 741 [0/25046 (0%)]\tLoss: 0.115220\n",
      "Train epoch: 741 [654140/25046 (80%)]\tLoss: 0.115154\n",
      "Make prediction for 5010 samples...\n",
      "0.28507638 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 742 [0/25046 (0%)]\tLoss: 0.124606\n",
      "Train epoch: 742 [654680/25046 (80%)]\tLoss: 0.150191\n",
      "Make prediction for 5010 samples...\n",
      "0.32069042 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 743 [0/25046 (0%)]\tLoss: 0.131170\n",
      "Train epoch: 743 [660180/25046 (80%)]\tLoss: 0.125994\n",
      "Make prediction for 5010 samples...\n",
      "0.305653 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 744 [0/25046 (0%)]\tLoss: 0.126631\n",
      "Train epoch: 744 [650780/25046 (80%)]\tLoss: 0.109694\n",
      "Make prediction for 5010 samples...\n",
      "0.28899485 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 745 [0/25046 (0%)]\tLoss: 0.099495\n",
      "Train epoch: 745 [662100/25046 (80%)]\tLoss: 0.146882\n",
      "Make prediction for 5010 samples...\n",
      "0.278811 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 746 [0/25046 (0%)]\tLoss: 0.116761\n",
      "Train epoch: 746 [652160/25046 (80%)]\tLoss: 0.141553\n",
      "Make prediction for 5010 samples...\n",
      "0.27907732 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 747 [0/25046 (0%)]\tLoss: 0.112488\n",
      "Train epoch: 747 [659560/25046 (80%)]\tLoss: 0.183932\n",
      "Make prediction for 5010 samples...\n",
      "0.3516396 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 748 [0/25046 (0%)]\tLoss: 0.150195\n",
      "Train epoch: 748 [661160/25046 (80%)]\tLoss: 0.126936\n",
      "Make prediction for 5010 samples...\n",
      "0.28504196 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 749 [0/25046 (0%)]\tLoss: 0.092142\n",
      "Train epoch: 749 [655160/25046 (80%)]\tLoss: 0.108825\n",
      "Make prediction for 5010 samples...\n",
      "0.27727762 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 750 [0/25046 (0%)]\tLoss: 0.101064\n",
      "Train epoch: 750 [657480/25046 (80%)]\tLoss: 0.145167\n",
      "Make prediction for 5010 samples...\n",
      "0.3029538 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 751 [0/25046 (0%)]\tLoss: 0.142702\n",
      "Train epoch: 751 [656760/25046 (80%)]\tLoss: 0.126404\n",
      "Make prediction for 5010 samples...\n",
      "0.31923825 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 752 [0/25046 (0%)]\tLoss: 0.133593\n",
      "Train epoch: 752 [653160/25046 (80%)]\tLoss: 0.135486\n",
      "Make prediction for 5010 samples...\n",
      "0.27189502 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 753 [0/25046 (0%)]\tLoss: 0.130837\n",
      "Train epoch: 753 [656220/25046 (80%)]\tLoss: 0.098302\n",
      "Make prediction for 5010 samples...\n",
      "0.27946144 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 754 [0/25046 (0%)]\tLoss: 0.106392\n",
      "Train epoch: 754 [657960/25046 (80%)]\tLoss: 0.112754\n",
      "Make prediction for 5010 samples...\n",
      "0.3330936 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 755 [0/25046 (0%)]\tLoss: 0.117734\n",
      "Train epoch: 755 [652840/25046 (80%)]\tLoss: 0.127356\n",
      "Make prediction for 5010 samples...\n",
      "0.28748217 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 756 [0/25046 (0%)]\tLoss: 0.126998\n",
      "Train epoch: 756 [653840/25046 (80%)]\tLoss: 0.134188\n",
      "Make prediction for 5010 samples...\n",
      "0.27462673 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 757 [0/25046 (0%)]\tLoss: 0.121746\n",
      "Train epoch: 757 [654680/25046 (80%)]\tLoss: 0.151897\n",
      "Make prediction for 5010 samples...\n",
      "0.3006718 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 758 [0/25046 (0%)]\tLoss: 0.127887\n",
      "Train epoch: 758 [655300/25046 (80%)]\tLoss: 0.119314\n",
      "Make prediction for 5010 samples...\n",
      "0.28315544 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 759 [0/25046 (0%)]\tLoss: 0.090729\n",
      "Train epoch: 759 [669400/25046 (80%)]\tLoss: 0.123430\n",
      "Make prediction for 5010 samples...\n",
      "0.3447002 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 760 [0/25046 (0%)]\tLoss: 0.133900\n",
      "Train epoch: 760 [655640/25046 (80%)]\tLoss: 0.106348\n",
      "Make prediction for 5010 samples...\n",
      "0.3118092 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 761 [0/25046 (0%)]\tLoss: 0.097799\n",
      "Train epoch: 761 [655700/25046 (80%)]\tLoss: 0.126517\n",
      "Make prediction for 5010 samples...\n",
      "0.28183517 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 762 [0/25046 (0%)]\tLoss: 0.103750\n",
      "Train epoch: 762 [655840/25046 (80%)]\tLoss: 0.130489\n",
      "Make prediction for 5010 samples...\n",
      "0.28999126 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 763 [0/25046 (0%)]\tLoss: 0.124494\n",
      "Train epoch: 763 [655800/25046 (80%)]\tLoss: 0.111909\n",
      "Make prediction for 5010 samples...\n",
      "0.2823643 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 764 [0/25046 (0%)]\tLoss: 0.100166\n",
      "Train epoch: 764 [655680/25046 (80%)]\tLoss: 0.148859\n",
      "Make prediction for 5010 samples...\n",
      "0.29171607 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 765 [0/25046 (0%)]\tLoss: 0.115796\n",
      "Train epoch: 765 [651300/25046 (80%)]\tLoss: 0.094007\n",
      "Make prediction for 5010 samples...\n",
      "0.29302168 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 766 [0/25046 (0%)]\tLoss: 0.128711\n",
      "Train epoch: 766 [650260/25046 (80%)]\tLoss: 0.125018\n",
      "Make prediction for 5010 samples...\n",
      "0.2931852 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 767 [0/25046 (0%)]\tLoss: 0.117924\n",
      "Train epoch: 767 [651500/25046 (80%)]\tLoss: 0.142448\n",
      "Make prediction for 5010 samples...\n",
      "0.284236 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 768 [0/25046 (0%)]\tLoss: 0.101610\n",
      "Train epoch: 768 [655960/25046 (80%)]\tLoss: 0.135567\n",
      "Make prediction for 5010 samples...\n",
      "0.29139084 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 769 [0/25046 (0%)]\tLoss: 0.106313\n",
      "Train epoch: 769 [651940/25046 (80%)]\tLoss: 0.109583\n",
      "Make prediction for 5010 samples...\n",
      "0.28996363 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 770 [0/25046 (0%)]\tLoss: 0.120522\n",
      "Train epoch: 770 [656160/25046 (80%)]\tLoss: 0.137207\n",
      "Make prediction for 5010 samples...\n",
      "0.32691067 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 771 [0/25046 (0%)]\tLoss: 0.114099\n",
      "Train epoch: 771 [656880/25046 (80%)]\tLoss: 0.108451\n",
      "Make prediction for 5010 samples...\n",
      "0.2827141 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 772 [0/25046 (0%)]\tLoss: 0.123886\n",
      "Train epoch: 772 [658040/25046 (80%)]\tLoss: 0.107359\n",
      "Make prediction for 5010 samples...\n",
      "0.28474072 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 773 [0/25046 (0%)]\tLoss: 0.110779\n",
      "Train epoch: 773 [656040/25046 (80%)]\tLoss: 0.112750\n",
      "Make prediction for 5010 samples...\n",
      "0.29842716 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 774 [0/25046 (0%)]\tLoss: 0.100042\n",
      "Train epoch: 774 [658740/25046 (80%)]\tLoss: 0.110131\n",
      "Make prediction for 5010 samples...\n",
      "0.3320777 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 775 [0/25046 (0%)]\tLoss: 0.148328\n",
      "Train epoch: 775 [659300/25046 (80%)]\tLoss: 0.102359\n",
      "Make prediction for 5010 samples...\n",
      "0.31942964 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 776 [0/25046 (0%)]\tLoss: 0.119237\n",
      "Train epoch: 776 [658720/25046 (80%)]\tLoss: 0.136676\n",
      "Make prediction for 5010 samples...\n",
      "0.28525636 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 777 [0/25046 (0%)]\tLoss: 0.102087\n",
      "Train epoch: 777 [659840/25046 (80%)]\tLoss: 0.112182\n",
      "Make prediction for 5010 samples...\n",
      "0.30192274 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 778 [0/25046 (0%)]\tLoss: 0.111047\n",
      "Train epoch: 778 [655580/25046 (80%)]\tLoss: 0.116789\n",
      "Make prediction for 5010 samples...\n",
      "0.29857177 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 779 [0/25046 (0%)]\tLoss: 0.147086\n",
      "Train epoch: 779 [662120/25046 (80%)]\tLoss: 0.105911\n",
      "Make prediction for 5010 samples...\n",
      "0.27888808 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 780 [0/25046 (0%)]\tLoss: 0.111579\n",
      "Train epoch: 780 [659940/25046 (80%)]\tLoss: 0.119869\n",
      "Make prediction for 5010 samples...\n",
      "0.2764318 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 781 [0/25046 (0%)]\tLoss: 0.116425\n",
      "Train epoch: 781 [654300/25046 (80%)]\tLoss: 0.135429\n",
      "Make prediction for 5010 samples...\n",
      "0.2685681 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 782 [0/25046 (0%)]\tLoss: 0.135254\n",
      "Train epoch: 782 [648900/25046 (80%)]\tLoss: 0.127770\n",
      "Make prediction for 5010 samples...\n",
      "0.29418117 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 783 [0/25046 (0%)]\tLoss: 0.111371\n",
      "Train epoch: 783 [653060/25046 (80%)]\tLoss: 0.121444\n",
      "Make prediction for 5010 samples...\n",
      "0.27513993 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 784 [0/25046 (0%)]\tLoss: 0.107354\n",
      "Train epoch: 784 [651860/25046 (80%)]\tLoss: 0.166560\n",
      "Make prediction for 5010 samples...\n",
      "0.28180405 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 785 [0/25046 (0%)]\tLoss: 0.172262\n",
      "Train epoch: 785 [655440/25046 (80%)]\tLoss: 0.102309\n",
      "Make prediction for 5010 samples...\n",
      "0.3008467 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 786 [0/25046 (0%)]\tLoss: 0.118878\n",
      "Train epoch: 786 [653340/25046 (80%)]\tLoss: 0.148029\n",
      "Make prediction for 5010 samples...\n",
      "0.28046447 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 787 [0/25046 (0%)]\tLoss: 0.137705\n",
      "Train epoch: 787 [660020/25046 (80%)]\tLoss: 0.114078\n",
      "Make prediction for 5010 samples...\n",
      "0.29079434 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 788 [0/25046 (0%)]\tLoss: 0.116462\n",
      "Train epoch: 788 [653460/25046 (80%)]\tLoss: 0.120380\n",
      "Make prediction for 5010 samples...\n",
      "0.30586782 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 789 [0/25046 (0%)]\tLoss: 0.121556\n",
      "Train epoch: 789 [654320/25046 (80%)]\tLoss: 0.138434\n",
      "Make prediction for 5010 samples...\n",
      "0.28116408 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 790 [0/25046 (0%)]\tLoss: 0.110523\n",
      "Train epoch: 790 [660080/25046 (80%)]\tLoss: 0.123943\n",
      "Make prediction for 5010 samples...\n",
      "0.28344727 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 791 [0/25046 (0%)]\tLoss: 0.108251\n",
      "Train epoch: 791 [654680/25046 (80%)]\tLoss: 0.119794\n",
      "Make prediction for 5010 samples...\n",
      "0.2743914 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 792 [0/25046 (0%)]\tLoss: 0.124345\n",
      "Train epoch: 792 [660060/25046 (80%)]\tLoss: 0.124714\n",
      "Make prediction for 5010 samples...\n",
      "0.2997311 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 793 [0/25046 (0%)]\tLoss: 0.121145\n",
      "Train epoch: 793 [655220/25046 (80%)]\tLoss: 0.114605\n",
      "Make prediction for 5010 samples...\n",
      "0.28036806 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 794 [0/25046 (0%)]\tLoss: 0.107894\n",
      "Train epoch: 794 [658500/25046 (80%)]\tLoss: 0.144735\n",
      "Make prediction for 5010 samples...\n",
      "0.28147402 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 795 [0/25046 (0%)]\tLoss: 0.146557\n",
      "Train epoch: 795 [652560/25046 (80%)]\tLoss: 0.115165\n",
      "Make prediction for 5010 samples...\n",
      "0.30096158 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 796 [0/25046 (0%)]\tLoss: 0.105558\n",
      "Train epoch: 796 [653620/25046 (80%)]\tLoss: 0.136832\n",
      "Make prediction for 5010 samples...\n",
      "0.28425962 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 797 [0/25046 (0%)]\tLoss: 0.117387\n",
      "Train epoch: 797 [659140/25046 (80%)]\tLoss: 0.131616\n",
      "Make prediction for 5010 samples...\n",
      "0.29202726 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 798 [0/25046 (0%)]\tLoss: 0.106162\n",
      "Train epoch: 798 [662640/25046 (80%)]\tLoss: 0.102008\n",
      "Make prediction for 5010 samples...\n",
      "0.27591184 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 799 [0/25046 (0%)]\tLoss: 0.106019\n",
      "Train epoch: 799 [661420/25046 (80%)]\tLoss: 0.115544\n",
      "Make prediction for 5010 samples...\n",
      "0.28402308 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 800 [0/25046 (0%)]\tLoss: 0.092515\n",
      "Train epoch: 800 [655940/25046 (80%)]\tLoss: 0.116261\n",
      "Make prediction for 5010 samples...\n",
      "0.27504483 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 801 [0/25046 (0%)]\tLoss: 0.108595\n",
      "Train epoch: 801 [654500/25046 (80%)]\tLoss: 0.116138\n",
      "Make prediction for 5010 samples...\n",
      "0.29150254 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 802 [0/25046 (0%)]\tLoss: 0.122631\n",
      "Train epoch: 802 [659180/25046 (80%)]\tLoss: 0.117164\n",
      "Make prediction for 5010 samples...\n",
      "0.29246137 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 803 [0/25046 (0%)]\tLoss: 0.099862\n",
      "Train epoch: 803 [654320/25046 (80%)]\tLoss: 0.128187\n",
      "Make prediction for 5010 samples...\n",
      "0.27487916 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 804 [0/25046 (0%)]\tLoss: 0.122795\n",
      "Train epoch: 804 [656600/25046 (80%)]\tLoss: 0.141777\n",
      "Make prediction for 5010 samples...\n",
      "0.27866888 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 805 [0/25046 (0%)]\tLoss: 0.111791\n",
      "Train epoch: 805 [657540/25046 (80%)]\tLoss: 0.107890\n",
      "Make prediction for 5010 samples...\n",
      "0.28602472 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 806 [0/25046 (0%)]\tLoss: 0.103344\n",
      "Train epoch: 806 [655980/25046 (80%)]\tLoss: 0.131365\n",
      "Make prediction for 5010 samples...\n",
      "0.28650254 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 807 [0/25046 (0%)]\tLoss: 0.108211\n",
      "Train epoch: 807 [659440/25046 (80%)]\tLoss: 0.135784\n",
      "Make prediction for 5010 samples...\n",
      "0.2759658 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 808 [0/25046 (0%)]\tLoss: 0.129714\n",
      "Train epoch: 808 [658280/25046 (80%)]\tLoss: 0.110857\n",
      "Make prediction for 5010 samples...\n",
      "0.27619547 No improvement since epoch  659 ; best_mse,best_ci: 0.26853764 658 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 809 [0/25046 (0%)]\tLoss: 0.115022\n",
      "Train epoch: 809 [651780/25046 (80%)]\tLoss: 0.116067\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 810 [0/25046 (0%)]\tLoss: 0.101478\n",
      "Train epoch: 810 [655500/25046 (80%)]\tLoss: 0.110411\n",
      "Make prediction for 5010 samples...\n",
      "0.29949865 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 811 [0/25046 (0%)]\tLoss: 0.098077\n",
      "Train epoch: 811 [658880/25046 (80%)]\tLoss: 0.143873\n",
      "Make prediction for 5010 samples...\n",
      "0.36949825 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 812 [0/25046 (0%)]\tLoss: 0.146563\n",
      "Train epoch: 812 [664340/25046 (80%)]\tLoss: 0.124915\n",
      "Make prediction for 5010 samples...\n",
      "0.32142618 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 813 [0/25046 (0%)]\tLoss: 0.142260\n",
      "Train epoch: 813 [651800/25046 (80%)]\tLoss: 0.104317\n",
      "Make prediction for 5010 samples...\n",
      "0.2789015 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 814 [0/25046 (0%)]\tLoss: 0.113966\n",
      "Train epoch: 814 [660420/25046 (80%)]\tLoss: 0.114939\n",
      "Make prediction for 5010 samples...\n",
      "0.2739169 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 815 [0/25046 (0%)]\tLoss: 0.118875\n",
      "Train epoch: 815 [654720/25046 (80%)]\tLoss: 0.119067\n",
      "Make prediction for 5010 samples...\n",
      "0.27284974 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 816 [0/25046 (0%)]\tLoss: 0.116327\n",
      "Train epoch: 816 [653200/25046 (80%)]\tLoss: 0.130287\n",
      "Make prediction for 5010 samples...\n",
      "0.27687383 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 817 [0/25046 (0%)]\tLoss: 0.101004\n",
      "Train epoch: 817 [655360/25046 (80%)]\tLoss: 0.105480\n",
      "Make prediction for 5010 samples...\n",
      "0.27597606 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 818 [0/25046 (0%)]\tLoss: 0.109179\n",
      "Train epoch: 818 [659280/25046 (80%)]\tLoss: 0.098962\n",
      "Make prediction for 5010 samples...\n",
      "0.2762 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 819 [0/25046 (0%)]\tLoss: 0.127098\n",
      "Train epoch: 819 [655020/25046 (80%)]\tLoss: 0.090739\n",
      "Make prediction for 5010 samples...\n",
      "0.27305335 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 820 [0/25046 (0%)]\tLoss: 0.109702\n",
      "Train epoch: 820 [656780/25046 (80%)]\tLoss: 0.110727\n",
      "Make prediction for 5010 samples...\n",
      "0.28646904 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 821 [0/25046 (0%)]\tLoss: 0.097694\n",
      "Train epoch: 821 [653600/25046 (80%)]\tLoss: 0.101492\n",
      "Make prediction for 5010 samples...\n",
      "0.287696 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 822 [0/25046 (0%)]\tLoss: 0.101836\n",
      "Train epoch: 822 [660940/25046 (80%)]\tLoss: 0.105184\n",
      "Make prediction for 5010 samples...\n",
      "0.3190068 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 823 [0/25046 (0%)]\tLoss: 0.132909\n",
      "Train epoch: 823 [650020/25046 (80%)]\tLoss: 0.110592\n",
      "Make prediction for 5010 samples...\n",
      "0.3004857 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 824 [0/25046 (0%)]\tLoss: 0.142096\n",
      "Train epoch: 824 [656420/25046 (80%)]\tLoss: 0.106760\n",
      "Make prediction for 5010 samples...\n",
      "0.32105696 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 825 [0/25046 (0%)]\tLoss: 0.133894\n",
      "Train epoch: 825 [659180/25046 (80%)]\tLoss: 0.112504\n",
      "Make prediction for 5010 samples...\n",
      "0.28090274 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 826 [0/25046 (0%)]\tLoss: 0.102284\n",
      "Train epoch: 826 [650020/25046 (80%)]\tLoss: 0.133979\n",
      "Make prediction for 5010 samples...\n",
      "0.31994078 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 827 [0/25046 (0%)]\tLoss: 0.135182\n",
      "Train epoch: 827 [657520/25046 (80%)]\tLoss: 0.118725\n",
      "Make prediction for 5010 samples...\n",
      "0.3269409 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 828 [0/25046 (0%)]\tLoss: 0.134945\n",
      "Train epoch: 828 [663180/25046 (80%)]\tLoss: 0.118839\n",
      "Make prediction for 5010 samples...\n",
      "0.2795799 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 829 [0/25046 (0%)]\tLoss: 0.129366\n",
      "Train epoch: 829 [652000/25046 (80%)]\tLoss: 0.126746\n",
      "Make prediction for 5010 samples...\n",
      "0.29318237 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 830 [0/25046 (0%)]\tLoss: 0.123882\n",
      "Train epoch: 830 [661560/25046 (80%)]\tLoss: 0.128166\n",
      "Make prediction for 5010 samples...\n",
      "0.2857009 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 831 [0/25046 (0%)]\tLoss: 0.102622\n",
      "Train epoch: 831 [658040/25046 (80%)]\tLoss: 0.100745\n",
      "Make prediction for 5010 samples...\n",
      "0.27783597 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 832 [0/25046 (0%)]\tLoss: 0.121419\n",
      "Train epoch: 832 [662540/25046 (80%)]\tLoss: 0.122525\n",
      "Make prediction for 5010 samples...\n",
      "0.30872032 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 833 [0/25046 (0%)]\tLoss: 0.128432\n",
      "Train epoch: 833 [657000/25046 (80%)]\tLoss: 0.109492\n",
      "Make prediction for 5010 samples...\n",
      "0.28245273 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 834 [0/25046 (0%)]\tLoss: 0.111171\n",
      "Train epoch: 834 [655840/25046 (80%)]\tLoss: 0.127230\n",
      "Make prediction for 5010 samples...\n",
      "0.27070546 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 835 [0/25046 (0%)]\tLoss: 0.116458\n",
      "Train epoch: 835 [658940/25046 (80%)]\tLoss: 0.123519\n",
      "Make prediction for 5010 samples...\n",
      "0.2740099 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 836 [0/25046 (0%)]\tLoss: 0.107670\n",
      "Train epoch: 836 [659720/25046 (80%)]\tLoss: 0.134299\n",
      "Make prediction for 5010 samples...\n",
      "0.2740958 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 837 [0/25046 (0%)]\tLoss: 0.102325\n",
      "Train epoch: 837 [658740/25046 (80%)]\tLoss: 0.123190\n",
      "Make prediction for 5010 samples...\n",
      "0.27720007 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 838 [0/25046 (0%)]\tLoss: 0.098456\n",
      "Train epoch: 838 [653880/25046 (80%)]\tLoss: 0.115115\n",
      "Make prediction for 5010 samples...\n",
      "0.26772234 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 839 [0/25046 (0%)]\tLoss: 0.088970\n",
      "Train epoch: 839 [660240/25046 (80%)]\tLoss: 0.108305\n",
      "Make prediction for 5010 samples...\n",
      "0.2953484 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 840 [0/25046 (0%)]\tLoss: 0.105191\n",
      "Train epoch: 840 [668240/25046 (80%)]\tLoss: 0.105261\n",
      "Make prediction for 5010 samples...\n",
      "0.3144512 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 841 [0/25046 (0%)]\tLoss: 0.133811\n",
      "Train epoch: 841 [649060/25046 (80%)]\tLoss: 0.111156\n",
      "Make prediction for 5010 samples...\n",
      "0.27974358 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 842 [0/25046 (0%)]\tLoss: 0.095562\n",
      "Train epoch: 842 [658700/25046 (80%)]\tLoss: 0.112697\n",
      "Make prediction for 5010 samples...\n",
      "0.29232058 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 843 [0/25046 (0%)]\tLoss: 0.092851\n",
      "Train epoch: 843 [658260/25046 (80%)]\tLoss: 0.118460\n",
      "Make prediction for 5010 samples...\n",
      "0.2833713 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 844 [0/25046 (0%)]\tLoss: 0.096962\n",
      "Train epoch: 844 [659700/25046 (80%)]\tLoss: 0.133799\n",
      "Make prediction for 5010 samples...\n",
      "0.2801075 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 845 [0/25046 (0%)]\tLoss: 0.145472\n",
      "Train epoch: 845 [658280/25046 (80%)]\tLoss: 0.108455\n",
      "Make prediction for 5010 samples...\n",
      "0.29214597 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 846 [0/25046 (0%)]\tLoss: 0.098426\n",
      "Train epoch: 846 [661360/25046 (80%)]\tLoss: 0.101082\n",
      "Make prediction for 5010 samples...\n",
      "0.32440084 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 847 [0/25046 (0%)]\tLoss: 0.116872\n",
      "Train epoch: 847 [655200/25046 (80%)]\tLoss: 0.097324\n",
      "Make prediction for 5010 samples...\n",
      "0.41157958 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 848 [0/25046 (0%)]\tLoss: 0.192409\n",
      "Train epoch: 848 [653860/25046 (80%)]\tLoss: 0.119645\n",
      "Make prediction for 5010 samples...\n",
      "0.27765566 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 849 [0/25046 (0%)]\tLoss: 0.100063\n",
      "Train epoch: 849 [658840/25046 (80%)]\tLoss: 0.103876\n",
      "Make prediction for 5010 samples...\n",
      "0.28045824 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 850 [0/25046 (0%)]\tLoss: 0.105601\n",
      "Train epoch: 850 [656540/25046 (80%)]\tLoss: 0.104671\n",
      "Make prediction for 5010 samples...\n",
      "0.27506885 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 851 [0/25046 (0%)]\tLoss: 0.106202\n",
      "Train epoch: 851 [656420/25046 (80%)]\tLoss: 0.131332\n",
      "Make prediction for 5010 samples...\n",
      "0.26822788 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 852 [0/25046 (0%)]\tLoss: 0.100086\n",
      "Train epoch: 852 [651500/25046 (80%)]\tLoss: 0.104242\n",
      "Make prediction for 5010 samples...\n",
      "0.27649292 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 853 [0/25046 (0%)]\tLoss: 0.097137\n",
      "Train epoch: 853 [658640/25046 (80%)]\tLoss: 0.110322\n",
      "Make prediction for 5010 samples...\n",
      "0.27786374 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 854 [0/25046 (0%)]\tLoss: 0.119996\n",
      "Train epoch: 854 [654580/25046 (80%)]\tLoss: 0.146594\n",
      "Make prediction for 5010 samples...\n",
      "0.29317224 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 855 [0/25046 (0%)]\tLoss: 0.090138\n",
      "Train epoch: 855 [659960/25046 (80%)]\tLoss: 0.147741\n",
      "Make prediction for 5010 samples...\n",
      "0.2830355 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 856 [0/25046 (0%)]\tLoss: 0.149771\n",
      "Train epoch: 856 [654700/25046 (80%)]\tLoss: 0.119646\n",
      "Make prediction for 5010 samples...\n",
      "0.27250007 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 857 [0/25046 (0%)]\tLoss: 0.110453\n",
      "Train epoch: 857 [654380/25046 (80%)]\tLoss: 0.103379\n",
      "Make prediction for 5010 samples...\n",
      "0.2845306 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 858 [0/25046 (0%)]\tLoss: 0.091243\n",
      "Train epoch: 858 [648940/25046 (80%)]\tLoss: 0.116188\n",
      "Make prediction for 5010 samples...\n",
      "0.28061986 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 859 [0/25046 (0%)]\tLoss: 0.107678\n",
      "Train epoch: 859 [656000/25046 (80%)]\tLoss: 0.126731\n",
      "Make prediction for 5010 samples...\n",
      "0.30836433 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 860 [0/25046 (0%)]\tLoss: 0.136582\n",
      "Train epoch: 860 [658260/25046 (80%)]\tLoss: 0.115345\n",
      "Make prediction for 5010 samples...\n",
      "0.2877742 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 861 [0/25046 (0%)]\tLoss: 0.112952\n",
      "Train epoch: 861 [653960/25046 (80%)]\tLoss: 0.115829\n",
      "Make prediction for 5010 samples...\n",
      "0.28898618 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 862 [0/25046 (0%)]\tLoss: 0.105301\n",
      "Train epoch: 862 [656640/25046 (80%)]\tLoss: 0.128620\n",
      "Make prediction for 5010 samples...\n",
      "0.2883447 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 863 [0/25046 (0%)]\tLoss: 0.122096\n",
      "Train epoch: 863 [652020/25046 (80%)]\tLoss: 0.094343\n",
      "Make prediction for 5010 samples...\n",
      "0.27179882 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 864 [0/25046 (0%)]\tLoss: 0.125655\n",
      "Train epoch: 864 [657560/25046 (80%)]\tLoss: 0.102920\n",
      "Make prediction for 5010 samples...\n",
      "0.2876649 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 865 [0/25046 (0%)]\tLoss: 0.110555\n",
      "Train epoch: 865 [654500/25046 (80%)]\tLoss: 0.117477\n",
      "Make prediction for 5010 samples...\n",
      "0.27955 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 866 [0/25046 (0%)]\tLoss: 0.107567\n",
      "Train epoch: 866 [660520/25046 (80%)]\tLoss: 0.096586\n",
      "Make prediction for 5010 samples...\n",
      "0.27377084 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 867 [0/25046 (0%)]\tLoss: 0.119958\n",
      "Train epoch: 867 [653740/25046 (80%)]\tLoss: 0.114933\n",
      "Make prediction for 5010 samples...\n",
      "0.282864 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 868 [0/25046 (0%)]\tLoss: 0.139205\n",
      "Train epoch: 868 [657980/25046 (80%)]\tLoss: 0.118335\n",
      "Make prediction for 5010 samples...\n",
      "0.289387 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 869 [0/25046 (0%)]\tLoss: 0.140730\n",
      "Train epoch: 869 [662100/25046 (80%)]\tLoss: 0.116797\n",
      "Make prediction for 5010 samples...\n",
      "0.29568315 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 870 [0/25046 (0%)]\tLoss: 0.100244\n",
      "Train epoch: 870 [652440/25046 (80%)]\tLoss: 0.132271\n",
      "Make prediction for 5010 samples...\n",
      "0.28096688 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 871 [0/25046 (0%)]\tLoss: 0.099350\n",
      "Train epoch: 871 [652580/25046 (80%)]\tLoss: 0.097880\n",
      "Make prediction for 5010 samples...\n",
      "0.2949429 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 872 [0/25046 (0%)]\tLoss: 0.112474\n",
      "Train epoch: 872 [657580/25046 (80%)]\tLoss: 0.108553\n",
      "Make prediction for 5010 samples...\n",
      "0.2916551 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 873 [0/25046 (0%)]\tLoss: 0.127158\n",
      "Train epoch: 873 [658660/25046 (80%)]\tLoss: 0.102353\n",
      "Make prediction for 5010 samples...\n",
      "0.29104295 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 874 [0/25046 (0%)]\tLoss: 0.114096\n",
      "Train epoch: 874 [652000/25046 (80%)]\tLoss: 0.133896\n",
      "Make prediction for 5010 samples...\n",
      "0.2952271 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 875 [0/25046 (0%)]\tLoss: 0.106899\n",
      "Train epoch: 875 [654380/25046 (80%)]\tLoss: 0.128768\n",
      "Make prediction for 5010 samples...\n",
      "0.28778684 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 876 [0/25046 (0%)]\tLoss: 0.096019\n",
      "Train epoch: 876 [654780/25046 (80%)]\tLoss: 0.104917\n",
      "Make prediction for 5010 samples...\n",
      "0.29714182 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 877 [0/25046 (0%)]\tLoss: 0.097732\n",
      "Train epoch: 877 [654340/25046 (80%)]\tLoss: 0.134995\n",
      "Make prediction for 5010 samples...\n",
      "0.3029538 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 878 [0/25046 (0%)]\tLoss: 0.120581\n",
      "Train epoch: 878 [653420/25046 (80%)]\tLoss: 0.113459\n",
      "Make prediction for 5010 samples...\n",
      "0.27700925 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 879 [0/25046 (0%)]\tLoss: 0.108675\n",
      "Train epoch: 879 [655840/25046 (80%)]\tLoss: 0.098746\n",
      "Make prediction for 5010 samples...\n",
      "0.2741437 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 880 [0/25046 (0%)]\tLoss: 0.104742\n",
      "Train epoch: 880 [660240/25046 (80%)]\tLoss: 0.129630\n",
      "Make prediction for 5010 samples...\n",
      "0.28764802 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 881 [0/25046 (0%)]\tLoss: 0.095492\n",
      "Train epoch: 881 [656540/25046 (80%)]\tLoss: 0.115544\n",
      "Make prediction for 5010 samples...\n",
      "0.2748914 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 882 [0/25046 (0%)]\tLoss: 0.099378\n",
      "Train epoch: 882 [654340/25046 (80%)]\tLoss: 0.129153\n",
      "Make prediction for 5010 samples...\n",
      "0.2696538 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 883 [0/25046 (0%)]\tLoss: 0.088335\n",
      "Train epoch: 883 [659700/25046 (80%)]\tLoss: 0.115958\n",
      "Make prediction for 5010 samples...\n",
      "0.27279514 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 884 [0/25046 (0%)]\tLoss: 0.093339\n",
      "Train epoch: 884 [655880/25046 (80%)]\tLoss: 0.114788\n",
      "Make prediction for 5010 samples...\n",
      "0.2730906 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 885 [0/25046 (0%)]\tLoss: 0.109213\n",
      "Train epoch: 885 [661060/25046 (80%)]\tLoss: 0.109434\n",
      "Make prediction for 5010 samples...\n",
      "0.27354407 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 886 [0/25046 (0%)]\tLoss: 0.129270\n",
      "Train epoch: 886 [651360/25046 (80%)]\tLoss: 0.141545\n",
      "Make prediction for 5010 samples...\n",
      "0.27276644 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 887 [0/25046 (0%)]\tLoss: 0.095934\n",
      "Train epoch: 887 [655820/25046 (80%)]\tLoss: 0.124209\n",
      "Make prediction for 5010 samples...\n",
      "0.29061776 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 888 [0/25046 (0%)]\tLoss: 0.125568\n",
      "Train epoch: 888 [664460/25046 (80%)]\tLoss: 0.124027\n",
      "Make prediction for 5010 samples...\n",
      "0.27326125 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 889 [0/25046 (0%)]\tLoss: 0.146632\n",
      "Train epoch: 889 [655560/25046 (80%)]\tLoss: 0.115364\n",
      "Make prediction for 5010 samples...\n",
      "0.29544714 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 890 [0/25046 (0%)]\tLoss: 0.124713\n",
      "Train epoch: 890 [653260/25046 (80%)]\tLoss: 0.124730\n",
      "Make prediction for 5010 samples...\n",
      "0.2823437 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 891 [0/25046 (0%)]\tLoss: 0.094054\n",
      "Train epoch: 891 [660360/25046 (80%)]\tLoss: 0.106355\n",
      "Make prediction for 5010 samples...\n",
      "0.27499026 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 892 [0/25046 (0%)]\tLoss: 0.104850\n",
      "Train epoch: 892 [656820/25046 (80%)]\tLoss: 0.121779\n",
      "Make prediction for 5010 samples...\n",
      "0.33932137 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 893 [0/25046 (0%)]\tLoss: 0.129664\n",
      "Train epoch: 893 [659720/25046 (80%)]\tLoss: 0.120229\n",
      "Make prediction for 5010 samples...\n",
      "0.27358142 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 894 [0/25046 (0%)]\tLoss: 0.127875\n",
      "Train epoch: 894 [651420/25046 (80%)]\tLoss: 0.119569\n",
      "Make prediction for 5010 samples...\n",
      "0.31531298 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 895 [0/25046 (0%)]\tLoss: 0.102993\n",
      "Train epoch: 895 [657580/25046 (80%)]\tLoss: 0.113773\n",
      "Make prediction for 5010 samples...\n",
      "0.27516583 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 896 [0/25046 (0%)]\tLoss: 0.093331\n",
      "Train epoch: 896 [655600/25046 (80%)]\tLoss: 0.112693\n",
      "Make prediction for 5010 samples...\n",
      "0.28689662 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 897 [0/25046 (0%)]\tLoss: 0.092659\n",
      "Train epoch: 897 [654460/25046 (80%)]\tLoss: 0.096175\n",
      "Make prediction for 5010 samples...\n",
      "0.2756463 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 898 [0/25046 (0%)]\tLoss: 0.134356\n",
      "Train epoch: 898 [659480/25046 (80%)]\tLoss: 0.129271\n",
      "Make prediction for 5010 samples...\n",
      "0.27295485 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 899 [0/25046 (0%)]\tLoss: 0.101356\n",
      "Train epoch: 899 [657800/25046 (80%)]\tLoss: 0.113629\n",
      "Make prediction for 5010 samples...\n",
      "0.27361694 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 900 [0/25046 (0%)]\tLoss: 0.114648\n",
      "Train epoch: 900 [650360/25046 (80%)]\tLoss: 0.121062\n",
      "Make prediction for 5010 samples...\n",
      "0.34242317 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 901 [0/25046 (0%)]\tLoss: 0.136602\n",
      "Train epoch: 901 [657760/25046 (80%)]\tLoss: 0.121277\n",
      "Make prediction for 5010 samples...\n",
      "0.27740324 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 902 [0/25046 (0%)]\tLoss: 0.100862\n",
      "Train epoch: 902 [655280/25046 (80%)]\tLoss: 0.094626\n",
      "Make prediction for 5010 samples...\n",
      "0.28448692 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 903 [0/25046 (0%)]\tLoss: 0.107098\n",
      "Train epoch: 903 [650380/25046 (80%)]\tLoss: 0.131136\n",
      "Make prediction for 5010 samples...\n",
      "0.28817457 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 904 [0/25046 (0%)]\tLoss: 0.095675\n",
      "Train epoch: 904 [651620/25046 (80%)]\tLoss: 0.114017\n",
      "Make prediction for 5010 samples...\n",
      "0.33587158 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 905 [0/25046 (0%)]\tLoss: 0.117558\n",
      "Train epoch: 905 [663360/25046 (80%)]\tLoss: 0.097615\n",
      "Make prediction for 5010 samples...\n",
      "0.27727184 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 906 [0/25046 (0%)]\tLoss: 0.111600\n",
      "Train epoch: 906 [658160/25046 (80%)]\tLoss: 0.137437\n",
      "Make prediction for 5010 samples...\n",
      "0.2719025 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 907 [0/25046 (0%)]\tLoss: 0.107491\n",
      "Train epoch: 907 [664360/25046 (80%)]\tLoss: 0.124907\n",
      "Make prediction for 5010 samples...\n",
      "0.2790057 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 908 [0/25046 (0%)]\tLoss: 0.091645\n",
      "Train epoch: 908 [663460/25046 (80%)]\tLoss: 0.135789\n",
      "Make prediction for 5010 samples...\n",
      "0.33276528 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 909 [0/25046 (0%)]\tLoss: 0.124564\n",
      "Train epoch: 909 [650780/25046 (80%)]\tLoss: 0.116434\n",
      "Make prediction for 5010 samples...\n",
      "0.32503834 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 910 [0/25046 (0%)]\tLoss: 0.121222\n",
      "Train epoch: 910 [652820/25046 (80%)]\tLoss: 0.108414\n",
      "Make prediction for 5010 samples...\n",
      "0.28028616 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 911 [0/25046 (0%)]\tLoss: 0.106609\n",
      "Train epoch: 911 [651940/25046 (80%)]\tLoss: 0.204614\n",
      "Make prediction for 5010 samples...\n",
      "0.31086388 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 912 [0/25046 (0%)]\tLoss: 0.121260\n",
      "Train epoch: 912 [651080/25046 (80%)]\tLoss: 0.101548\n",
      "Make prediction for 5010 samples...\n",
      "0.27026954 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 913 [0/25046 (0%)]\tLoss: 0.101670\n",
      "Train epoch: 913 [663080/25046 (80%)]\tLoss: 0.113637\n",
      "Make prediction for 5010 samples...\n",
      "0.27244338 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 914 [0/25046 (0%)]\tLoss: 0.094857\n",
      "Train epoch: 914 [662060/25046 (80%)]\tLoss: 0.115016\n",
      "Make prediction for 5010 samples...\n",
      "0.276663 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 915 [0/25046 (0%)]\tLoss: 0.110787\n",
      "Train epoch: 915 [656100/25046 (80%)]\tLoss: 0.113469\n",
      "Make prediction for 5010 samples...\n",
      "0.28005794 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 916 [0/25046 (0%)]\tLoss: 0.130347\n",
      "Train epoch: 916 [660420/25046 (80%)]\tLoss: 0.095631\n",
      "Make prediction for 5010 samples...\n",
      "0.27355397 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 917 [0/25046 (0%)]\tLoss: 0.101615\n",
      "Train epoch: 917 [659740/25046 (80%)]\tLoss: 0.116632\n",
      "Make prediction for 5010 samples...\n",
      "0.27776873 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 918 [0/25046 (0%)]\tLoss: 0.095583\n",
      "Train epoch: 918 [657940/25046 (80%)]\tLoss: 0.092109\n",
      "Make prediction for 5010 samples...\n",
      "0.28114334 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 919 [0/25046 (0%)]\tLoss: 0.101189\n",
      "Train epoch: 919 [654020/25046 (80%)]\tLoss: 0.105710\n",
      "Make prediction for 5010 samples...\n",
      "0.27477673 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 920 [0/25046 (0%)]\tLoss: 0.102630\n",
      "Train epoch: 920 [657840/25046 (80%)]\tLoss: 0.099431\n",
      "Make prediction for 5010 samples...\n",
      "0.26690108 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 921 [0/25046 (0%)]\tLoss: 0.099315\n",
      "Train epoch: 921 [657100/25046 (80%)]\tLoss: 0.107461\n",
      "Make prediction for 5010 samples...\n",
      "0.2826818 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 922 [0/25046 (0%)]\tLoss: 0.090523\n",
      "Train epoch: 922 [663780/25046 (80%)]\tLoss: 0.111029\n",
      "Make prediction for 5010 samples...\n",
      "0.27259672 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 923 [0/25046 (0%)]\tLoss: 0.102581\n",
      "Train epoch: 923 [657160/25046 (80%)]\tLoss: 0.101831\n",
      "Make prediction for 5010 samples...\n",
      "0.31013066 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 924 [0/25046 (0%)]\tLoss: 0.110933\n",
      "Train epoch: 924 [658560/25046 (80%)]\tLoss: 0.124674\n",
      "Make prediction for 5010 samples...\n",
      "0.27519688 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 925 [0/25046 (0%)]\tLoss: 0.103520\n",
      "Train epoch: 925 [656880/25046 (80%)]\tLoss: 0.087881\n",
      "Make prediction for 5010 samples...\n",
      "0.31639692 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 926 [0/25046 (0%)]\tLoss: 0.113335\n",
      "Train epoch: 926 [654060/25046 (80%)]\tLoss: 0.128256\n",
      "Make prediction for 5010 samples...\n",
      "0.29492003 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 927 [0/25046 (0%)]\tLoss: 0.101647\n",
      "Train epoch: 927 [649440/25046 (80%)]\tLoss: 0.108507\n",
      "Make prediction for 5010 samples...\n",
      "0.2923604 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 928 [0/25046 (0%)]\tLoss: 0.113107\n",
      "Train epoch: 928 [653740/25046 (80%)]\tLoss: 0.095124\n",
      "Make prediction for 5010 samples...\n",
      "0.27037913 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 929 [0/25046 (0%)]\tLoss: 0.106393\n",
      "Train epoch: 929 [655580/25046 (80%)]\tLoss: 0.126329\n",
      "Make prediction for 5010 samples...\n",
      "0.2816817 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 930 [0/25046 (0%)]\tLoss: 0.082622\n",
      "Train epoch: 930 [656540/25046 (80%)]\tLoss: 0.129259\n",
      "Make prediction for 5010 samples...\n",
      "0.27711964 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 931 [0/25046 (0%)]\tLoss: 0.118254\n",
      "Train epoch: 931 [652740/25046 (80%)]\tLoss: 0.106033\n",
      "Make prediction for 5010 samples...\n",
      "0.30138302 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 932 [0/25046 (0%)]\tLoss: 0.098102\n",
      "Train epoch: 932 [655660/25046 (80%)]\tLoss: 0.092922\n",
      "Make prediction for 5010 samples...\n",
      "0.31405208 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 933 [0/25046 (0%)]\tLoss: 0.129697\n",
      "Train epoch: 933 [659480/25046 (80%)]\tLoss: 0.103642\n",
      "Make prediction for 5010 samples...\n",
      "0.2791481 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 934 [0/25046 (0%)]\tLoss: 0.107904\n",
      "Train epoch: 934 [657940/25046 (80%)]\tLoss: 0.144893\n",
      "Make prediction for 5010 samples...\n",
      "0.27187276 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 935 [0/25046 (0%)]\tLoss: 0.093867\n",
      "Train epoch: 935 [661980/25046 (80%)]\tLoss: 0.079698\n",
      "Make prediction for 5010 samples...\n",
      "0.2751401 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 936 [0/25046 (0%)]\tLoss: 0.111454\n",
      "Train epoch: 936 [652280/25046 (80%)]\tLoss: 0.127617\n",
      "Make prediction for 5010 samples...\n",
      "0.273812 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 937 [0/25046 (0%)]\tLoss: 0.108467\n",
      "Train epoch: 937 [659580/25046 (80%)]\tLoss: 0.098840\n",
      "Make prediction for 5010 samples...\n",
      "0.27293926 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 938 [0/25046 (0%)]\tLoss: 0.097522\n",
      "Train epoch: 938 [653280/25046 (80%)]\tLoss: 0.102049\n",
      "Make prediction for 5010 samples...\n",
      "0.2793314 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 939 [0/25046 (0%)]\tLoss: 0.129256\n",
      "Train epoch: 939 [661160/25046 (80%)]\tLoss: 0.154714\n",
      "Make prediction for 5010 samples...\n",
      "0.32290116 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 940 [0/25046 (0%)]\tLoss: 0.150225\n",
      "Train epoch: 940 [655700/25046 (80%)]\tLoss: 0.097475\n",
      "Make prediction for 5010 samples...\n",
      "0.29699188 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 941 [0/25046 (0%)]\tLoss: 0.106089\n",
      "Train epoch: 941 [653540/25046 (80%)]\tLoss: 0.086592\n",
      "Make prediction for 5010 samples...\n",
      "0.26887432 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 942 [0/25046 (0%)]\tLoss: 0.135721\n",
      "Train epoch: 942 [653400/25046 (80%)]\tLoss: 0.120271\n",
      "Make prediction for 5010 samples...\n",
      "0.27952498 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 943 [0/25046 (0%)]\tLoss: 0.094716\n",
      "Train epoch: 943 [655740/25046 (80%)]\tLoss: 0.121985\n",
      "Make prediction for 5010 samples...\n",
      "0.29324454 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 944 [0/25046 (0%)]\tLoss: 0.107195\n",
      "Train epoch: 944 [659140/25046 (80%)]\tLoss: 0.139556\n",
      "Make prediction for 5010 samples...\n",
      "0.3043899 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 945 [0/25046 (0%)]\tLoss: 0.105638\n",
      "Train epoch: 945 [655100/25046 (80%)]\tLoss: 0.128937\n",
      "Make prediction for 5010 samples...\n",
      "0.27454978 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 946 [0/25046 (0%)]\tLoss: 0.119683\n",
      "Train epoch: 946 [657540/25046 (80%)]\tLoss: 0.121624\n",
      "Make prediction for 5010 samples...\n",
      "0.27157167 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 947 [0/25046 (0%)]\tLoss: 0.092659\n",
      "Train epoch: 947 [660020/25046 (80%)]\tLoss: 0.151491\n",
      "Make prediction for 5010 samples...\n",
      "0.27260444 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 948 [0/25046 (0%)]\tLoss: 0.115856\n",
      "Train epoch: 948 [650140/25046 (80%)]\tLoss: 0.108022\n",
      "Make prediction for 5010 samples...\n",
      "0.2783774 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 949 [0/25046 (0%)]\tLoss: 0.083924\n",
      "Train epoch: 949 [657020/25046 (80%)]\tLoss: 0.090949\n",
      "Make prediction for 5010 samples...\n",
      "0.27354342 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 950 [0/25046 (0%)]\tLoss: 0.080351\n",
      "Train epoch: 950 [659780/25046 (80%)]\tLoss: 0.111127\n",
      "Make prediction for 5010 samples...\n",
      "0.26785153 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 951 [0/25046 (0%)]\tLoss: 0.102826\n",
      "Train epoch: 951 [656040/25046 (80%)]\tLoss: 0.126700\n",
      "Make prediction for 5010 samples...\n",
      "0.27392885 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 952 [0/25046 (0%)]\tLoss: 0.128735\n",
      "Train epoch: 952 [655860/25046 (80%)]\tLoss: 0.110846\n",
      "Make prediction for 5010 samples...\n",
      "0.27030125 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 953 [0/25046 (0%)]\tLoss: 0.090805\n",
      "Train epoch: 953 [657660/25046 (80%)]\tLoss: 0.095991\n",
      "Make prediction for 5010 samples...\n",
      "0.33295006 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 954 [0/25046 (0%)]\tLoss: 0.123487\n",
      "Train epoch: 954 [663840/25046 (80%)]\tLoss: 0.117370\n",
      "Make prediction for 5010 samples...\n",
      "0.2903451 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 955 [0/25046 (0%)]\tLoss: 0.101278\n",
      "Train epoch: 955 [658880/25046 (80%)]\tLoss: 0.112251\n",
      "Make prediction for 5010 samples...\n",
      "0.2920592 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 956 [0/25046 (0%)]\tLoss: 0.102437\n",
      "Train epoch: 956 [655260/25046 (80%)]\tLoss: 0.121677\n",
      "Make prediction for 5010 samples...\n",
      "0.29061365 No improvement since epoch  809 ; best_mse,best_ci: 0.26639336 808 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 957 [0/25046 (0%)]\tLoss: 0.099430\n",
      "Train epoch: 957 [655140/25046 (80%)]\tLoss: 0.109629\n",
      "Make prediction for 5010 samples...\n",
      "rmse improved at epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 958 [0/25046 (0%)]\tLoss: 0.106722\n",
      "Train epoch: 958 [659440/25046 (80%)]\tLoss: 0.103332\n",
      "Make prediction for 5010 samples...\n",
      "0.2683162 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 959 [0/25046 (0%)]\tLoss: 0.096596\n",
      "Train epoch: 959 [658600/25046 (80%)]\tLoss: 0.094275\n",
      "Make prediction for 5010 samples...\n",
      "0.27245128 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 960 [0/25046 (0%)]\tLoss: 0.120298\n",
      "Train epoch: 960 [655420/25046 (80%)]\tLoss: 0.165720\n",
      "Make prediction for 5010 samples...\n",
      "0.27606663 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 961 [0/25046 (0%)]\tLoss: 0.109206\n",
      "Train epoch: 961 [655400/25046 (80%)]\tLoss: 0.105853\n",
      "Make prediction for 5010 samples...\n",
      "0.3349106 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 962 [0/25046 (0%)]\tLoss: 0.117642\n",
      "Train epoch: 962 [654880/25046 (80%)]\tLoss: 0.092994\n",
      "Make prediction for 5010 samples...\n",
      "0.27976692 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 963 [0/25046 (0%)]\tLoss: 0.096150\n",
      "Train epoch: 963 [653220/25046 (80%)]\tLoss: 0.108849\n",
      "Make prediction for 5010 samples...\n",
      "0.27455062 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 964 [0/25046 (0%)]\tLoss: 0.123492\n",
      "Train epoch: 964 [657240/25046 (80%)]\tLoss: 0.125026\n",
      "Make prediction for 5010 samples...\n",
      "0.30732802 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 965 [0/25046 (0%)]\tLoss: 0.119625\n",
      "Train epoch: 965 [655180/25046 (80%)]\tLoss: 0.082556\n",
      "Make prediction for 5010 samples...\n",
      "0.30944663 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 966 [0/25046 (0%)]\tLoss: 0.118911\n",
      "Train epoch: 966 [650420/25046 (80%)]\tLoss: 0.088970\n",
      "Make prediction for 5010 samples...\n",
      "0.26898474 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 967 [0/25046 (0%)]\tLoss: 0.085581\n",
      "Train epoch: 967 [651860/25046 (80%)]\tLoss: 0.104870\n",
      "Make prediction for 5010 samples...\n",
      "0.27486438 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 968 [0/25046 (0%)]\tLoss: 0.100858\n",
      "Train epoch: 968 [659000/25046 (80%)]\tLoss: 0.106517\n",
      "Make prediction for 5010 samples...\n",
      "0.27678058 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 969 [0/25046 (0%)]\tLoss: 0.095781\n",
      "Train epoch: 969 [657460/25046 (80%)]\tLoss: 0.105357\n",
      "Make prediction for 5010 samples...\n",
      "0.27220473 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 970 [0/25046 (0%)]\tLoss: 0.113795\n",
      "Train epoch: 970 [652980/25046 (80%)]\tLoss: 0.107684\n",
      "Make prediction for 5010 samples...\n",
      "0.27802145 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 971 [0/25046 (0%)]\tLoss: 0.104843\n",
      "Train epoch: 971 [650560/25046 (80%)]\tLoss: 0.104410\n",
      "Make prediction for 5010 samples...\n",
      "0.28465363 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 972 [0/25046 (0%)]\tLoss: 0.099809\n",
      "Train epoch: 972 [655600/25046 (80%)]\tLoss: 0.111520\n",
      "Make prediction for 5010 samples...\n",
      "0.272322 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 973 [0/25046 (0%)]\tLoss: 0.101648\n",
      "Train epoch: 973 [651980/25046 (80%)]\tLoss: 0.119201\n",
      "Make prediction for 5010 samples...\n",
      "0.2892187 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 974 [0/25046 (0%)]\tLoss: 0.093370\n",
      "Train epoch: 974 [656780/25046 (80%)]\tLoss: 0.100700\n",
      "Make prediction for 5010 samples...\n",
      "0.26774713 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 975 [0/25046 (0%)]\tLoss: 0.101716\n",
      "Train epoch: 975 [661920/25046 (80%)]\tLoss: 0.130111\n",
      "Make prediction for 5010 samples...\n",
      "0.3022997 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 976 [0/25046 (0%)]\tLoss: 0.117884\n",
      "Train epoch: 976 [660440/25046 (80%)]\tLoss: 0.107813\n",
      "Make prediction for 5010 samples...\n",
      "0.2652732 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 977 [0/25046 (0%)]\tLoss: 0.100653\n",
      "Train epoch: 977 [659140/25046 (80%)]\tLoss: 0.098389\n",
      "Make prediction for 5010 samples...\n",
      "0.27860498 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 978 [0/25046 (0%)]\tLoss: 0.123911\n",
      "Train epoch: 978 [655920/25046 (80%)]\tLoss: 0.105296\n",
      "Make prediction for 5010 samples...\n",
      "0.29671803 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 979 [0/25046 (0%)]\tLoss: 0.094461\n",
      "Train epoch: 979 [657400/25046 (80%)]\tLoss: 0.116830\n",
      "Make prediction for 5010 samples...\n",
      "0.26878855 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 980 [0/25046 (0%)]\tLoss: 0.095962\n",
      "Train epoch: 980 [649880/25046 (80%)]\tLoss: 0.166757\n",
      "Make prediction for 5010 samples...\n",
      "0.28954044 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 981 [0/25046 (0%)]\tLoss: 0.135380\n",
      "Train epoch: 981 [648520/25046 (80%)]\tLoss: 0.115920\n",
      "Make prediction for 5010 samples...\n",
      "0.2985498 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 982 [0/25046 (0%)]\tLoss: 0.135823\n",
      "Train epoch: 982 [657860/25046 (80%)]\tLoss: 0.116684\n",
      "Make prediction for 5010 samples...\n",
      "0.27263206 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 983 [0/25046 (0%)]\tLoss: 0.102821\n",
      "Train epoch: 983 [658440/25046 (80%)]\tLoss: 0.113125\n",
      "Make prediction for 5010 samples...\n",
      "0.29187182 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 984 [0/25046 (0%)]\tLoss: 0.101809\n",
      "Train epoch: 984 [650960/25046 (80%)]\tLoss: 0.125134\n",
      "Make prediction for 5010 samples...\n",
      "0.29493567 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 985 [0/25046 (0%)]\tLoss: 0.104480\n",
      "Train epoch: 985 [662020/25046 (80%)]\tLoss: 0.086476\n",
      "Make prediction for 5010 samples...\n",
      "0.27085423 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 986 [0/25046 (0%)]\tLoss: 0.100485\n",
      "Train epoch: 986 [654440/25046 (80%)]\tLoss: 0.097251\n",
      "Make prediction for 5010 samples...\n",
      "0.27627265 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 987 [0/25046 (0%)]\tLoss: 0.105922\n",
      "Train epoch: 987 [659400/25046 (80%)]\tLoss: 0.117069\n",
      "Make prediction for 5010 samples...\n",
      "0.27358833 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 988 [0/25046 (0%)]\tLoss: 0.094153\n",
      "Train epoch: 988 [656420/25046 (80%)]\tLoss: 0.124680\n",
      "Make prediction for 5010 samples...\n",
      "0.27298295 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 989 [0/25046 (0%)]\tLoss: 0.101782\n",
      "Train epoch: 989 [652480/25046 (80%)]\tLoss: 0.109111\n",
      "Make prediction for 5010 samples...\n",
      "0.27894855 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 990 [0/25046 (0%)]\tLoss: 0.090216\n",
      "Train epoch: 990 [652860/25046 (80%)]\tLoss: 0.102822\n",
      "Make prediction for 5010 samples...\n",
      "0.2906657 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 991 [0/25046 (0%)]\tLoss: 0.097123\n",
      "Train epoch: 991 [654940/25046 (80%)]\tLoss: 0.119644\n",
      "Make prediction for 5010 samples...\n",
      "0.26760742 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 992 [0/25046 (0%)]\tLoss: 0.095819\n",
      "Train epoch: 992 [649140/25046 (80%)]\tLoss: 0.099271\n",
      "Make prediction for 5010 samples...\n",
      "0.27255997 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 993 [0/25046 (0%)]\tLoss: 0.095823\n",
      "Train epoch: 993 [651460/25046 (80%)]\tLoss: 0.088694\n",
      "Make prediction for 5010 samples...\n",
      "0.27135232 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 994 [0/25046 (0%)]\tLoss: 0.095954\n",
      "Train epoch: 994 [660020/25046 (80%)]\tLoss: 0.138425\n",
      "Make prediction for 5010 samples...\n",
      "0.27009913 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 995 [0/25046 (0%)]\tLoss: 0.136127\n",
      "Train epoch: 995 [658680/25046 (80%)]\tLoss: 0.095638\n",
      "Make prediction for 5010 samples...\n",
      "0.3106176 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 996 [0/25046 (0%)]\tLoss: 0.090542\n",
      "Train epoch: 996 [661220/25046 (80%)]\tLoss: 0.102896\n",
      "Make prediction for 5010 samples...\n",
      "0.32459506 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 997 [0/25046 (0%)]\tLoss: 0.120893\n",
      "Train epoch: 997 [649140/25046 (80%)]\tLoss: 0.128499\n",
      "Make prediction for 5010 samples...\n",
      "0.2757306 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 998 [0/25046 (0%)]\tLoss: 0.087259\n",
      "Train epoch: 998 [653220/25046 (80%)]\tLoss: 0.109187\n",
      "Make prediction for 5010 samples...\n",
      "0.2931208 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 999 [0/25046 (0%)]\tLoss: 0.100157\n",
      "Train epoch: 999 [661100/25046 (80%)]\tLoss: 0.103719\n",
      "Make prediction for 5010 samples...\n",
      "0.27663612 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n",
      "Training on 25046 samples...\n",
      "Train epoch: 1000 [0/25046 (0%)]\tLoss: 0.088180\n",
      "Train epoch: 1000 [656120/25046 (80%)]\tLoss: 0.094757\n",
      "Make prediction for 5010 samples...\n",
      "0.28398854 No improvement since epoch  957 ; best_mse,best_ci: 0.26374927 956 davis\n"
     ]
    }
   ],
   "source": [
    "for hyperparameters in hyperparameter_grid:\n",
    "    # Unpack the hyperparameters\n",
    "    learning_rate, batch_size= hyperparameters\n",
    "    print(learning_rate, batch_size)\n",
    "\n",
    "    # Define the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model =GCNNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define your loss function and data loader\n",
    "    loss_fn = nn.MSELoss()\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # best mse, ci and epoch\n",
    "    best_mse = 1000\n",
    "    best_ci = 0\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Set the files name\n",
    "    model_file_name = 'model_' + str(learning_rate) + '_' + str(batch_size )+ '_'  +  '.model'\n",
    "    best_result_file_name = 'best_result_' + str(learning_rate) + '_' + str(batch_size ) + '_' +  '.csv'\n",
    "    result_file_name = 'result_' + str(learning_rate) + '_' + str(batch_size ) + '_' +  '.csv'\n",
    "    losses_file_name = 'losses_' + str(learning_rate) + '_' + str(batch_size ) + '_' +  '.csv'\n",
    "\n",
    "    # create a CSV file and write the header row\n",
    "    with open(result_file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['RMSE', 'MSE', 'Pearson', 'Spearman', 'CI', 'Learning rate', 'Batch size', 'Epoch'])\n",
    "        \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train(model, device, train_loader, optimizer, epoch+1, losses_file_name)\n",
    "        G,P = predicting(model, device, test_loader)\n",
    "        ret = [rmse(G,P),mse(G,P),pearson(G,P),spearman(G,P),ci(G,P), learning_rate, batch_size, epoch]\n",
    "\n",
    "        # append ret to CSV file\n",
    "        with open(result_file_name, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(ret)\n",
    "        \n",
    "        # save the best results to CSV File\n",
    "        if ret[1]<best_mse:\n",
    "            torch.save(model.state_dict(), model_file_name)\n",
    "            with open(best_result_file_name,'w') as f:\n",
    "                f.write(','.join(map(str,ret)))\n",
    "            best_epoch = epoch+1\n",
    "            best_mse = ret[1]\n",
    "            best_ci = ret[-1]\n",
    "            print('rmse improved at epoch ', best_epoch, '; best_mse,best_ci:', best_mse,best_ci,dataset)        \n",
    "        else:\n",
    "            print(ret[1],'No improvement since epoch ', best_epoch, '; best_mse,best_ci:', best_mse,best_ci,dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3530f7147189b0a9fe5cdb31e8230923c8f37dd5da51ac8b12ce15e01533f3f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
